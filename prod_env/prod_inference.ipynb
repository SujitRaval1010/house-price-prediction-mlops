{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================================\n",
    "# üöÄ PRODUCTION BATCH INFERENCE - CONFIG DRIVEN (FIXED)\n",
    "# =============================================================================\n",
    "# Purpose: Run batch inference using production serving endpoint\n",
    "# Now reads from pipeline_config.yml - No hardcoding!\n",
    "# Prerequisites: Run 07_create_serving_endpoint.py first\n",
    "# =============================================================================\n",
    "\n",
    "# COMMAND ----------\n",
    "%pip install xgboost\n",
    "\n",
    "# COMMAND ----------\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "# COMMAND ----------\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "import yaml\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ PRODUCTION BATCH INFERENCE (CONFIG-DRIVEN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ LOAD PIPELINE CONFIGURATION (Dynamic Path)\n",
    "# =============================================================================\n",
    "print(\"\\nüìã Loading pipeline configuration from pipeline_config.yml...\")\n",
    "\n",
    "import os, yaml, sys, traceback\n",
    "\n",
    "try:\n",
    "    # Detect current script path and repo root\n",
    "    try:\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        current_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "\n",
    "    # Try same directory first\n",
    "    config_path = os.path.join(current_dir, \"pipeline_config.yml\")\n",
    "\n",
    "    # If not found, try dev_env folder\n",
    "    if not os.path.exists(config_path):\n",
    "        config_path = os.path.join(project_root, \"dev_env\", \"pipeline_config.yml\")\n",
    "\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"pipeline_config.yml not found at {config_path}\")\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "\n",
    "    print(f\"‚úÖ Loaded pipeline_config.yml from: {config_path}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract Config Parameters\n",
    "    # -----------------------------\n",
    "    MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "    BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "    ENDPOINT_NAME = f\"{BASE_NAME.replace('_', '-')}-{MODEL_TYPE}-prod\"\n",
    "\n",
    "    # Data Configuration\n",
    "    data_cfg = pipeline_cfg[\"data\"]\n",
    "    DATA_CATALOG, DATA_SCHEMA, INPUT_TABLE = data_cfg[\"input_table\"].split(\".\")\n",
    "    FEATURE_COLS = data_cfg[\"features\"]\n",
    "    LABEL_COL = data_cfg[\"label\"]\n",
    "\n",
    "    # Output Configuration\n",
    "    OUTPUT_TABLE = f\"{DATA_CATALOG}.{DATA_SCHEMA}.prod_inference_{MODEL_TYPE}\"\n",
    "\n",
    "    # Batch Configuration\n",
    "    BATCH_SIZE = pipeline_cfg.get(\"inference\", {}).get(\"batch_size\", 100)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"‚úÖ Configuration loaded successfully!\")\n",
    "    print(f\"\\nüìä Configuration Details:\")\n",
    "    print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "    print(f\"   Endpoint Name: {ENDPOINT_NAME}\")\n",
    "    print(f\"   Input Table: {DATA_CATALOG}.{DATA_SCHEMA}.{INPUT_TABLE}\")\n",
    "    print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "    print(f\"   Features: {FEATURE_COLS}\")\n",
    "    print(f\"   Label: {LABEL_COL}\")\n",
    "    print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå ERROR: {e}\")\n",
    "    print(\"üí° Please ensure pipeline_config.yml exists in the same or dev_env directory.\")\n",
    "    dbutils.notebook.exit(\"CONFIG_NOT_FOUND\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading configuration: {e}\")\n",
    "    traceback.print_exc()\n",
    "    dbutils.notebook.exit(f\"CONFIG_ERROR: {e}\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ CLIENTS INITIALIZATION\n",
    "# =============================================================================\n",
    "try:\n",
    "    w = WorkspaceClient()\n",
    "    spark = SparkSession.builder.appName(f\"{MODEL_TYPE.upper()}_Inference\").getOrCreate()\n",
    "    print(\"\\n‚úÖ Workspace & Spark initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Initialization failed: {e}\")\n",
    "    dbutils.notebook.exit(f\"INIT_FAILED: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 1: VERIFY ENDPOINT STATUS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìã STEP 1: Verifying Endpoint Readiness\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üîç Endpoint: {ENDPOINT_NAME}\")\n",
    "\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.get(name=ENDPOINT_NAME)\n",
    "    \n",
    "    if hasattr(endpoint, 'state') and hasattr(endpoint.state, 'ready'):\n",
    "        if endpoint.state.ready:\n",
    "            print(f\"‚úÖ Endpoint '{ENDPOINT_NAME}' is READY\")\n",
    "            print(f\"   State: {endpoint.state}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Endpoint NOT fully ready\")\n",
    "            print(f\"   State: {endpoint.state}\")\n",
    "            print(f\"   Proceeding cautiously...\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Cannot determine endpoint state ‚Üí Proceeding cautiously\")\n",
    "        \n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Endpoint error: {e}\"\n",
    "    print(error_msg)\n",
    "    print(f\"\\nüí° Troubleshooting:\")\n",
    "    print(f\"   1. Verify endpoint exists: {ENDPOINT_NAME}\")\n",
    "    print(f\"   2. Run 07_create_serving_endpoint.py first\")\n",
    "    print(f\"   3. Check endpoint status in Databricks UI\")\n",
    "    traceback.print_exc()\n",
    "    dbutils.notebook.exit(f\"ENDPOINT_ERROR: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 2: LOAD INPUT DATA\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìã STEP 2: Loading Input Data\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üîç Table: {DATA_CATALOG}.{DATA_SCHEMA}.{INPUT_TABLE}\")\n",
    "\n",
    "try:\n",
    "    df_spark = spark.read.format(\"delta\").table(\n",
    "        f\"{DATA_CATALOG}.{DATA_SCHEMA}.{INPUT_TABLE}\"\n",
    "    )\n",
    "    df = df_spark.toPandas()\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded successfully\")\n",
    "    print(f\"   Total rows: {len(df):,}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Validate required columns\n",
    "    missing_features = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
    "    \n",
    "    # Extract label if available\n",
    "    y_true = df[LABEL_COL] if LABEL_COL in df.columns else None\n",
    "    if y_true is not None:\n",
    "        print(f\"   Label column '{LABEL_COL}' found ‚Üí Will calculate metrics\")\n",
    "    else:\n",
    "        print(f\"   Label column '{LABEL_COL}' not found ‚Üí No metrics\")\n",
    "    \n",
    "    print(f\"   Feature columns: {FEATURE_COLS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Data loading failed: {e}\"\n",
    "    print(error_msg)\n",
    "    print(f\"\\nüí° Troubleshooting:\")\n",
    "    print(f\"   1. Verify table exists: {DATA_CATALOG}.{DATA_SCHEMA}.{INPUT_TABLE}\")\n",
    "    print(f\"   2. Check feature columns in pipeline_config.yml\")\n",
    "    traceback.print_exc()\n",
    "    dbutils.notebook.exit(f\"DATA_LOAD_FAILED: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 3: BATCH INFERENCE\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìã STEP 3: Running Batch Inference\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "all_predictions = []\n",
    "num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "print(f\"üîÑ Processing {len(df)} rows in {num_batches} batch(es) of size {BATCH_SIZE}\")\n",
    "\n",
    "try:\n",
    "    for batch_idx in range(num_batches):\n",
    "        start = batch_idx * BATCH_SIZE\n",
    "        end = min((batch_idx + 1) * BATCH_SIZE, len(df))\n",
    "        batch_df = df.iloc[start:end][FEATURE_COLS]\n",
    "\n",
    "        # Call serving endpoint\n",
    "        response = w.serving_endpoints.query(\n",
    "            name=ENDPOINT_NAME,\n",
    "            dataframe_records=batch_df.to_dict(\"records\")\n",
    "        )\n",
    "\n",
    "        predictions = response.predictions\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "        print(f\"   ‚úì Batch {batch_idx+1}/{num_batches} complete ({len(predictions)} predictions)\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Inference complete!\")\n",
    "    print(f\"   Total predictions: {len(all_predictions):,}\")\n",
    "    print(f\"   Sample predictions: {all_predictions[:5]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Inference failed: {e}\"\n",
    "    print(error_msg)\n",
    "    traceback.print_exc()\n",
    "    dbutils.notebook.exit(f\"INFERENCE_FAILED: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 4: PREPARE RESULTS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìã STEP 4: Preparing Results\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Add predictions and metadata to dataframe\n",
    "df[\"predicted_price\"] = all_predictions\n",
    "df[\"prediction_timestamp\"] = datetime.now()\n",
    "df[\"endpoint_name\"] = ENDPOINT_NAME\n",
    "df[\"model_type\"] = MODEL_TYPE.upper()\n",
    "df[\"inference_method\"] = \"serving_endpoint\"\n",
    "df[\"prediction_date\"] = datetime.now().strftime('%Y-%m-%d')\n",
    "df[\"batch_id\"] = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "print(f\"‚úÖ Results prepared\")\n",
    "print(f\"   Rows with predictions: {len(df):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 5: SAVE RESULTS TO DELTA (AVOID DUPLICATES)\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìã STEP 5: Saving Results to Delta\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üîç Output table: {OUTPUT_TABLE}\")\n",
    "\n",
    "try:\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # Check for duplicates\n",
    "    try:\n",
    "        prev_df = spark.read.table(OUTPUT_TABLE).toPandas()\n",
    "        if len(prev_df) > 0:\n",
    "            last_pred_prev = prev_df[\"predicted_price\"].iloc[-1]\n",
    "            last_pred_new = df[\"predicted_price\"].iloc[-1]\n",
    "            \n",
    "            if math.isclose(last_pred_prev, last_pred_new, rel_tol=1e-6):\n",
    "                print(\"‚ÑπÔ∏è Duplicate batch detected ‚Äî skipping save\")\n",
    "                dbutils.notebook.exit(\"SKIPPED_DUPLICATE\")\n",
    "    except:\n",
    "        print(\"‚ÑπÔ∏è Output table does not exist ‚Üí Creating new one\")\n",
    "\n",
    "    # Write to Delta\n",
    "    spark_df.write.mode(\"append\")\\\n",
    "        .format(\"delta\")\\\n",
    "        .option(\"mergeSchema\", \"true\")\\\n",
    "        .saveAsTable(OUTPUT_TABLE)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved to {OUTPUT_TABLE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Save warning: {e}\")\n",
    "    print(\"   Continuing with metrics calculation...\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 6: CALCULATE PERFORMANCE METRICS (IF LABELS AVAILABLE)\n",
    "# =============================================================================\n",
    "if y_true is not None and len(y_true) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 6: Calculating Performance Metrics\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        y_pred = df[\"predicted_price\"]\n",
    "\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true==0, 1, y_true))) * 100\n",
    "        \n",
    "        from sklearn.metrics import r2_score\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"\\nüìä Production Inference Metrics:\")\n",
    "        print(f\"   MAE  : {mae:,.3f}\")\n",
    "        print(f\"   RMSE : {rmse:,.3f}\")\n",
    "        print(f\"   MAPE : {mape:.2f}%\")\n",
    "        print(f\"   R¬≤   : {r2:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Metrics calculation failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"\\n‚ÑπÔ∏è No ground truth labels available - skipping metrics\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ‚úÖ PRODUCTION INFERENCE COMPLETE ‚úÖ‚úÖ\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüìä Execution Summary:\")\n",
    "print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "print(f\"   Endpoint: {ENDPOINT_NAME}\")\n",
    "print(f\"   Input Table: {DATA_CATALOG}.{DATA_SCHEMA}.{INPUT_TABLE}\")\n",
    "print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "print(f\"   Rows Processed: {len(df):,}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Timestamp: {datetime.now()}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save for workflow\n",
    "try:\n",
    "    dbutils.jobs.taskValues.set(key=\"inference_rows\", value=len(df))\n",
    "    dbutils.jobs.taskValues.set(key=\"output_table\", value=OUTPUT_TABLE)\n",
    "    print(\"\\n‚úÖ Task values saved for workflow\")\n",
    "except:\n",
    "    print(\"\\n‚ÑπÔ∏è Not running in workflow - skipping task values\")\n",
    "\n",
    "dbutils.notebook.exit(\"SUCCESS\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
