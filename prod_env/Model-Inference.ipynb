{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ================================================================\n",
    "# üöÄ SMART PRODUCTION BATCH INFERENCE (VIA SERVING ENDPOINT)\n",
    "#    Auto-adaptive model type, dynamic endpoint + result table\n",
    "# ================================================================\n",
    "%pip install xgboost\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys, math\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ SMART PRODUCTION BATCH INFERENCE VIA SERVING ENDPOINT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "DATA_CATALOG = \"workspace\"\n",
    "DATA_SCHEMA = \"default\"\n",
    "INPUT_TABLE = \"house_price_delta\"\n",
    "\n",
    "# Default batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Metric thresholds (for quick performance checks)\n",
    "MAPE_THRESHOLD = 15.0\n",
    "R2_THRESHOLD = 0.75\n",
    "\n",
    "# =============================================================================\n",
    "# INITIALIZE CLIENTS\n",
    "# =============================================================================\n",
    "try:\n",
    "    w = WorkspaceClient()\n",
    "    print(\"‚úì Databricks Workspace Client initialized\")\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"SmartBatchInferenceProd\").getOrCreate()\n",
    "    print(\"‚úì Spark session initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing clients: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-DETECT MODEL TYPE & ENDPOINT NAME\n",
    "# =============================================================================\n",
    "try:\n",
    "    # Search latest model in UC (based on update time)\n",
    "    import mlflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    client = MlflowClient()\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    experiments = client.search_experiments(view_type=mlflow.entities.ViewType.ACTIVE_ONLY)\n",
    "    latest_exp = max(experiments, key=lambda exp: exp.last_update_time)\n",
    "    exp_name = latest_exp.name.lower()\n",
    "\n",
    "    if \"xgboost\" in exp_name:\n",
    "        model_type = \"xgboost\"\n",
    "    elif \"rf\" in exp_name or \"randomforest\" in exp_name:\n",
    "        model_type = \"rf\"\n",
    "    elif \"linear\" in exp_name:\n",
    "        model_type = \"linear\"\n",
    "    else:\n",
    "        model_type = \"generic\"\n",
    "\n",
    "    model_name = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_{model_type}_uc\"\n",
    "    ENDPOINT_NAME = f\"house-price-{model_type}-prod\"\n",
    "    OUTPUT_TABLE = f\"{DATA_CATALOG}.{DATA_SCHEMA}.prod_inference_{model_type}\"\n",
    "\n",
    "    print(f\"üìò Latest Experiment: {latest_exp.name}\")\n",
    "    print(f\"‚úÖ Detected Model Type: {model_type.upper()}\")\n",
    "    print(f\"‚úÖ Using Endpoint: {ENDPOINT_NAME}\")\n",
    "    print(f\"‚úÖ Output Table: {OUTPUT_TABLE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error detecting model type or endpoint: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK ENDPOINT STATUS\n",
    "# =============================================================================\n",
    "print(\"\\nüîç Checking endpoint readiness...\")\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.get(name=ENDPOINT_NAME)\n",
    "\n",
    "    if endpoint.state and \"READY\" in str(endpoint.state.ready):\n",
    "        print(f\"‚úÖ Endpoint '{ENDPOINT_NAME}' is READY\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Endpoint '{ENDPOINT_NAME}' may not be ready. Proceeding with caution...\")\n",
    "\n",
    "    if endpoint.config and endpoint.config.served_entities:\n",
    "        for entity in endpoint.config.served_entities:\n",
    "            print(f\"   ‚Ä¢ Model: {entity.entity_name} | Version: {entity.entity_version}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cannot access endpoint '{ENDPOINT_NAME}': {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD INPUT DATA\n",
    "# =============================================================================\n",
    "print(\"\\nüì¶ Loading input data...\")\n",
    "FULL_INPUT_TABLE = f\"{DATA_CATALOG}.{DATA_SCHEMA}.{INPUT_TABLE}\"\n",
    "\n",
    "try:\n",
    "    df_spark = spark.read.format(\"delta\").table(FULL_INPUT_TABLE)\n",
    "    df = df_spark.toPandas()\n",
    "    print(f\"‚úÖ Loaded {len(df)} records from {FULL_INPUT_TABLE}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load input Delta table: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if \"price\" not in df.columns:\n",
    "    print(\"‚ö†Ô∏è No 'price' column found ‚Äî proceeding with inference-only mode\")\n",
    "    y_true = None\n",
    "else:\n",
    "    y_true = df[\"price\"]\n",
    "\n",
    "# Infer feature columns automatically (exclude target and identifiers)\n",
    "FEATURE_COLUMNS = [c for c in df.columns if c not in [\"price\", \"id\", \"timestamp\"]]\n",
    "print(f\"üîç Using features: {', '.join(FEATURE_COLUMNS)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAKE PREDICTIONS VIA ENDPOINT\n",
    "# =============================================================================\n",
    "print(\"\\nüöÄ Performing batch inference via serving endpoint...\")\n",
    "all_predictions = []\n",
    "num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "try:\n",
    "    for batch_idx in range(num_batches):\n",
    "        start = batch_idx * BATCH_SIZE\n",
    "        end = min((batch_idx + 1) * BATCH_SIZE, len(df))\n",
    "        batch = df.iloc[start:end]\n",
    "        batch_records = batch[FEATURE_COLUMNS].to_dict('records')\n",
    "\n",
    "        # Query serving endpoint\n",
    "        response = w.serving_endpoints.query(name=ENDPOINT_NAME, dataframe_records=batch_records)\n",
    "\n",
    "        # Extract predictions\n",
    "        predictions = response.predictions if hasattr(response, 'predictions') else response\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "        if (batch_idx + 1) % 5 == 0 or batch_idx == num_batches - 1:\n",
    "            print(f\"   Processed {end}/{len(df)} samples...\")\n",
    "\n",
    "    df[\"predicted_price\"] = all_predictions\n",
    "    df[\"prediction_timestamp\"] = datetime.now()\n",
    "    df[\"endpoint_name\"] = ENDPOINT_NAME\n",
    "    df[\"inference_method\"] = \"serving_endpoint\"\n",
    "\n",
    "    print(f\"‚úÖ Predictions complete: {len(df)} rows\")\n",
    "    print(f\"   Range: {min(all_predictions):,.2f} - {max(all_predictions):,.2f}\")\n",
    "    print(f\"   Mean: {np.mean(all_predictions):,.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during inference: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS TO DELTA (with duplicate check)\n",
    "# =============================================================================\n",
    "print(\"\\nüíæ Saving predictions to Delta table...\")\n",
    "\n",
    "try:\n",
    "    prediction_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    batch_id = f\"{prediction_date}_{datetime.now().strftime('%H%M%S')}\"\n",
    "\n",
    "    df[\"prediction_date\"] = prediction_date\n",
    "    df[\"batch_id\"] = batch_id\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    table_exists = False\n",
    "    try:\n",
    "        existing_df = spark.read.table(OUTPUT_TABLE).toPandas()\n",
    "        table_exists = True\n",
    "        if not existing_df.empty:\n",
    "            last = existing_df.iloc[-1]\n",
    "            if math.isclose(last.predicted_price, df.iloc[-1].predicted_price, rel_tol=1e-6):\n",
    "                print(f\"‚ÑπÔ∏è Duplicate predictions detected, skipping save.\")\n",
    "                sys.exit(0)\n",
    "    except Exception:\n",
    "        print(f\"‚ÑπÔ∏è Table does not exist yet. Creating new...\")\n",
    "\n",
    "    mode = \"append\" if table_exists else \"overwrite\"\n",
    "    spark_df.write.mode(mode).format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "    print(f\"‚úÖ Saved predictions to {OUTPUT_TABLE} (mode={mode.upper()})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Save operation failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE MONITORING\n",
    "# =============================================================================\n",
    "if y_true is not None:\n",
    "    print(\"\\nüìä Evaluating model performance...\")\n",
    "    y_pred = df[\"predicted_price\"]\n",
    "\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "    print(f\"   MAE  : {mae:,.2f}\")\n",
    "    print(f\"   RMSE : {rmse:,.2f}\")\n",
    "    print(f\"   MAPE : {mape:.2f}%\")\n",
    "\n",
    "    if mape > MAPE_THRESHOLD:\n",
    "        print(f\"‚ö†Ô∏è WARNING: MAPE {mape:.2f}% exceeds {MAPE_THRESHOLD}% threshold!\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Model performance within acceptable range.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ PRODUCTION BATCH INFERENCE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model Type     : {model_type.upper()}\")\n",
    "print(f\"Endpoint Used  : {ENDPOINT_NAME}\")\n",
    "print(f\"Output Table   : {OUTPUT_TABLE}\")\n",
    "print(f\"Records        : {len(df)}\")\n",
    "print(f\"Timestamp      : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nBENEFITS:\")\n",
    "print(\"  ‚Ä¢ Unified model deployment (same as API)\")\n",
    "print(\"  ‚Ä¢ Environment consistency via UC endpoint\")\n",
    "print(\"  ‚Ä¢ Automatic endpoint + table detection\")\n",
    "print(\"  ‚Ä¢ Smart duplicate prevention\")\n",
    "print(\"  ‚Ä¢ Continuous production monitoring\\n\")\n",
    "\n",
    "try:\n",
    "    dbutils.notebook.exit(\"INFERENCE_SUCCESS\")\n",
    "except:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
