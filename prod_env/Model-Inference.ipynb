{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PRODUCTION BATCH INFERENCE VIA SERVING ENDPOINT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "ENDPOINT_NAME = \"house-price-prediction-prod\"\n",
    "\n",
    "DATA_CATALOG_NAME = \"workspace\"\n",
    "DATA_SCHEMA_NAME = \"default\"\n",
    "INPUT_TABLE_NAME = \"house_price_delta\"\n",
    "OUTPUT_TABLE_NAME = \"production_predictions\"\n",
    "\n",
    "FULL_INPUT_TABLE = f\"{DATA_CATALOG_NAME}.{DATA_SCHEMA_NAME}.{INPUT_TABLE_NAME}\"\n",
    "FULL_OUTPUT_TABLE = f\"{DATA_CATALOG_NAME}.{DATA_SCHEMA_NAME}.{OUTPUT_TABLE_NAME}\"\n",
    "\n",
    "FEATURE_COLUMNS = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "BATCH_SIZE = 100  # Process in batches to avoid API limits\n",
    "\n",
    "# =============================================================================\n",
    "# INITIALIZE CLIENTS\n",
    "# =============================================================================\n",
    "try:\n",
    "    w = WorkspaceClient()\n",
    "    print(\"Databricks Workspace Client initialized\")\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"BatchInferenceViaEndpoint\").getOrCreate()\n",
    "    print(\"Spark session initialized\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing clients: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK ENDPOINT STATUS\n",
    "# =============================================================================\n",
    "print(f\"\\nChecking endpoint status: {ENDPOINT_NAME}\")\n",
    "\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.get(name=ENDPOINT_NAME)\n",
    "    \n",
    "    if endpoint.state and endpoint.state.ready:\n",
    "        ready_status = str(endpoint.state.ready)\n",
    "        if \"READY\" in ready_status:\n",
    "            print(f\"  Endpoint is READY\")\n",
    "        else:\n",
    "            print(f\"  Warning: Endpoint status is {ready_status}\")\n",
    "            print(f\"  Proceeding anyway...\")\n",
    "    \n",
    "    # Get model info from endpoint\n",
    "    if endpoint.config and endpoint.config.served_entities:\n",
    "        for entity in endpoint.config.served_entities:\n",
    "            print(f\"  Model: {entity.entity_name}\")\n",
    "            print(f\"  Version: {entity.entity_version}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: Cannot access endpoint: {e}\")\n",
    "    print(f\"Ensure endpoint '{ENDPOINT_NAME}' exists and is ready\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD INPUT DATA\n",
    "# =============================================================================\n",
    "print(f\"\\nLoading data from: {FULL_INPUT_TABLE}\")\n",
    "\n",
    "try:\n",
    "    spark_df = spark.read.format(\"delta\").table(FULL_INPUT_TABLE)\n",
    "    row_count = spark_df.count()\n",
    "    print(f\"  Loaded {row_count} rows\")\n",
    "    \n",
    "    # Verify columns\n",
    "    missing_cols = [c for c in FEATURE_COLUMNS if c not in spark_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing columns: {missing_cols}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Select features\n",
    "    if 'price' in spark_df.columns:\n",
    "        pandas_df = spark_df.select(*FEATURE_COLUMNS, 'price').toPandas()\n",
    "    else:\n",
    "        pandas_df = spark_df.select(*FEATURE_COLUMNS).toPandas()\n",
    "    \n",
    "    print(f\"  Features: {', '.join(FEATURE_COLUMNS)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# MAKE PREDICTIONS VIA ENDPOINT\n",
    "# =============================================================================\n",
    "print(f\"\\nMaking predictions via serving endpoint...\")\n",
    "print(f\"  Processing {len(pandas_df)} samples in batches of {BATCH_SIZE}\")\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "try:\n",
    "    # Process in batches\n",
    "    num_batches = (len(pandas_df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = min((batch_idx + 1) * BATCH_SIZE, len(pandas_df))\n",
    "        \n",
    "        batch_df = pandas_df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Prepare batch data\n",
    "        batch_records = batch_df[FEATURE_COLUMNS].to_dict('records')\n",
    "        \n",
    "        # Call endpoint\n",
    "        response = w.serving_endpoints.query(\n",
    "            name=ENDPOINT_NAME,\n",
    "            dataframe_records=batch_records\n",
    "        )\n",
    "        \n",
    "        # Extract predictions\n",
    "        if hasattr(response, 'predictions'):\n",
    "            predictions = response.predictions\n",
    "        else:\n",
    "            # Response might be in different format\n",
    "            predictions = response\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        \n",
    "        if (batch_idx + 1) % 5 == 0 or batch_idx == num_batches - 1:\n",
    "            print(f\"  Processed {end_idx}/{len(pandas_df)} samples...\")\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    pandas_df['predicted_price'] = all_predictions\n",
    "    pandas_df['prediction_timestamp'] = datetime.now()\n",
    "    pandas_df['inference_method'] = 'serving_endpoint'\n",
    "    pandas_df['endpoint_name'] = ENDPOINT_NAME\n",
    "    \n",
    "    print(f\"  Predictions completed\")\n",
    "    print(f\"  Price Range: {min(all_predictions):,.2f} - {max(all_predictions):,.2f}\")\n",
    "    print(f\"  Mean Prediction: {np.mean(all_predictions):,.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE PREDICTIONS\n",
    "# =============================================================================\n",
    "print(f\"\\nSaving predictions to: {FULL_OUTPUT_TABLE}\")\n",
    "\n",
    "try:\n",
    "    prediction_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    prediction_datetime = datetime.now()\n",
    "    batch_id = f\"{prediction_date}_endpoint_{datetime.now().strftime('%H%M%S')}\"\n",
    "    \n",
    "    pandas_df['prediction_date'] = prediction_date\n",
    "    pandas_df['batch_id'] = batch_id\n",
    "    \n",
    "    print(f\"  Batch ID: {batch_id}\")\n",
    "    print(f\"  Prediction Date: {prediction_date}\")\n",
    "    \n",
    "    result_df = spark.createDataFrame(pandas_df)\n",
    "    \n",
    "    # Check if table exists\n",
    "    table_exists = False\n",
    "    try:\n",
    "        existing_table = spark.read.format(\"delta\").table(FULL_OUTPUT_TABLE)\n",
    "        table_exists = True\n",
    "        existing_count = existing_table.count()\n",
    "        print(f\"  Existing records: {existing_count}\")\n",
    "    except Exception:\n",
    "        print(f\"  Table doesn't exist - will create new\")\n",
    "    \n",
    "    # Save with append mode\n",
    "    if table_exists:\n",
    "        result_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(FULL_OUTPUT_TABLE)\n",
    "        print(f\"  Mode: APPEND\")\n",
    "    else:\n",
    "        result_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .partitionBy(\"prediction_date\") \\\n",
    "            .saveAsTable(FULL_OUTPUT_TABLE)\n",
    "        print(f\"  Mode: CREATE with partitioning\")\n",
    "    \n",
    "    print(f\"  Predictions saved successfully\")\n",
    "    print(f\"  Rows written: {len(pandas_df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: Save operation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY SAMPLE RESULTS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"SAMPLE PREDICTIONS (First 10 rows)\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "sample = pandas_df[[*FEATURE_COLUMNS, 'predicted_price']].head(10).copy()\n",
    "sample['predicted_price'] = sample['predicted_price'].apply(lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "print(sample.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE MONITORING\n",
    "# =============================================================================\n",
    "if 'price' in pandas_df.columns:\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"PRODUCTION PERFORMANCE MONITORING\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    actual = pandas_df['price']\n",
    "    pred = pandas_df['predicted_price']\n",
    "    \n",
    "    mae = abs(actual - pred).mean()\n",
    "    mape = (abs(actual - pred) / actual * 100).mean()\n",
    "    rmse_calc = np.sqrt(((actual - pred) ** 2).mean())\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  MAE: {mae:,.2f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    print(f\"  RMSE: {rmse_calc:,.2f}\")\n",
    "    \n",
    "    if mape > 15.0:\n",
    "        print(f\"\\n  WARNING: Model performance degraded!\")\n",
    "        print(f\"  MAPE {mape:.2f}% exceeds 15% threshold\")\n",
    "    else:\n",
    "        print(f\"\\n  Model performance is within acceptable range\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"BATCH INFERENCE COMPLETE (VIA SERVING ENDPOINT)\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"Predictions Made: {len(all_predictions)} rows\")\n",
    "print(f\"Endpoint: {ENDPOINT_NAME}\")\n",
    "print(f\"Output Table: {FULL_OUTPUT_TABLE}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "print(f\"\\nBENEFITS OF USING SERVING ENDPOINT:\")\n",
    "print(f\"  - No sklearn version mismatch issues\")\n",
    "print(f\"  - Consistent environment (isolated)\")\n",
    "print(f\"  - Same model used by API and batch\")\n",
    "print(f\"  - Easier to monitor and debug\")\n",
    "\n",
    "try:\n",
    "    dbutils.notebook.exit(\"INFERENCE_SUCCESS\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
