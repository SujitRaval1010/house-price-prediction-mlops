{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82154add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ================================================================\n",
    "# üöÄ SMART PRODUCTION BATCH INFERENCE (SERVING ENDPOINT VERSION)\n",
    "#    - Auto-detects model type\n",
    "#    - Uses UC-based production serving endpoint\n",
    "#    - Saves predictions to Delta with duplicate protection\n",
    "# ================================================================\n",
    "\n",
    "%pip install xgboost\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ SMART PRODUCTION BATCH INFERENCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "DATA_CATALOG = \"workspace\"\n",
    "DATA_SCHEMA = \"default\"\n",
    "INPUT_TABLE = \"house_price_delta\"\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "MAPE_THRESHOLD = 15.0\n",
    "R2_THRESHOLD = 0.75\n",
    "\n",
    "# =============================================================================\n",
    "# INIT CLIENTS\n",
    "# =============================================================================\n",
    "try:\n",
    "    w = WorkspaceClient()\n",
    "    spark = SparkSession.builder.appName(\"ProdInference\").getOrCreate()\n",
    "    print(\"‚úì Workspace & Spark initialized\")\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"INIT_FAILED: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DETECT MODEL TYPE + ENDPOINT\n",
    "# =============================================================================\n",
    "try:\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "\n",
    "    experiments = client.search_experiments(view_type=mlflow.entities.ViewType.ACTIVE_ONLY)\n",
    "    latest_exp = max(experiments, key=lambda exp: exp.last_update_time)\n",
    "    exp_name = latest_exp.name.lower()\n",
    "\n",
    "    if \"xgboost\" in exp_name:\n",
    "        model_type = \"xgboost\"\n",
    "    elif \"rf\" in exp_name or \"randomforest\" in exp_name:\n",
    "        model_type = \"rf\"\n",
    "    elif \"linear\" in exp_name:\n",
    "        model_type = \"linear\"\n",
    "    else:\n",
    "        model_type = \"generic\"\n",
    "\n",
    "    model_name = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_{model_type}_uc\"\n",
    "    ENDPOINT_NAME = f\"house-price-{model_type}-prod\"\n",
    "    OUTPUT_TABLE = f\"{DATA_CATALOG}.{DATA_SCHEMA}.prod_inference_{model_type}\"\n",
    "\n",
    "    print(f\"üìò Model Type     : {model_type.upper()}\")\n",
    "    print(f\"‚úÖ Using Endpoint : {ENDPOINT_NAME}\")\n",
    "    print(f\"‚úÖ Output Table   : {OUTPUT_TABLE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"MODEL_DETECTION_FAILED: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFY ENDPOINT STATUS\n",
    "# =============================================================================\n",
    "print(\"\\nüîç Checking endpoint readiness...\")\n",
    "\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.get(name=ENDPOINT_NAME)\n",
    "\n",
    "    if endpoint.state.ready:\n",
    "        print(f\"‚úÖ Endpoint '{ENDPOINT_NAME}' is READY\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Endpoint NOT fully ready ‚Üí Proceeding cautiously\")\n",
    "\n",
    "    for m in endpoint.config.served_entities:\n",
    "        print(f\"   ‚Ä¢ {m.entity_name} ‚Üí Version {m.entity_version}\")\n",
    "\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"ENDPOINT_ERROR: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD INPUT DATA\n",
    "# =============================================================================\n",
    "print(\"\\nüì¶ Loading input data...\")\n",
    "\n",
    "try:\n",
    "    df_spark = spark.read.format(\"delta\").table(f\"{DATA_CATALOG}.{DATA_SCHEMA}.{INPUT_TABLE}\")\n",
    "    df = df_spark.toPandas()\n",
    "    print(f\"‚úÖ Loaded {len(df)} records\")\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"DATA_LOAD_FAILED: {e}\")\n",
    "\n",
    "y_true = df[\"price\"] if \"price\" in df.columns else None\n",
    "\n",
    "FEATURE_COLUMNS = [c for c in df.columns if c not in [\"price\", \"id\", \"timestamp\"]]\n",
    "\n",
    "print(f\"üîç Feature Columns: {FEATURE_COLUMNS}\")\n",
    "\n",
    "# =============================================================================\n",
    "# BATCH INFERENCE\n",
    "# =============================================================================\n",
    "print(\"\\nüöÄ Performing inference...\")\n",
    "\n",
    "all_predictions = []\n",
    "num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "try:\n",
    "    for batch_idx in range(num_batches):\n",
    "        start = batch_idx * BATCH_SIZE\n",
    "        end = min((batch_idx + 1) * BATCH_SIZE, len(df))\n",
    "        batch_df = df.iloc[start:end][FEATURE_COLUMNS]\n",
    "\n",
    "        response = w.serving_endpoints.query(\n",
    "            name=ENDPOINT_NAME,\n",
    "            dataframe_records=batch_df.to_dict(\"records\")\n",
    "        )\n",
    "\n",
    "        predictions = response.predictions\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "        print(f\"   ‚Üí Batch {batch_idx+1}/{num_batches} complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"INFERENCE_FAILED: {e}\")\n",
    "\n",
    "df[\"predicted_price\"] = all_predictions\n",
    "df[\"prediction_timestamp\"] = datetime.now()\n",
    "df[\"endpoint_name\"] = ENDPOINT_NAME\n",
    "df[\"inference_method\"] = \"serving_endpoint\"\n",
    "\n",
    "print(f\"‚úÖ Generated {len(all_predictions)} predictions\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS TO DELTA (AVOID DUPLICATES)\n",
    "# =============================================================================\n",
    "print(\"\\nüíæ Saving predictions...\")\n",
    "\n",
    "try:\n",
    "    df[\"prediction_date\"] = datetime.now().strftime('%Y-%m-%d')\n",
    "    df[\"batch_id\"] = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # Duplicate check on last prediction row\n",
    "    try:\n",
    "        prev_df = spark.read.table(OUTPUT_TABLE).toPandas()\n",
    "        last_pred_prev = prev_df[\"predicted_price\"].iloc[-1]\n",
    "        last_pred_new = df[\"predicted_price\"].iloc[-1]\n",
    "\n",
    "        if math.isclose(last_pred_prev, last_pred_new, rel_tol=1e-6):\n",
    "            print(\"‚ÑπÔ∏è Duplicate batch detected ‚Äî skipping save\")\n",
    "            dbutils.notebook.exit(\"SKIPPED_DUPLICATE\")\n",
    "    except:\n",
    "        print(\"‚ÑπÔ∏è Output table does not exist ‚Üí Creating new one\")\n",
    "\n",
    "    spark_df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "    print(f\"‚úÖ Saved to {OUTPUT_TABLE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"SAVE_FAILED: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE METRICS\n",
    "# =============================================================================\n",
    "if y_true is not None:\n",
    "    print(\"\\nüìä Evaluating model performance...\")\n",
    "    y_pred = df[\"predicted_price\"]\n",
    "\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    print(f\"   MAE  : {mae:.3f}\")\n",
    "    print(f\"   RMSE : {rmse:.3f}\")\n",
    "    print(f\"   MAPE : {mape:.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ PRODUCTION INFERENCE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model Type     : {model_type.upper()}\")\n",
    "print(f\"Endpoint Used  : {ENDPOINT_NAME}\")\n",
    "print(f\"Output Table   : {OUTPUT_TABLE}\")\n",
    "print(f\"Rows Processed : {len(df)}\")\n",
    "print(f\"Timestamp      : {datetime.now()}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dbutils.notebook.exit(\"SUCCESS\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
