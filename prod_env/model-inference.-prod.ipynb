{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82154add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ================================================================\n",
    "# ðŸš€ PRODUCTION BATCH INFERENCE (XGBOOST ONLY)\n",
    "# ================================================================\n",
    "\n",
    "%pip install xgboost\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸš€ PRODUCTION BATCH INFERENCE (XGBOOST)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "DATA_CATALOG = \"workspace\"\n",
    "DATA_SCHEMA = \"default\"\n",
    "INPUT_TABLE = \"house_price_delta\"\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "OUTPUT_TABLE = f\"{DATA_CATALOG}.{DATA_SCHEMA}.prod_inference_xgboost\"\n",
    "ENDPOINT_NAME = \"house-price-xgboost-prod\"\n",
    "\n",
    "# =============================================================================\n",
    "# INIT CLIENTS\n",
    "# =============================================================================\n",
    "try:\n",
    "    w = WorkspaceClient()\n",
    "    spark = SparkSession.builder.appName(\"XGBoostInference\").getOrCreate()\n",
    "    print(\"âœ“ Workspace & Spark initialized\")\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"INIT_FAILED: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFY ENDPOINT STATUS\n",
    "# =============================================================================\n",
    "print(\"\\nðŸ” Checking endpoint readiness...\")\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.get(name=ENDPOINT_NAME)\n",
    "    if getattr(endpoint.state, \"ready\", False):\n",
    "        print(f\"âœ… Endpoint '{ENDPOINT_NAME}' is READY\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Endpoint NOT fully ready â†’ Proceeding cautiously\")\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"ENDPOINT_ERROR: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD INPUT DATA\n",
    "# =============================================================================\n",
    "print(\"\\nðŸ“¦ Loading input data...\")\n",
    "try:\n",
    "    df_spark = spark.read.format(\"delta\").table(f\"{DATA_CATALOG}.{DATA_SCHEMA}.{INPUT_TABLE}\")\n",
    "    df = df_spark.toPandas()\n",
    "    print(f\"âœ… Loaded {len(df)} records\")\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"DATA_LOAD_FAILED: {e}\")\n",
    "\n",
    "y_true = df[\"price\"] if \"price\" in df.columns else None\n",
    "FEATURE_COLUMNS = [c for c in df.columns if c not in [\"price\", \"id\", \"timestamp\"]]\n",
    "print(f\"ðŸ” Feature Columns: {FEATURE_COLUMNS}\")\n",
    "\n",
    "# =============================================================================\n",
    "# BATCH INFERENCE\n",
    "# =============================================================================\n",
    "print(\"\\nðŸš€ Performing inference...\")\n",
    "all_predictions = []\n",
    "num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "try:\n",
    "    for batch_idx in range(num_batches):\n",
    "        start = batch_idx * BATCH_SIZE\n",
    "        end = min((batch_idx + 1) * BATCH_SIZE, len(df))\n",
    "        batch_df = df.iloc[start:end][FEATURE_COLUMNS]\n",
    "\n",
    "        response = w.serving_endpoints.query(\n",
    "            name=ENDPOINT_NAME,\n",
    "            dataframe_records=batch_df.to_dict(\"records\")\n",
    "        )\n",
    "\n",
    "        predictions = response.predictions\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "        print(f\"   â†’ Batch {batch_idx+1}/{num_batches} complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"INFERENCE_FAILED: {e}\")\n",
    "\n",
    "df[\"predicted_price\"] = all_predictions\n",
    "df[\"prediction_timestamp\"] = datetime.now()\n",
    "df[\"endpoint_name\"] = ENDPOINT_NAME\n",
    "df[\"inference_method\"] = \"serving_endpoint\"\n",
    "print(f\"âœ… Generated {len(all_predictions)} predictions\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS TO DELTA (AVOID DUPLICATES)\n",
    "# =============================================================================\n",
    "print(\"\\nðŸ’¾ Saving predictions...\")\n",
    "try:\n",
    "    df[\"prediction_date\"] = datetime.now().strftime('%Y-%m-%d')\n",
    "    df[\"batch_id\"] = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # Duplicate check\n",
    "    try:\n",
    "        prev_df = spark.read.table(OUTPUT_TABLE).toPandas()\n",
    "        if len(prev_df) > 0:\n",
    "            last_pred_prev = prev_df[\"predicted_price\"].iloc[-1]\n",
    "            last_pred_new = df[\"predicted_price\"].iloc[-1]\n",
    "            if math.isclose(last_pred_prev, last_pred_new, rel_tol=1e-6):\n",
    "                print(\"â„¹ï¸ Duplicate batch detected â€” skipping save\")\n",
    "                dbutils.notebook.exit(\"SKIPPED_DUPLICATE\")\n",
    "    except:\n",
    "        print(\"â„¹ï¸ Output table does not exist â†’ Creating new one\")\n",
    "\n",
    "    spark_df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "    print(f\"âœ… Saved to {OUTPUT_TABLE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"SAVE_FAILED: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE METRICS\n",
    "# =============================================================================\n",
    "if y_true is not None and len(y_true) > 0:\n",
    "    print(\"\\nðŸ“Š Evaluating model performance...\")\n",
    "    y_pred = df[\"predicted_price\"]\n",
    "\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true==0, 1, y_true))) * 100\n",
    "\n",
    "    print(f\"   MAE  : {mae:.3f}\")\n",
    "    print(f\"   RMSE : {rmse:.3f}\")\n",
    "    print(f\"   MAPE : {mape:.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ PRODUCTION INFERENCE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model Type     : XGBOOST\")\n",
    "print(f\"Endpoint Used  : {ENDPOINT_NAME}\")\n",
    "print(f\"Output Table   : {OUTPUT_TABLE}\")\n",
    "print(f\"Rows Processed : {len(df)}\")\n",
    "print(f\"Timestamp      : {datetime.now()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dbutils.notebook.exit(\"SUCCESS\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
