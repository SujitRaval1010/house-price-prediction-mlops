{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"UAT MODEL INFERENCE - STAGING VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# Unity Catalog Model Details\n",
    "UC_CATALOG_NAME = \"workspace\"\n",
    "UC_SCHEMA_NAME = \"ml\"\n",
    "MODEL_NAME = f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.house_price_model_uc\"\n",
    "\n",
    "# Data Configuration\n",
    "DATA_CATALOG_NAME = \"workspace\"\n",
    "DATA_SCHEMA_NAME = \"default\"\n",
    "TABLE_NAME = \"house_price_delta\"\n",
    "FULL_TABLE_NAME = f\"{DATA_CATALOG_NAME}.{DATA_SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "# MLflow Experiment\n",
    "EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Delta_RF\"\n",
    "\n",
    "# Feature Configuration\n",
    "FEATURE_COLUMNS = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "LABEL_COLUMN = 'price'\n",
    "\n",
    "# UAT Validation Thresholds\n",
    "MAX_ACCEPTABLE_MAPE = 15.0  # Maximum 15% error\n",
    "MIN_ACCEPTABLE_R2 = 0.75     # Minimum R2 score\n",
    "\n",
    "# =============================================================================\n",
    "# SPARK SESSION INITIALIZATION\n",
    "# =============================================================================\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"UAT_ModelInference\").getOrCreate()\n",
    "    print(\"Spark session initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Spark: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# GET MODEL ALIAS FROM WIDGET\n",
    "# =============================================================================\n",
    "try:\n",
    "    model_alias = dbutils.widgets.get(\"alias\")\n",
    "    print(f\"Model Alias from widget: {model_alias}\")\n",
    "except:\n",
    "    model_alias = \"Staging\"\n",
    "    print(f\"Widget not found, using default: {model_alias}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD MODEL FROM MLFLOW RUN (Community Edition Compatible)\n",
    "# =============================================================================\n",
    "print(f\"\\nLoading model for UAT validation...\")\n",
    "print(f\"Target Alias: {model_alias}\")\n",
    "\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    # Get experiment\n",
    "    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if not experiment:\n",
    "        print(f\"Error: Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"Experiment: {experiment.name}\")\n",
    "    \n",
    "    # Get latest successful run\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string=\"status = 'FINISHED'\",\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=1\n",
    "    )\n",
    "    \n",
    "    if not runs:\n",
    "        print(\"Error: No successful runs found\")\n",
    "        print(\"Please run the training script first\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    latest_run = runs[0]\n",
    "    run_id = latest_run.info.run_id\n",
    "    \n",
    "    print(f\"\\nModel Details:\")\n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(f\"  Run Name: {latest_run.info.run_name}\")\n",
    "    \n",
    "    # Display training parameters\n",
    "    print(f\"\\n  Training Parameters:\")\n",
    "    for key, value in latest_run.data.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Display training metrics\n",
    "    print(f\"\\n  Training Metrics:\")\n",
    "    training_metrics = {}\n",
    "    for key, value in latest_run.data.metrics.items():\n",
    "        print(f\"    {key}: {value:.4f}\")\n",
    "        training_metrics[key] = value\n",
    "    \n",
    "    # Load model from run\n",
    "    model_uri = f\"runs:/{run_id}/sklearn_rf_model\"\n",
    "    print(f\"\\nLoading model from: {model_uri}\")\n",
    "    \n",
    "    model = mlflow.sklearn.load_model(model_uri)\n",
    "    print(\"Model loaded successfully for UAT validation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD TEST DATA FROM DELTA TABLE\n",
    "# =============================================================================\n",
    "print(f\"\\nLoading test data from: {FULL_TABLE_NAME}\")\n",
    "\n",
    "try:\n",
    "    # Load data from Delta table\n",
    "    spark_df = spark.read.format(\"delta\").table(FULL_TABLE_NAME)\n",
    "    row_count = spark_df.count()\n",
    "    \n",
    "    print(f\"Data loaded: {row_count} rows\")\n",
    "    \n",
    "    # Verify required columns exist\n",
    "    available_columns = spark_df.columns\n",
    "    missing_columns = [col for col in FEATURE_COLUMNS + [LABEL_COLUMN] \n",
    "                      if col not in available_columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"\\nError: Missing columns: {missing_columns}\")\n",
    "        print(f\"Available columns: {available_columns}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"All required columns present\")\n",
    "    \n",
    "    # Convert to Pandas for inference\n",
    "    pandas_df = spark_df.select(*FEATURE_COLUMNS, LABEL_COLUMN).toPandas()\n",
    "    print(f\"Converted to Pandas: {pandas_df.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading data: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# MAKE PREDICTIONS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"RUNNING INFERENCE ON UAT DATA\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "try:\n",
    "    # Extract features and labels\n",
    "    X_test = pandas_df[FEATURE_COLUMNS]\n",
    "    y_actual = pandas_df[LABEL_COLUMN]\n",
    "    \n",
    "    # Make predictions\n",
    "    y_predicted = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nPredictions completed: {len(y_predicted)} samples\")\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    pandas_df['predicted_price'] = y_predicted\n",
    "    pandas_df['prediction_error'] = y_actual - y_predicted\n",
    "    pandas_df['absolute_error'] = abs(pandas_df['prediction_error'])\n",
    "    pandas_df['percentage_error'] = (pandas_df['absolute_error'] / y_actual) * 100\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during prediction: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# CALCULATE PERFORMANCE METRICS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"UAT VALIDATION METRICS\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "try:\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_actual, y_predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    r2 = r2_score(y_actual, y_predicted)\n",
    "    mape = (abs(y_actual - y_predicted) / y_actual * 100).mean()\n",
    "    \n",
    "    # Additional statistics\n",
    "    median_error = pandas_df['absolute_error'].median()\n",
    "    max_error = pandas_df['absolute_error'].max()\n",
    "    min_error = pandas_df['absolute_error'].min()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nRegression Metrics:\")\n",
    "    print(f\"  Mean Absolute Error (MAE):  ${mae:,.2f}\")\n",
    "    print(f\"  Root Mean Squared Error:     ${rmse:,.2f}\")\n",
    "    print(f\"  R² Score:                    {r2:.4f}\")\n",
    "    print(f\"  Mean Absolute % Error:       {mape:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nError Statistics:\")\n",
    "    print(f\"  Median Absolute Error:       ${median_error:,.2f}\")\n",
    "    print(f\"  Maximum Error:               ${max_error:,.2f}\")\n",
    "    print(f\"  Minimum Error:               ${min_error:,.2f}\")\n",
    "    \n",
    "    print(f\"\\nPrediction Statistics:\")\n",
    "    print(f\"  Actual Price Range:   ${y_actual.min():,.2f} - ${y_actual.max():,.2f}\")\n",
    "    print(f\"  Predicted Range:      ${y_predicted.min():,.2f} - ${y_predicted.max():,.2f}\")\n",
    "    print(f\"  Mean Actual Price:    ${y_actual.mean():,.2f}\")\n",
    "    print(f\"  Mean Predicted Price: ${y_predicted.mean():,.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError calculating metrics: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# UAT VALIDATION - PASS/FAIL CRITERIA\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"UAT VALIDATION RESULTS\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "validation_passed = True\n",
    "validation_results = []\n",
    "\n",
    "# Check MAPE threshold\n",
    "if mape <= MAX_ACCEPTABLE_MAPE:\n",
    "    validation_results.append(f\"PASS: MAPE {mape:.2f}% <= {MAX_ACCEPTABLE_MAPE}%\")\n",
    "else:\n",
    "    validation_results.append(f\"FAIL: MAPE {mape:.2f}% > {MAX_ACCEPTABLE_MAPE}%\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check R² threshold\n",
    "if r2 >= MIN_ACCEPTABLE_R2:\n",
    "    validation_results.append(f\"PASS: R² {r2:.4f} >= {MIN_ACCEPTABLE_R2}\")\n",
    "else:\n",
    "    validation_results.append(f\"FAIL: R² {r2:.4f} < {MIN_ACCEPTABLE_R2}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Print validation results\n",
    "print(f\"\\nValidation Criteria:\")\n",
    "for result in validation_results:\n",
    "    status = result.split(\":\")[0]\n",
    "    if status == \"PASS\":\n",
    "        print(f\"  {result}\")\n",
    "    else:\n",
    "        print(f\"  {result}\")\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "if validation_passed:\n",
    "    print(\"UAT VALIDATION: PASSED\")\n",
    "    print(\"Model is ready for promotion to Production\")\n",
    "else:\n",
    "    print(\"UAT VALIDATION: FAILED\")\n",
    "    print(\"Model does not meet quality thresholds\")\n",
    "    print(\"Please retrain with better parameters or more data\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY SAMPLE PREDICTIONS\n",
    "# =============================================================================\n",
    "print(f\"\\nSample Predictions (First 10 rows):\")\n",
    "\n",
    "sample_df = pandas_df[[*FEATURE_COLUMNS, LABEL_COLUMN, 'predicted_price', \n",
    "                        'absolute_error', 'percentage_error']].head(10).copy()\n",
    "\n",
    "# Round values for better readability (without $ sign)\n",
    "sample_df['price'] = sample_df['price'].round(2)\n",
    "sample_df['predicted_price'] = sample_df['predicted_price'].round(2)\n",
    "sample_df['absolute_error'] = sample_df['absolute_error'].round(2)\n",
    "sample_df['percentage_error'] = sample_df['percentage_error'].round(2)\n",
    "\n",
    "print(sample_df.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# INTELLIGENT SAVE - PERFORMANCE-BASED DEDUPLICATION CHECK\n",
    "# =============================================================================\n",
    "SAVE_RESULTS = True\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    import hashlib\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    output_table = f\"{DATA_CATALOG_NAME}.{DATA_SCHEMA_NAME}.uat_predictions_{model_alias.lower()}\"\n",
    "    \n",
    "    print(f\"\\nChecking if results need to be saved...\")\n",
    "    \n",
    "    try:\n",
    "        # Create unique fingerprint INCLUDING PERFORMANCE METRICS\n",
    "        fingerprint_data = {\n",
    "            'model_params': {\n",
    "                'best_n_estimators': latest_run.data.params.get('best_n_estimators'),\n",
    "                'best_max_depth': latest_run.data.params.get('best_max_depth'),\n",
    "                'best_min_samples_split': latest_run.data.params.get('best_min_samples_split'),\n",
    "                'best_min_samples_leaf': latest_run.data.params.get('best_min_samples_leaf')\n",
    "            },\n",
    "            'training_metrics': {\n",
    "                'test_rmse': round(training_metrics.get('test_rmse', 0), 2),\n",
    "                'test_r2_score': round(training_metrics.get('test_r2_score', 0), 4),\n",
    "                'best_cv_rmse': round(training_metrics.get('best_cv_rmse', 0), 2)\n",
    "            },\n",
    "            'uat_metrics': {\n",
    "                'rmse': round(rmse, 2),\n",
    "                'r2': round(r2, 4),\n",
    "                'mae': round(mae, 2),\n",
    "                'mape': round(mape, 2)\n",
    "            },\n",
    "            'data_size': len(pandas_df),\n",
    "            'feature_columns': sorted(FEATURE_COLUMNS),\n",
    "            'alias': model_alias,\n",
    "            'run_id': run_id\n",
    "        }\n",
    "        \n",
    "        # Generate hash\n",
    "        fingerprint_str = json.dumps(fingerprint_data, sort_keys=True)\n",
    "        current_fingerprint = hashlib.md5(fingerprint_str.encode()).hexdigest()\n",
    "        \n",
    "        print(f\"Current Run Fingerprint: {current_fingerprint}\")\n",
    "        print(f\"\\n  Model Parameters:\")\n",
    "        print(f\"    n_estimators: {fingerprint_data['model_params']['best_n_estimators']}\")\n",
    "        print(f\"    max_depth: {fingerprint_data['model_params']['best_max_depth']}\")\n",
    "        print(f\"    min_samples_split: {fingerprint_data['model_params']['best_min_samples_split']}\")\n",
    "        print(f\"    min_samples_leaf: {fingerprint_data['model_params']['best_min_samples_leaf']}\")\n",
    "        print(f\"\\n  Training Metrics:\")\n",
    "        print(f\"    Test RMSE: {fingerprint_data['training_metrics']['test_rmse']}\")\n",
    "        print(f\"    Test R²: {fingerprint_data['training_metrics']['test_r2_score']}\")\n",
    "        print(f\"    CV RMSE: {fingerprint_data['training_metrics']['best_cv_rmse']}\")\n",
    "        print(f\"\\n  UAT Performance:\")\n",
    "        print(f\"    RMSE: {fingerprint_data['uat_metrics']['rmse']}\")\n",
    "        print(f\"    R²: {fingerprint_data['uat_metrics']['r2']}\")\n",
    "        print(f\"    MAE: {fingerprint_data['uat_metrics']['mae']}\")\n",
    "        print(f\"    MAPE: {fingerprint_data['uat_metrics']['mape']}%\")\n",
    "        print(f\"\\n  Data Size: {fingerprint_data['data_size']} rows\")\n",
    "        print(f\"  Run ID: {run_id}\")\n",
    "        \n",
    "        # Check if table exists and has previous fingerprint\n",
    "        should_save = True\n",
    "        change_reason = \"Initial save\"\n",
    "        \n",
    "        try:\n",
    "            existing_table = spark.read.format(\"delta\").table(output_table)\n",
    "            \n",
    "            if 'run_fingerprint' in existing_table.columns:\n",
    "                # Get last saved fingerprint\n",
    "                last_row = existing_table.orderBy(col('saved_timestamp').desc()).first()\n",
    "                \n",
    "                if last_row and last_row['run_fingerprint'] == current_fingerprint:\n",
    "                    should_save = False\n",
    "                    print(f\"\\n{'='*70}\")\n",
    "                    print(f\"SKIPPING SAVE: Identical results already exist\")\n",
    "                    print(f\"{'='*70}\")\n",
    "                    print(f\"  Previous fingerprint matches current run\")\n",
    "                    print(f\"  Model parameters AND performance metrics are unchanged\")\n",
    "                    print(f\"  Last saved: {last_row['saved_timestamp']}\")\n",
    "                else:\n",
    "                    change_reason = \"Performance or parameters changed\"\n",
    "                    print(f\"\\n{'='*70}\")\n",
    "                    print(f\"DETECTED CHANGE: New results differ from previous save\")\n",
    "                    print(f\"{'='*70}\")\n",
    "                    print(f\"  Previous fingerprint: {last_row['run_fingerprint']}\")\n",
    "                    print(f\"  Current fingerprint:  {current_fingerprint}\")\n",
    "                    print(f\"  Last saved: {last_row['saved_timestamp']}\")\n",
    "                    print(f\"\\n  Reason: Model performance or parameters have changed\")\n",
    "                    \n",
    "                    # Show what changed\n",
    "                    if last_row['uat_rmse'] != round(rmse, 2):\n",
    "                        print(f\"    UAT RMSE: {last_row['uat_rmse']} → {round(rmse, 2)}\")\n",
    "                    if last_row['uat_r2'] != round(r2, 4):\n",
    "                        print(f\"    UAT R²: {last_row['uat_r2']} → {round(r2, 4)}\")\n",
    "            else:\n",
    "                change_reason = \"Adding fingerprint tracking\"\n",
    "                print(f\"\\nTable exists but no fingerprint column - will save with fingerprint tracking\")\n",
    "                \n",
    "        except Exception as table_check_error:\n",
    "            # Table doesn't exist yet\n",
    "            change_reason = \"Creating new table\"\n",
    "            print(f\"\\nTable does not exist yet - will create with fingerprint tracking\")\n",
    "        \n",
    "        # Save only if different from previous run\n",
    "        if should_save:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"SAVING NEW RESULTS\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Table: {output_table}\")\n",
    "            print(f\"Reason: {change_reason}\")\n",
    "            \n",
    "            # Add metadata columns\n",
    "            pandas_df['run_fingerprint'] = current_fingerprint\n",
    "            pandas_df['run_id'] = run_id\n",
    "            pandas_df['saved_timestamp'] = datetime.now()\n",
    "            pandas_df['model_alias'] = model_alias\n",
    "            \n",
    "            # Save all parameters\n",
    "            pandas_df['best_n_estimators'] = int(latest_run.data.params.get('best_n_estimators', 0))\n",
    "            pandas_df['best_max_depth'] = int(latest_run.data.params.get('best_max_depth', 0))\n",
    "            pandas_df['best_min_samples_split'] = int(latest_run.data.params.get('best_min_samples_split', 2))\n",
    "            pandas_df['best_min_samples_leaf'] = int(latest_run.data.params.get('best_min_samples_leaf', 1))\n",
    "            \n",
    "            # Save training metrics\n",
    "            pandas_df['training_test_rmse'] = training_metrics.get('test_rmse', 0)\n",
    "            pandas_df['training_test_r2'] = training_metrics.get('test_r2_score', 0)\n",
    "            pandas_df['training_cv_rmse'] = training_metrics.get('best_cv_rmse', 0)\n",
    "            \n",
    "            # Save UAT metrics\n",
    "            pandas_df['uat_rmse'] = round(rmse, 2)\n",
    "            pandas_df['uat_r2'] = round(r2, 4)\n",
    "            pandas_df['uat_mae'] = round(mae, 2)\n",
    "            pandas_df['uat_mape'] = round(mape, 2)\n",
    "            \n",
    "            pandas_df['validation_status'] = 'PASSED' if validation_passed else 'FAILED'\n",
    "            pandas_df['change_reason'] = change_reason\n",
    "            \n",
    "            # Convert to Spark DataFrame\n",
    "            result_spark_df = spark.createDataFrame(pandas_df)\n",
    "            \n",
    "            # Save to Delta table (overwrite to keep latest)\n",
    "            result_spark_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(output_table)\n",
    "            \n",
    "            print(f\"\\n✅ UAT results saved successfully!\")\n",
    "            print(f\"  Run ID: {run_id}\")\n",
    "            print(f\"  Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"  Records: {len(pandas_df)}\")\n",
    "            print(f\"  Fingerprint: {current_fingerprint}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Warning: Could not save results: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY IN DATABRICKS (if available)\n",
    "# =============================================================================\n",
    "try:\n",
    "    result_spark_df = spark.createDataFrame(pandas_df)\n",
    "    display(result_spark_df)\n",
    "except NameError:\n",
    "    print(\"\\nNote: display() not available outside Databricks notebook\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXIT WITH APPROPRIATE CODE\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"UAT INFERENCE COMPLETE\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"\\nValidation failed - Model needs improvement\")\n",
    "    # In Databricks notebook, use dbutils instead of sys.exit\n",
    "    try:\n",
    "        dbutils.notebook.exit(\"FAILED\")\n",
    "    except:\n",
    "        raise Exception(\"UAT Validation Failed: Model does not meet quality thresholds\")\n",
    "else:\n",
    "    print(\"\\nModel validated successfully for promotion to Production\")\n",
    "    # Success exit\n",
    "    try:\n",
    "        dbutils.notebook.exit(\"PASSED\")\n",
    "    except:\n",
    "        pass  # In non-notebook environment, just continue"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
