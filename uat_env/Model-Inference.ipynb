{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import re # New import for regex\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"UAT MODEL INFERENCE - STAGING VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# Unity Catalog Model Details\n",
    "UC_CATALOG_NAME = \"workspace\"\n",
    "UC_SCHEMA_NAME = \"ml\"\n",
    "MODEL_NAME = f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.house_price_model_uc\"\n",
    "\n",
    "# Data Configuration\n",
    "DATA_CATALOG_NAME = \"workspace\"\n",
    "DATA_SCHEMA_NAME = \"default\"\n",
    "TABLE_NAME = \"house_price_delta\"\n",
    "FULL_TABLE_NAME = f\"{DATA_CATALOG_NAME}.{DATA_SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "# MLflow Experiment\n",
    "EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Delta_RF\"\n",
    "\n",
    "# Feature Configuration\n",
    "FEATURE_COLUMNS = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "LABEL_COLUMN = 'price'\n",
    "\n",
    "# UAT Validation Thresholds\n",
    "MAX_ACCEPTABLE_MAPE = 15.0 \t# Maximum 15% error\n",
    "MIN_ACCEPTABLE_R2 = 0.75 \t# Minimum R2 score\n",
    "\n",
    "# =============================================================================\n",
    "# SPARK SESSION INITIALIZATION\n",
    "# =============================================================================\n",
    "try:\n",
    "\tspark = SparkSession.builder.appName(\"UAT_ModelInference\").getOrCreate()\n",
    "\tprint(\"Spark session initialized\")\n",
    "except Exception as e:\n",
    "\tprint(f\"Error initializing Spark: {e}\")\n",
    "\tsys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# GET MODEL ALIAS FROM WIDGET\n",
    "# =============================================================================\n",
    "try:\n",
    "\tmodel_alias = dbutils.widgets.get(\"alias\")\n",
    "\tprint(f\"Model Alias from widget: {model_alias}\")\n",
    "except:\n",
    "\tmodel_alias = \"Staging\"\n",
    "\tprint(f\"Widget not found, using default: {model_alias}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD MODEL FROM MLFLOW RUN (Community Edition Compatible)\n",
    "# =============================================================================\n",
    "print(f\"\\nLoading model for UAT validation...\")\n",
    "print(f\"Target Alias: {model_alias}\")\n",
    "\n",
    "try:\n",
    "\tclient = MlflowClient()\n",
    "\t\n",
    "\t# Get experiment\n",
    "\texperiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\tif not experiment:\n",
    "\t\tprint(f\"Error: Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\t\tsys.exit(1)\n",
    "\t\n",
    "\tprint(f\"Experiment: {experiment.name}\")\n",
    "\t\n",
    "\t# Get latest successful run\n",
    "\truns = client.search_runs(\n",
    "\t\texperiment_ids=[experiment.experiment_id],\n",
    "\t\tfilter_string=\"status = 'FINISHED'\",\n",
    "\t\torder_by=[\"start_time DESC\"],\n",
    "\t\tmax_results=1\n",
    "\t)\n",
    "\t\n",
    "\tif not runs:\n",
    "\t\tprint(\"Error: No successful runs found\")\n",
    "\t\tprint(\"Please run the training script first\")\n",
    "\t\tsys.exit(1)\n",
    "\t\n",
    "\tlatest_run = runs[0]\n",
    "\trun_id = latest_run.info.run_id\n",
    "\t\n",
    "\tprint(f\"\\nModel Details:\")\n",
    "\tprint(f\" \tRun ID: {run_id}\")\n",
    "\tprint(f\" \tRun Name: {latest_run.info.run_name}\")\n",
    "\t\n",
    "\t# Display training parameters\n",
    "\tprint(f\"\\n \tTraining Parameters:\")\n",
    "\tfor key, value in latest_run.data.params.items():\n",
    "\t\tprint(f\" \t \t{key}: {value}\")\n",
    "\t\n",
    "\t# Display training metrics\n",
    "\tprint(f\"\\n \tTraining Metrics:\")\n",
    "\ttraining_metrics = {}\n",
    "\tfor key, value in latest_run.data.metrics.items():\n",
    "\t\tprint(f\" \t \t{key}: {value:.4f}\")\n",
    "\t\ttraining_metrics[key] = value\n",
    "\t\n",
    "\t# Load model from run\n",
    "\tmodel_uri = f\"runs:/{run_id}/sklearn_rf_model\"\n",
    "\tprint(f\"\\nLoading model from: {model_uri}\")\n",
    "\t\n",
    "\tmodel = mlflow.sklearn.load_model(model_uri)\n",
    "\tprint(\"Model loaded successfully for UAT validation\")\n",
    "\t\n",
    "except Exception as e:\n",
    "\tprint(f\"\\nError loading model: {e}\")\n",
    "\timport traceback\n",
    "\ttraceback.print_exc()\n",
    "\tsys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD TEST DATA FROM DELTA TABLE\n",
    "# =============================================================================\n",
    "print(f\"\\nLoading test data from: {FULL_TABLE_NAME}\")\n",
    "\n",
    "try:\n",
    "\t# Load data from Delta table\n",
    "\tspark_df = spark.read.format(\"delta\").table(FULL_TABLE_NAME)\n",
    "\trow_count = spark_df.count()\n",
    "\t\n",
    "\tprint(f\"Data loaded: {row_count} rows\")\n",
    "\t\n",
    "\t# Verify required columns exist\n",
    "\tavailable_columns = spark_df.columns\n",
    "\tmissing_columns = [col for col in FEATURE_COLUMNS + [LABEL_COLUMN] \n",
    "\t\t\t\t\t  if col not in available_columns]\n",
    "\t\n",
    "\tif missing_columns:\n",
    "\t\tprint(f\"\\nError: Missing columns: {missing_columns}\")\n",
    "\t\tprint(f\"Available columns: {available_columns}\")\n",
    "\t\tsys.exit(1)\n",
    "\t\n",
    "\tprint(\"All required columns present\")\n",
    "\t\n",
    "\t# Convert to Pandas for inference\n",
    "\tpandas_df = spark_df.select(*FEATURE_COLUMNS, LABEL_COLUMN).toPandas()\n",
    "\tprint(f\"Converted to Pandas: {pandas_df.shape}\")\n",
    "\t\n",
    "except Exception as e:\n",
    "\tprint(f\"\\nError loading data: {e}\")\n",
    "\tsys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# MAKE PREDICTIONS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"RUNNING INFERENCE ON UAT DATA\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "try:\n",
    "\t# Extract features and labels\n",
    "\tX_test = pandas_df[FEATURE_COLUMNS]\n",
    "\ty_actual = pandas_df[LABEL_COLUMN]\n",
    "\t\n",
    "\t# Make predictions\n",
    "\ty_predicted = model.predict(X_test)\n",
    "\t\n",
    "\tprint(f\"\\nPredictions completed: {len(y_predicted)} samples\")\n",
    "\t\n",
    "\t# Add predictions to dataframe\n",
    "\tpandas_df['predicted_price'] = y_predicted\n",
    "\tpandas_df['prediction_error'] = y_actual - y_predicted\n",
    "\tpandas_df['absolute_error'] = abs(pandas_df['prediction_error'])\n",
    "\tpandas_df['percentage_error'] = (pandas_df['absolute_error'] / y_actual) * 100\n",
    "\t\n",
    "except Exception as e:\n",
    "\tprint(f\"\\nError during prediction: {e}\")\n",
    "\timport traceback\n",
    "\ttraceback.print_exc()\n",
    "\tsys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# CALCULATE PERFORMANCE METRICS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"UAT VALIDATION METRICS\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "try:\n",
    "\t# Calculate metrics\n",
    "\tmae = mean_absolute_error(y_actual, y_predicted)\n",
    "\trmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\tr2 = r2_score(y_actual, y_predicted)\n",
    "\tmape = (abs(y_actual - y_predicted) / y_actual * 100).mean()\n",
    "\t\n",
    "\t# Additional statistics\n",
    "\tmedian_error = pandas_df['absolute_error'].median()\n",
    "\tmax_error = pandas_df['absolute_error'].max()\n",
    "\tmin_error = pandas_df['absolute_error'].min()\n",
    "\t\n",
    "\t# Print metrics\n",
    "\tprint(f\"\\nRegression Metrics:\")\n",
    "\tprint(f\" \tMean Absolute Error (MAE): \t${mae:,.2f}\")\n",
    "\tprint(f\" \tRoot Mean Squared Error: \t \t ${rmse:,.2f}\")\n",
    "\tprint(f\" \tRÂ² Score: \t \t \t \t \t \t{r2:.4f}\")\n",
    "\tprint(f\" \tMean Absolute % Error: \t \t \t {mape:.2f}%\")\n",
    "\t\n",
    "\tprint(f\"\\nError Statistics:\")\n",
    "\tprint(f\" \tMedian Absolute Error: \t \t \t ${median_error:,.2f}\")\n",
    "\tprint(f\" \tMaximum Error: \t \t \t \t \t ${max_error:,.2f}\")\n",
    "\tprint(f\" \tMinimum Error: \t \t \t \t \t ${min_error:,.2f}\")\n",
    "\t\n",
    "\tprint(f\"\\nPrediction Statistics:\")\n",
    "\tprint(f\" \tActual Price Range: \t ${y_actual.min():,.2f} - ${y_actual.max():,.2f}\")\n",
    "\tprint(f\" \tPredicted Range: \t \t${y_predicted.min():,.2f} - ${y_predicted.max():,.2f}\")\n",
    "\tprint(f\" \tMean Actual Price: \t \t${y_actual.mean():,.2f}\")\n",
    "\tprint(f\" \tMean Predicted Price: ${y_predicted.mean():,.2f}\")\n",
    "\t\n",
    "except Exception as e:\n",
    "\tprint(f\"\\nError calculating metrics: {e}\")\n",
    "\tsys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# UAT VALIDATION - PASS/FAIL CRITERIA\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"UAT VALIDATION RESULTS\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "validation_passed = True\n",
    "validation_results = []\n",
    "\n",
    "# Check MAPE threshold\n",
    "if mape <= MAX_ACCEPTABLE_MAPE:\n",
    "\tvalidation_results.append(f\"PASS: MAPE {mape:.2f}% <= {MAX_ACCEPTABLE_MAPE}%\")\n",
    "else:\n",
    "\tvalidation_results.append(f\"FAIL: MAPE {mape:.2f}% > {MAX_ACCEPTABLE_MAPE}%\")\n",
    "\tvalidation_passed = False\n",
    "\n",
    "# Check RÂ² threshold\n",
    "if r2 >= MIN_ACCEPTABLE_R2:\n",
    "\tvalidation_results.append(f\"PASS: RÂ² {r2:.4f} >= {MIN_ACCEPTABLE_R2}\")\n",
    "else:\n",
    "\tvalidation_results.append(f\"FAIL: RÂ² {r2:.4f} < {MIN_ACCEPTABLE_R2}\")\n",
    "\tvalidation_passed = False\n",
    "\n",
    "# Print validation results\n",
    "print(f\"\\nValidation Criteria:\")\n",
    "for result in validation_results:\n",
    "\tstatus = result.split(\":\")[0]\n",
    "\tif status == \"PASS\":\n",
    "\t\tprint(f\" \t{result}\")\n",
    "\telse:\n",
    "\t\tprint(f\" \t{result}\")\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "if validation_passed:\n",
    "\tprint(\"UAT VALIDATION: PASSED\")\n",
    "\tprint(\"Model is ready for promotion to Production\")\n",
    "else:\n",
    "\tprint(\"UAT VALIDATION: FAILED\")\n",
    "\tprint(\"Model does not meet quality thresholds\")\n",
    "\tprint(\"Please retrain with better parameters or more data\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY SAMPLE PREDICTIONS\n",
    "# =============================================================================\n",
    "print(f\"\\nSample Predictions (First 10 rows):\")\n",
    "\n",
    "sample_df = pandas_df[[*FEATURE_COLUMNS, LABEL_COLUMN, 'predicted_price', \n",
    "\t\t\t\t\t\t'absolute_error', 'percentage_error']].head(10).copy()\n",
    "\n",
    "# Round values for better readability (without $ sign)\n",
    "sample_df['price'] = sample_df['price'].round(2)\n",
    "sample_df['predicted_price'] = sample_df['predicted_price'].round(2)\n",
    "sample_df['absolute_error'] = sample_df['absolute_error'].round(2)\n",
    "sample_df['percentage_error'] = sample_df['percentage_error'].round(2)\n",
    "\n",
    "print(sample_df.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# INTELLIGENT SAVE - PERFORMANCE-BASED DEDUPLICATION CHECK (MODIFIED LOGIC)\n",
    "# =============================================================================\n",
    "SAVE_RESULTS = True\n",
    "\n",
    "# FIX: Base table name for versioning (e.g., uat_predictions_staging)\n",
    "BASE_TABLE_NAME = f\"uat_predictions_{model_alias.lower()}\"\n",
    "BASE_OUTPUT_TABLE_PATH = f\"{DATA_CATALOG_NAME}.{DATA_SCHEMA_NAME}\"\n",
    "\n",
    "# Helper function to find the latest table version and its fingerprint\n",
    "def get_latest_version_info(spark, base_name, catalog, schema):\n",
    "    \"\"\"Finds the highest numbered table and its last saved fingerprint.\"\"\"\n",
    "    try:\n",
    "        # Use SQL to list relevant tables that follow the naming convention (base_name_X)\n",
    "        # We also check the unversioned table (base_name) as the first version\n",
    "        table_list_df = spark.sql(f\"SHOW TABLES IN {catalog}.{schema} LIKE '{base_name}%'\").toPandas()\n",
    "        \n",
    "        latest_version = 0\n",
    "        latest_table_name = f\"{catalog}.{schema}.{base_name}\" # Default to unversioned table\n",
    "        latest_fingerprint = None\n",
    "        \n",
    "        # Dictionary to store {version: full_table_name}\n",
    "        version_map = {0: latest_table_name}\n",
    "        \n",
    "        # Parse existing tables for version numbers\n",
    "        for _, row in table_list_df.iterrows():\n",
    "            table_name = row['tableName']\n",
    "            \n",
    "            # Check for pattern BASE_TABLE_NAME_X\n",
    "            match = re.match(rf\"^{base_name}_(\\d+)$\", table_name, re.IGNORECASE)\n",
    "            if match:\n",
    "                version = int(match.group(1))\n",
    "                version_map[version] = f\"{catalog}.{schema}.{table_name}\"\n",
    "                if version > latest_version:\n",
    "                    latest_version = version\n",
    "                    \n",
    "        # Update latest_table_name to the highest found version (0 is unversioned base)\n",
    "        if latest_version > 0:\n",
    "            latest_table_name = version_map[latest_version]\n",
    "        \n",
    "        # Now, attempt to read the latest table to get its fingerprint\n",
    "        if latest_table_name:\n",
    "            try:\n",
    "                # Get the latest run's fingerprint and other UAT metrics from the existing table\n",
    "                latest_table_df = spark.read.format(\"delta\").table(latest_table_name) \\\n",
    "                    .select(col('run_fingerprint')) \\\n",
    "                    .orderBy(col('saved_timestamp').desc()) \\\n",
    "                    .limit(1).collect()\n",
    "                \n",
    "                if latest_table_df:\n",
    "                    latest_fingerprint = latest_table_df[0]['run_fingerprint']\n",
    "                    \n",
    "            except Exception as read_error:\n",
    "                # Table exists but cannot be read or lacks fingerprint column\n",
    "                print(f\"Warning: Could not read fingerprint from {latest_table_name}: {read_error}\")\n",
    "\n",
    "        return latest_version, latest_table_name, latest_fingerprint\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing tables in {catalog}.{schema}: {e}\")\n",
    "        # Fallback if listing fails, assume base table is version 0\n",
    "        return 0, f\"{catalog}.{schema}.{base_name}\", None \n",
    "\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "\timport hashlib\n",
    "\timport json\n",
    "\tfrom datetime import datetime\n",
    "\t\n",
    "\t\n",
    "\tprint(f\"\\nChecking if results need to be saved...\")\n",
    "\t\n",
    "\ttry:\n",
    "\t\t# 1. Create unique fingerprint\n",
    "\t\tfingerprint_data = {\n",
    "\t\t\t'model_params': {\n",
    "\t\t\t\t'best_n_estimators': latest_run.data.params.get('best_n_estimators'),\n",
    "\t\t\t\t'best_max_depth': latest_run.data.params.get('best_max_depth'),\n",
    "\t\t\t\t'best_min_samples_split': latest_run.data.params.get('best_min_samples_split'),\n",
    "\t\t\t\t'best_min_samples_leaf': latest_run.data.params.get('best_min_samples_leaf')\n",
    "\t\t\t},\n",
    "\t\t\t'training_metrics': {\n",
    "\t\t\t\t'test_rmse': round(training_metrics.get('test_rmse', 0), 2),\n",
    "\t\t\t\t'test_r2_score': round(training_metrics.get('test_r2_score', 0), 4),\n",
    "\t\t\t\t'best_cv_rmse': round(training_metrics.get('best_cv_rmse', 0), 2)\n",
    "\t\t\t},\n",
    "\t\t\t'uat_metrics': {\n",
    "\t\t\t\t'rmse': round(rmse, 2),\n",
    "\t\t\t\t'r2': round(r2, 4),\n",
    "\t\t\t\t'mae': round(mae, 2),\n",
    "\t\t\t\t'mape': round(mape, 2)\n",
    "\t\t\t},\n",
    "\t\t\t'data_size': len(pandas_df),\n",
    "\t\t\t'feature_columns': sorted(FEATURE_COLUMNS),\n",
    "\t\t\t'alias': model_alias,\n",
    "\t\t\t'run_id': run_id\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t# Generate hash\n",
    "\t\tfingerprint_str = json.dumps(fingerprint_data, sort_keys=True)\n",
    "\t\tcurrent_fingerprint = hashlib.md5(fingerprint_str.encode()).hexdigest()\n",
    "\t\t\n",
    "\t\tprint(f\"Current Run Fingerprint: {current_fingerprint}\")\n",
    "\t\t\n",
    "\t\t# 2. Check existing tables and get saving decision\n",
    "\t\tlatest_version, latest_table_name, latest_fingerprint = get_latest_version_info(\n",
    "\t\t\tspark, BASE_TABLE_NAME, DATA_CATALOG_NAME, DATA_SCHEMA_NAME\n",
    "\t\t)\n",
    "\n",
    "\t\tprint(f\"Latest existing table found: {latest_table_name} (Version {latest_version})\")\n",
    "\t\tprint(f\"Latest saved fingerprint: {latest_fingerprint}\")\n",
    "\n",
    "\t\t# Default saving mode and table\n",
    "\t\tsave_mode = \"overwrite\"\n",
    "\t\toutput_table = latest_table_name\n",
    "\t\tchange_reason = \"\"\n",
    "\t\t\n",
    "\t\t# Decision Logic\n",
    "\t\tif latest_fingerprint is None:\n",
    "\t\t\t# Case 1: No previous table/fingerprint found (first run). Use base table name (version 0).\n",
    "\t\t\toutput_table = f\"{BASE_OUTPUT_TABLE_PATH}.{BASE_TABLE_NAME}_1\" # Start at version 1 for clarity\n",
    "\t\t\tsave_mode = \"append\" # Use append for initial creation\n",
    "\t\t\tchange_reason = \"Initial save: creating version 1\"\n",
    "\t\t\tprint(\"\\nACTION: Creating first version table.\")\n",
    "\n",
    "\t\telif current_fingerprint == latest_fingerprint:\n",
    "\t\t\t# Case 2: Fingerprint matches (Performance/Parameters SAME) -> Overwrite existing latest table\n",
    "\t\t\toutput_table = latest_table_name\n",
    "\t\t\tsave_mode = \"overwrite\"\n",
    "\t\t\tchange_reason = f\"Identical performance/params. Overwriting existing table {latest_version}.\"\n",
    "\t\t\tprint(f\"\\nACTION: Overwriting table {latest_version}. No functional change detected.\")\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\t# Case 3: Fingerprint mismatch (Performance/Parameters CHANGED) -> Create new table version\n",
    "\t\t\tnew_version = latest_version + 1\n",
    "\t\t\toutput_table = f\"{BASE_OUTPUT_TABLE_PATH}.{BASE_TABLE_NAME}_{new_version}\"\n",
    "\t\t\tsave_mode = \"append\" # Use append for initial creation\n",
    "\t\t\tchange_reason = f\"Performance or parameters changed. Creating new table version {new_version}.\"\n",
    "\t\t\tprint(f\"\\nACTION: Creating new version table ({new_version}). Change detected.\")\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\tprint(f\"\\n{'='*70}\")\n",
    "\t\tprint(f\"SAVING RESULTS\")\n",
    "\t\tprint(f\"{'='*70}\")\n",
    "\t\tprint(f\"Target Table: {output_table}\")\n",
    "\t\tprint(f\"Save Mode: {save_mode}\")\n",
    "\t\tprint(f\"Reason: {change_reason}\")\n",
    "\t\t\n",
    "\t\t# 3. Save Data\n",
    "\t\t# Add metadata columns\n",
    "\t\tpandas_df['run_fingerprint'] = current_fingerprint\n",
    "\t\tpandas_df['run_id'] = run_id\n",
    "\t\tpandas_df['saved_timestamp'] = datetime.now()\n",
    "\t\tpandas_df['model_alias'] = model_alias\n",
    "\t\t\n",
    "\t\t# Save all parameters\n",
    "\t\tpandas_df['best_n_estimators'] = int(latest_run.data.params.get('best_n_estimators', 0))\n",
    "\t\tpandas_df['best_max_depth'] = int(latest_run.data.params.get('best_max_depth', 0))\n",
    "\t\tpandas_df['best_min_samples_split'] = int(latest_run.data.params.get('best_min_samples_split', 2))\n",
    "\t\tpandas_df['best_min_samples_leaf'] = int(latest_run.data.params.get('best_min_samples_leaf', 1))\n",
    "\t\t\n",
    "\t\t# Save training metrics\n",
    "\t\tpandas_df['training_test_rmse'] = training_metrics.get('test_rmse', 0)\n",
    "\t\tpandas_df['training_test_r2'] = training_metrics.get('test_r2_score', 0)\n",
    "\t\tpandas_df['training_cv_rmse'] = training_metrics.get('best_cv_rmse', 0)\n",
    "\t\t\n",
    "\t\t# Save UAT metrics\n",
    "\t\tpandas_df['uat_rmse'] = round(rmse, 2)\n",
    "\t\tpandas_df['uat_r2'] = round(r2, 4)\n",
    "\t\tpandas_df['uat_mae'] = round(mae, 2)\n",
    "\t\tpandas_df['uat_mape'] = round(mape, 2)\n",
    "\t\t\n",
    "\t\tpandas_df['validation_status'] = 'PASSED' if validation_passed else 'FAILED'\n",
    "\t\tpandas_df['change_reason'] = change_reason\n",
    "\t\t\n",
    "\t\t# Convert to Spark DataFrame\n",
    "\t\tresult_spark_df = spark.createDataFrame(pandas_df)\n",
    "\t\t\n",
    "\t\t# Save to Delta table \n",
    "\t\t# If save_mode is 'append', it creates the table if it doesn't exist.\n",
    "\t\t# If save_mode is 'overwrite', it overwrites the existing table.\n",
    "\t\tresult_spark_df.write \\\n",
    "\t\t\t.format(\"delta\") \\\n",
    "\t\t\t.mode(save_mode) \\\n",
    "\t\t\t.option(\"overwriteSchema\", \"true\") \\\n",
    "\t\t\t.saveAsTable(output_table)\n",
    "\t\t\n",
    "\t\tprint(f\"\\nâ UAT results saved successfully!\")\n",
    "\t\tprint(f\" \tRun ID: {run_id}\")\n",
    "\t\tprint(f\" \tTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\t\tprint(f\" \tRecords: {len(pandas_df)}\")\n",
    "\t\tprint(f\" \tFingerprint: {current_fingerprint}\")\n",
    "\t\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"\\nâ ï¸ Warning: Could not save results: {e}\")\n",
    "\t\timport traceback\n",
    "\t\ttraceback.print_exc()\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY IN DATABRICKS (if available)\n",
    "# =============================================================================\n",
    "try:\n",
    "\tresult_spark_df = spark.createDataFrame(pandas_df)\n",
    "\tdisplay(result_spark_df)\n",
    "except NameError:\n",
    "\tprint(\"\\nNote: display() not available outside Databricks notebook\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXIT WITH APPROPRIATE CODE\n",
    "# =============================================================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"UAT INFERENCE COMPLETE\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "if not validation_passed:\n",
    "\tprint(\"\\nValidation failed - Model needs improvement\")\n",
    "\t# In Databricks notebook, use dbutils instead of sys.exit\n",
    "\ttry:\n",
    "\t\tdbutils.notebook.exit(\"FAILED\")\n",
    "\texcept:\n",
    "\t\traise Exception(\"UAT Validation Failed: Model does not meet quality thresholds\")\n",
    "else:\n",
    "\tprint(\"\\nModel validated successfully for promotion to Production\")\n",
    "\t# Success exit\n",
    "\ttry:\n",
    "\t\tdbutils.notebook.exit(\"PASSED\")\n",
    "\texcept:\n",
    "\t\tpass \t# In non-notebook environment, just continue\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
