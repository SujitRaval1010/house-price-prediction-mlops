{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================\n",
    "# üöÄ UAT MODEL INFERENCE ‚Äì SMART VERSIONED UC STAGING HANDLER\n",
    "# =============================================================\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================\n",
    "UC_CATALOG_NAME = \"workspace\"\n",
    "UC_SCHEMA_NAME = \"ml\"\n",
    "DELTA_INPUT_TABLE = \"workspace.default.house_price_delta\"\n",
    "\n",
    "# Metric thresholds\n",
    "MAPE_THRESHOLD = 15.0\n",
    "R2_THRESHOLD = 0.75\n",
    "\n",
    "# =============================================================\n",
    "# INITIALIZATION\n",
    "# =============================================================\n",
    "spark = SparkSession.builder.appName(\"UAT_Model_Inference_Auto\").getOrCreate()\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ UAT MODEL INFERENCE ‚Äì SMART VERSIONED UC STAGING HANDLER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================\n",
    "# 1Ô∏è‚É£ Auto-detect latest finished experiment\n",
    "# =============================================================\n",
    "def get_latest_experiment(client):\n",
    "    experiments = client.search_experiments(view_type=mlflow.entities.ViewType.ACTIVE_ONLY)\n",
    "    latest_exp = max(experiments, key=lambda exp: exp.last_update_time)\n",
    "    print(f\"üìò Latest Finished Experiment: {latest_exp.name}\")\n",
    "    return latest_exp\n",
    "\n",
    "# =============================================================\n",
    "# 2Ô∏è‚É£ Infer model type and UC model name\n",
    "# =============================================================\n",
    "def infer_model_name(exp_name: str):\n",
    "    exp_lower = exp_name.lower()\n",
    "    if \"xgboost\" in exp_lower:\n",
    "        model_type = \"xgboost\"\n",
    "    elif \"rf\" in exp_lower or \"randomforest\" in exp_lower:\n",
    "        model_type = \"rf\"\n",
    "    elif \"linear\" in exp_lower:\n",
    "        model_type = \"linear\"\n",
    "    else:\n",
    "        model_type = \"generic\"\n",
    "    model_name = f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.house_price_{model_type}_uc\"\n",
    "    print(f\"‚úÖ Detected Model Type: {model_type.upper()}\")\n",
    "    print(f\"‚úÖ Using Registered Model: {model_name}\")\n",
    "    return model_name, model_type\n",
    "\n",
    "# =============================================================\n",
    "# 3Ô∏è‚É£ Load latest staging model from UC\n",
    "# =============================================================\n",
    "def load_staging_model(client, model_name):\n",
    "    model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "    # Filter versions that have alias 'staging' (UC-specific)\n",
    "    staging_versions = []\n",
    "    for v in model_versions:\n",
    "        full_version = client.get_model_version(model_name, v.version)\n",
    "        aliases = [a.lower() for a in full_version.aliases] if full_version.aliases else []\n",
    "        if \"staging\" in aliases:\n",
    "            staging_versions.append(full_version)\n",
    "\n",
    "    if not staging_versions:\n",
    "        raise ValueError(f\"‚ùå No model with alias 'staging' found for {model_name}\")\n",
    "\n",
    "    version = max(staging_versions, key=lambda x: int(x.version)).version\n",
    "    print(f\"‚è≥ Loading model version {version} from alias 'staging'...\")\n",
    "    model_uri = f\"models:/{model_name}@staging\"\n",
    "    model = mlflow.pyfunc.load_model(model_uri)\n",
    "    print(f\"‚úÖ Model version {version} loaded successfully.\")\n",
    "    return model, version\n",
    "\n",
    "# =============================================================\n",
    "# 4Ô∏è‚É£ Run inference\n",
    "# =============================================================\n",
    "def run_inference(model):\n",
    "    print(\"üèÅ Loading UAT input Delta table for inference...\")\n",
    "    \n",
    "    # Try to load table directly\n",
    "    try:\n",
    "        df_spark = spark.table(DELTA_INPUT_TABLE)\n",
    "        df = df_spark.toPandas()\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in error_msg or \"cannot be found\" in error_msg:\n",
    "            raise ValueError(\n",
    "                f\"‚ùå Delta table '{DELTA_INPUT_TABLE}' does not exist.\\n\"\n",
    "                f\"   Please create the table first or verify the table name.\\n\"\n",
    "                f\"   Expected format: catalog.schema.table_name\\n\"\n",
    "                f\"   Original error: {error_msg}\"\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    if \"price\" not in df.columns:\n",
    "        raise ValueError(\"‚ùå Input Delta table must contain 'price' column as target.\")\n",
    "\n",
    "    X = df.drop(columns=[\"price\"])\n",
    "    y_true = df[\"price\"]\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    print(\"‚úÖ Inference completed successfully.\")\n",
    "    return df, y_true, y_pred\n",
    "\n",
    "# =============================================================\n",
    "# 5Ô∏è‚É£ Evaluate metrics\n",
    "# =============================================================\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f\"\\nüìä Evaluation Metrics:\")\n",
    "    print(f\"   ‚Ä¢ MAE  : {mae:.3f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE : {rmse:.3f}\")\n",
    "    print(f\"   ‚Ä¢ R¬≤   : {r2:.3f}\")\n",
    "    print(f\"   ‚Ä¢ MAPE : {mape:.2f}%\")\n",
    "    return mae, rmse, r2, mape\n",
    "\n",
    "# =============================================================\n",
    "# 6Ô∏è‚É£ Compare thresholds and log result\n",
    "# =============================================================\n",
    "def evaluate_uat_status(mape, r2):\n",
    "    if mape <= MAPE_THRESHOLD and r2 >= R2_THRESHOLD:\n",
    "        status = \"PASSED\"\n",
    "        print(\"‚úÖ UAT VALIDATION: PASSED (within thresholds)\")\n",
    "    else:\n",
    "        status = \"FAILED\"\n",
    "        print(\"‚ùå UAT VALIDATION: FAILED (outside thresholds)\")\n",
    "    return status\n",
    "\n",
    "# =============================================================\n",
    "# 7Ô∏è‚É£ Log results to Delta (avoid creating duplicate tables)\n",
    "# =============================================================\n",
    "def log_results_to_delta(model_name, model_version, mae, rmse, r2, mape, status):\n",
    "    output_table = f\"workspace.default.uat_inference_{model_name.split('.')[-1]}\"\n",
    "    result_df = pd.DataFrame([{\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"model_name\": model_name,\n",
    "        \"model_version\": int(model_version),\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"mape\": mape,\n",
    "        \"uat_status\": status\n",
    "    }])\n",
    "    \n",
    "    # Check if table exists (Spark Connect compatible)\n",
    "    try:\n",
    "        existing_df = spark.table(output_table).toPandas()\n",
    "        # Compare last row metrics to avoid duplicate logging\n",
    "        if not existing_df.empty:\n",
    "            last_row = existing_df.iloc[-1]\n",
    "            if (math.isclose(last_row.mae, mae, rel_tol=1e-6) and\n",
    "                math.isclose(last_row.rmse, rmse, rel_tol=1e-6) and\n",
    "                math.isclose(last_row.r2, r2, rel_tol=1e-6) and\n",
    "                math.isclose(last_row.mape, mape, rel_tol=1e-6)):\n",
    "                print(f\"‚ÑπÔ∏è Metrics unchanged. Skipping write to {output_table}\")\n",
    "                return\n",
    "    except Exception:\n",
    "        # Table doesn't exist, will be created on first write\n",
    "        pass\n",
    "    \n",
    "    spark_df = spark.createDataFrame(result_df)\n",
    "    spark_df.write.mode(\"append\").saveAsTable(output_table)\n",
    "    print(f\"üìù Results logged to Delta table: {output_table}\")\n",
    "\n",
    "# =============================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        latest_exp = get_latest_experiment(client)\n",
    "        model_name, model_type = infer_model_name(latest_exp.name)\n",
    "        model, model_version = load_staging_model(client, model_name)\n",
    "        df, y_true, y_pred = run_inference(model)\n",
    "        mae, rmse, r2, mape = calculate_metrics(y_true, y_pred)\n",
    "        status = evaluate_uat_status(mape, r2)\n",
    "        log_results_to_delta(model_name, model_version, mae, rmse, r2, mape, status)\n",
    "        print(\"\\nüéØ UAT process completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
