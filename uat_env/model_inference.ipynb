{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================================\n",
    "# ‚úÖ FIXED UAT MODEL INFERENCE SCRIPT\n",
    "# =============================================================================\n",
    "# Hard-coded configuration matching training, registration, and evaluation scripts\n",
    "# =============================================================================\n",
    "\n",
    "# COMMAND ----------\n",
    "%pip install xgboost requests\n",
    "\n",
    "# COMMAND ----------\n",
    "# üîÑ Restart Python to use updated packages\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "# COMMAND ----------\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ HARD-CODED CONFIGURATION (MUST MATCH ALL OTHER SCRIPTS!)\n",
    "# =============================================================================\n",
    "# These values are DIRECTLY from training_script.py and Model_Registration.ipynb\n",
    "\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"  # ‚úÖ EXACT MATCH\n",
    "STAGING_ALIAS = \"Staging\"  # ‚úÖ Capitalized to match registration script\n",
    "\n",
    "DELTA_INPUT_TABLE = \"workspace.default.house_price_delta\"\n",
    "FEATURE_COLS = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "LABEL_COL = 'price'\n",
    "\n",
    "# UAT Thresholds\n",
    "MAPE_THRESHOLD = 15.0  # Maximum acceptable MAPE (%)\n",
    "R2_THRESHOLD = 0.75    # Minimum acceptable R¬≤\n",
    "\n",
    "OUTPUT_TABLE = \"workspace.default.uat_inference_house_price_xgboost\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ UAT MODEL INFERENCE - FIXED VERSION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìã CONFIGURATION:\")\n",
    "print(f\"   Model Name: {MODEL_NAME}\")\n",
    "print(f\"   Staging Alias: @{STAGING_ALIAS}\")\n",
    "print(f\"   Input Table: {DELTA_INPUT_TABLE}\")\n",
    "print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "print(f\"   Feature Columns: {FEATURE_COLS}\")\n",
    "print(f\"   MAPE Threshold: ‚â§ {MAPE_THRESHOLD}%\")\n",
    "print(f\"   R¬≤ Threshold: ‚â• {R2_THRESHOLD}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ SLACK NOTIFICATION SETUP\n",
    "# =============================================================================\n",
    "def get_slack_webhook():\n",
    "    \"\"\"Retrieve Slack webhook from secrets with fallback scopes\"\"\"\n",
    "    for scope in [\"shared-scope\", \"dev-scope\"]:\n",
    "        try:\n",
    "            webhook = dbutils.secrets.get(scope, \"SLACK_WEBHOOK_URL\")\n",
    "            if webhook and webhook.strip():\n",
    "                print(f\"‚úì Slack webhook configured from scope '{scope}'\")\n",
    "                return webhook\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Slack webhook not found in scope '{scope}': {e}\")\n",
    "    return None\n",
    "\n",
    "SLACK_WEBHOOK_URL = get_slack_webhook()\n",
    "\n",
    "def send_slack_notification(message, level=\"info\"):\n",
    "    \"\"\"Send notification to Slack channel\"\"\"\n",
    "    if not SLACK_WEBHOOK_URL:\n",
    "        print(f\"‚ö†Ô∏è Slack webhook not configured\")\n",
    "        print(f\"üì¢ Message: {message}\")\n",
    "        return\n",
    "    \n",
    "    emoji_map = {\n",
    "        \"info\": \"‚ÑπÔ∏è\",\n",
    "        \"success\": \"‚úÖ\",\n",
    "        \"warning\": \"‚ö†Ô∏è\",\n",
    "        \"error\": \"‚ùå\"\n",
    "    }\n",
    "    \n",
    "    formatted_message = f\"{emoji_map.get(level, '‚ÑπÔ∏è')} {message}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            SLACK_WEBHOOK_URL, \n",
    "            json={\"text\": formatted_message},\n",
    "            timeout=5\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Slack notification sent: {level}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Slack notification failed: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error sending Slack notification: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ INITIALIZATION\n",
    "# =============================================================================\n",
    "spark = SparkSession.builder.appName(\"UAT_Inference_Fixed\").getOrCreate()\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"\\n‚úÖ MLflow and Spark initialized\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 1: LOAD MODEL FROM STAGING ALIAS\n",
    "# =============================================================================\n",
    "def load_staging_model():\n",
    "    \"\"\"Load model from Unity Catalog using Staging alias\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìã STEP 1: Loading Model from @{STAGING_ALIAS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Try direct alias lookup\n",
    "        print(f\"‚è≥ Attempting to load: models:/{MODEL_NAME}@{STAGING_ALIAS}\")\n",
    "        \n",
    "        try:\n",
    "            model_version = client.get_model_version_by_alias(MODEL_NAME, STAGING_ALIAS)\n",
    "            version = model_version.version\n",
    "            run_id = model_version.run_id\n",
    "            \n",
    "            print(f\"‚úÖ Found model with @{STAGING_ALIAS} alias\")\n",
    "            print(f\"   Version: v{version}\")\n",
    "            print(f\"   Run ID: {run_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Direct alias lookup failed: {e}\")\n",
    "            print(f\"   Trying alternative search method...\")\n",
    "            \n",
    "            # Method 2: Search through all versions\n",
    "            model_versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
    "            \n",
    "            if not model_versions:\n",
    "                raise ValueError(\n",
    "                    f\"‚ùå No model versions found for {MODEL_NAME}\\n\"\n",
    "                    f\"üí° Solution: Run Model_Registration.ipynb first to register a model\"\n",
    "                )\n",
    "            \n",
    "            # Filter versions with the staging alias\n",
    "            staging_versions = []\n",
    "            print(f\"\\nüîç Searching through {len(model_versions)} version(s)...\")\n",
    "            \n",
    "            for v in model_versions:\n",
    "                full_version = client.get_model_version(MODEL_NAME, v.version)\n",
    "                version_aliases = full_version.aliases if full_version.aliases else []\n",
    "                \n",
    "                # Case-insensitive comparison\n",
    "                if any(alias.lower() == STAGING_ALIAS.lower() for alias in version_aliases):\n",
    "                    staging_versions.append(full_version)\n",
    "                    print(f\"   ‚úì Version v{v.version} has @{STAGING_ALIAS} alias\")\n",
    "            \n",
    "            if not staging_versions:\n",
    "                # List available versions for debugging\n",
    "                print(f\"\\n‚ùå No model with alias '@{STAGING_ALIAS}' found!\")\n",
    "                print(f\"\\nüìã Available versions for {MODEL_NAME}:\")\n",
    "                for v in model_versions[:10]:\n",
    "                    full_v = client.get_model_version(MODEL_NAME, v.version)\n",
    "                    v_aliases = full_v.aliases if full_v.aliases else [\"No aliases\"]\n",
    "                    print(f\"   Version v{v.version}: Aliases = {v_aliases}\")\n",
    "                \n",
    "                raise ValueError(\n",
    "                    f\"\\n‚ùå No model with alias '@{STAGING_ALIAS}' found for {MODEL_NAME}\\n\"\n",
    "                    f\"üí° Solution: Run Model_Evaluation.ipynb to promote a model to @{STAGING_ALIAS}\"\n",
    "                )\n",
    "            \n",
    "            # Get latest version from staging\n",
    "            model_version = max(staging_versions, key=lambda x: int(x.version))\n",
    "            version = model_version.version\n",
    "            run_id = model_version.run_id\n",
    "            \n",
    "            print(f\"\\n‚úÖ Found {len(staging_versions)} version(s) with @{STAGING_ALIAS} alias\")\n",
    "            print(f\"   Loading latest: v{version}\")\n",
    "        \n",
    "        # Load the model\n",
    "        model_uri = f\"models:/{MODEL_NAME}@{STAGING_ALIAS}\"\n",
    "        print(f\"\\n‚è≥ Loading model...\")\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ MODEL LOADED SUCCESSFULLY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Model Name: {MODEL_NAME}\")\n",
    "        print(f\"   Version: v{version}\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Status: {model_version.status}\")\n",
    "        \n",
    "        # Get metric from tags if available\n",
    "        metric_tag = model_version.tags.get(\"metric_rmse\", \"N/A\")\n",
    "        print(f\"   Training RMSE: {metric_tag}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return model, version, run_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚ùå FAILED TO LOAD MODEL\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"\\nüí° Troubleshooting Steps:\")\n",
    "        print(f\"   1. Verify model exists: {MODEL_NAME}\")\n",
    "        print(f\"   2. Check if model is registered in Unity Catalog\")\n",
    "        print(f\"   3. Run Model_Evaluation.ipynb to promote a model to @{STAGING_ALIAS}\")\n",
    "        print(f\"   4. Verify alias is exactly '{STAGING_ALIAS}' (case-sensitive)\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 2: LOAD UAT DATA\n",
    "# =============================================================================\n",
    "def load_uat_data():\n",
    "    \"\"\"Load UAT data from Delta table\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 2: Loading UAT Data\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"   Loading from: {DELTA_INPUT_TABLE}\")\n",
    "        df_spark = spark.table(DELTA_INPUT_TABLE)\n",
    "        df = df_spark.toPandas()\n",
    "\n",
    "        print(f\"   Total rows: {len(df)}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "        # Validate required columns\n",
    "        missing_features = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
    "\n",
    "        if LABEL_COL not in df.columns:\n",
    "            raise ValueError(f\"Missing label column: {LABEL_COL}\")\n",
    "\n",
    "        # Select features and labels\n",
    "        X = df[FEATURE_COLS]\n",
    "        y_true = df[LABEL_COL]\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ DATA LOADED SUCCESSFULLY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Labels shape: {y_true.shape}\")\n",
    "        print(f\"   Sample features:\\n{X.head(3)}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return df, X, y_true\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚ùå FAILED TO LOAD DATA\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in error_msg or \"cannot be found\" in error_msg:\n",
    "            print(f\"   Delta table '{DELTA_INPUT_TABLE}' does not exist\")\n",
    "            print(f\"\\nüí° Solution:\")\n",
    "            print(f\"   1. Create the table first\")\n",
    "            print(f\"   2. Verify the table name: {DELTA_INPUT_TABLE}\")\n",
    "            print(f\"   3. Check catalog and schema exist\")\n",
    "        else:\n",
    "            print(f\"   Error: {e}\")\n",
    "        \n",
    "        print(f\"{'='*80}\\n\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 3: RUN INFERENCE\n",
    "# =============================================================================\n",
    "def run_inference(model, X):\n",
    "    \"\"\"Run model inference on UAT data\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 3: Running Inference\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"   Generating predictions for {len(X)} samples...\")\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ INFERENCE COMPLETE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Predictions generated: {len(y_pred)}\")\n",
    "        print(f\"   Sample predictions: {y_pred[:5]}\")\n",
    "        print(f\"   Min prediction: {y_pred.min():.2f}\")\n",
    "        print(f\"   Max prediction: {y_pred.max():.2f}\")\n",
    "        print(f\"   Mean prediction: {y_pred.mean():.2f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚ùå INFERENCE FAILED\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 4: CALCULATE METRICS\n",
    "# =============================================================================\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 4: Evaluating Model Performance\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        print(f\"\\nüìä Evaluation Metrics:\")\n",
    "        print(f\"   MAE  : {mae:>12,.2f}\")\n",
    "        print(f\"   RMSE : {rmse:>12,.2f}\")\n",
    "        print(f\"   R¬≤   : {r2:>12.4f}\")\n",
    "        print(f\"   MAPE : {mape:>12.2f}%\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return mae, rmse, r2, mape\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Evaluation failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 5: UAT VALIDATION\n",
    "# =============================================================================\n",
    "def validate_uat(mape, r2, model_version):\n",
    "    \"\"\"Validate model against UAT thresholds\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 5: UAT Validation\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\nüìè Validation Thresholds:\")\n",
    "    print(f\"   MAPE: ‚â§ {MAPE_THRESHOLD}%\")\n",
    "    print(f\"   R¬≤:   ‚â• {R2_THRESHOLD}\")\n",
    "\n",
    "    print(f\"\\nüìä Actual Performance:\")\n",
    "    mape_pass = mape <= MAPE_THRESHOLD\n",
    "    r2_pass = r2 >= R2_THRESHOLD\n",
    "    \n",
    "    print(f\"   MAPE: {mape:.2f}% {'‚úÖ PASS' if mape_pass else '‚ùå FAIL'}\")\n",
    "    print(f\"   R¬≤:   {r2:.4f}  {'‚úÖ PASS' if r2_pass else '‚ùå FAIL'}\")\n",
    "\n",
    "    if mape_pass and r2_pass:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ‚úÖ UAT PASSED ‚úÖ‚úÖ\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Model v{model_version} is ready for production!\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "        send_slack_notification(\n",
    "            f\"‚úÖ Model `{MODEL_NAME}` v{model_version} PASSED UAT\\n\"\n",
    "            f\"üìä MAPE: {mape:.2f}%, R¬≤: {r2:.4f}\\n\"\n",
    "            f\"üöÄ Ready for production promotion!\",\n",
    "            level=\"success\"\n",
    "        )\n",
    "        return \"PASSED\"\n",
    "    else:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚ùå‚ùå UAT FAILED ‚ùå‚ùå\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        fail_reasons = []\n",
    "        if not mape_pass:\n",
    "            fail_reasons.append(f\"MAPE too high ({mape:.2f}% > {MAPE_THRESHOLD}%)\")\n",
    "        if not r2_pass:\n",
    "            fail_reasons.append(f\"R¬≤ too low ({r2:.4f} < {R2_THRESHOLD})\")\n",
    "\n",
    "        print(f\"   Failure reasons:\")\n",
    "        for reason in fail_reasons:\n",
    "            print(f\"   ‚Ä¢ {reason}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "        send_slack_notification(\n",
    "            f\"‚ùå Model `{MODEL_NAME}` v{model_version} FAILED UAT\\n\"\n",
    "            f\"üìä MAPE: {mape:.2f}%, R¬≤: {r2:.4f}\\n\"\n",
    "            f\"üö´ Reasons: {', '.join(fail_reasons)}\",\n",
    "            level=\"error\"\n",
    "        )\n",
    "\n",
    "        return \"FAILED\"\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 6: LOG RESULTS\n",
    "# =============================================================================\n",
    "def log_results(model_version, run_id, mae, rmse, r2, mape, status):\n",
    "    \"\"\"Log UAT results to Delta table\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 6: Logging Results\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists and for duplicates\n",
    "        table_exists = False\n",
    "        \n",
    "        try:\n",
    "            existing = spark.table(OUTPUT_TABLE)\n",
    "            table_exists = True\n",
    "            existing_df = existing.toPandas()\n",
    "            print(f\"   Table exists: Yes\")\n",
    "            print(f\"   Existing rows: {len(existing_df)}\")\n",
    "            \n",
    "            # Check for duplicate\n",
    "            if not existing_df.empty:\n",
    "                last = existing_df.iloc[-1]\n",
    "                \n",
    "                is_duplicate = (\n",
    "                    int(last.model_version) == int(model_version) and\n",
    "                    math.isclose(float(last.mae), mae, rel_tol=1e-6) and\n",
    "                    math.isclose(float(last.rmse), rmse, rel_tol=1e-6)\n",
    "                )\n",
    "                \n",
    "                if is_duplicate:\n",
    "                    print(\"\\n   ‚ÑπÔ∏è Duplicate entry detected - skipping log\")\n",
    "                    return\n",
    "                    \n",
    "        except Exception:\n",
    "            print(f\"   Table exists: No (will be created)\")\n",
    "        \n",
    "        # Prepare result data\n",
    "        result_df = pd.DataFrame([{\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"model_version\": int(model_version),\n",
    "            \"run_id\": run_id,\n",
    "            \"mae\": float(mae),\n",
    "            \"rmse\": float(rmse),\n",
    "            \"r2\": float(r2),\n",
    "            \"mape\": float(mape),\n",
    "            \"uat_status\": status\n",
    "        }])\n",
    "\n",
    "        # Write to Delta\n",
    "        spark_df = spark.createDataFrame(result_df)\n",
    "        \n",
    "        if table_exists:\n",
    "            spark_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "        else:\n",
    "            spark_df.write.mode(\"append\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ RESULTS LOGGED SUCCESSFULLY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "        print(f\"   Model Version: v{model_version}\")\n",
    "        print(f\"   UAT Status: {status}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Failed to log results: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ MAIN EXECUTION\n",
    "# =============================================================================\n",
    "def main():\n",
    "    \"\"\"Main UAT inference pipeline\"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üé¨ STARTING UAT INFERENCE PIPELINE\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        # Execute pipeline steps\n",
    "        model, model_version, run_id = load_staging_model()\n",
    "        df, X, y_true = load_uat_data()\n",
    "        y_pred = run_inference(model, X)\n",
    "        mae, rmse, r2, mape = evaluate_model(y_true, y_pred)\n",
    "        status = validate_uat(mape, r2, model_version)\n",
    "        log_results(model_version, run_id, mae, rmse, r2, mape, status)\n",
    "\n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ú® UAT INFERENCE COMPLETED SUCCESSFULLY ‚ú®\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nüìä Final Summary:\")\n",
    "        print(f\"   Model: {MODEL_NAME}\")\n",
    "        print(f\"   Version: v{model_version}\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   UAT Status: {status}\")\n",
    "        print(f\"   Metrics:\")\n",
    "        print(f\"     ‚Ä¢ RMSE: {rmse:,.2f}\")\n",
    "        print(f\"     ‚Ä¢ MAPE: {mape:.2f}%\")\n",
    "        print(f\"     ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "        print(f\"     ‚Ä¢ MAE:  {mae:,.2f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        # Exit with appropriate code\n",
    "        print(f\"\\n‚úÖ UAT pipeline completed with status: {status}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ùå UAT INFERENCE FAILED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        send_slack_notification(\n",
    "            f\"‚ùå UAT pipeline failed for `{MODEL_NAME}`\\n\"\n",
    "            f\"Error: {str(e)}\",\n",
    "            level=\"error\"\n",
    "        )\n",
    "        \n",
    "        sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ EXECUTE\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================\n",
    "# # ‚úÖ UAT MODEL INFERENCE SCRIPT (FINAL ALIGNED VERSION)\n",
    "# # =============================================================\n",
    "# # COMMAND ----------\n",
    "# %pip install xgboost\n",
    "\n",
    "# # COMMAND ----------\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# from pyspark.sql import SparkSession\n",
    "# from datetime import datetime\n",
    "# import warnings\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ CONFIGURATION (ALIGNED WITH REGISTRATION & STAGING SCRIPTS)\n",
    "# # =============================================================\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "# STAGING_ALIAS = \"Staging\"\n",
    "\n",
    "# # Delta input table for UAT inference\n",
    "# DELTA_INPUT_TABLE = \"workspace.default.house_price_delta\"\n",
    "\n",
    "# # Feature columns (must match training script)\n",
    "# FEATURE_COLS = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "# LABEL_COL = 'price'\n",
    "\n",
    "# # Thresholds for validation\n",
    "# MAPE_THRESHOLD = 15.0   # target < 15%\n",
    "# R2_THRESHOLD   = 0.75   # target > 0.75\n",
    "\n",
    "# # Output table for UAT results\n",
    "# OUTPUT_TABLE = \"workspace.default.uat_inference_house_price_xgboost\"\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ INITIALIZATION\n",
    "# # =============================================================\n",
    "# print(\"=\"*80)\n",
    "# print(\"üöÄ UAT MODEL INFERENCE - ALIGNED VERSION\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"UAT_Inference_Aligned\").getOrCreate()\n",
    "# mlflow.set_registry_uri(\"databricks-uc\")\n",
    "# client = MlflowClient()\n",
    "\n",
    "# print(f\"\\nüìã Configuration:\")\n",
    "# print(f\"   Model: {MODEL_NAME}\")\n",
    "# print(f\"   Alias: {STAGING_ALIAS}\")\n",
    "# print(f\"   Input Table: {DELTA_INPUT_TABLE}\")\n",
    "# print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "# print(f\"   Feature Columns: {FEATURE_COLS}\")\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 1Ô∏è‚É£ Load model from STAGING alias\n",
    "# # =============================================================\n",
    "# def load_staging_model(client, model_name, alias):\n",
    "#     \"\"\"\n",
    "#     Load model from Unity Catalog using alias (aligned with staging script)\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"üìã STEP 1: Loading Model from @{alias}\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Search for model versions with staging alias\n",
    "#         model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "        \n",
    "#         # Filter versions that have the staging alias\n",
    "#         staging_versions = []\n",
    "#         for v in model_versions:\n",
    "#             full_version = client.get_model_version(model_name, v.version)\n",
    "#             aliases = [a.lower() for a in full_version.aliases] if full_version.aliases else []\n",
    "#             if alias.lower() in aliases:\n",
    "#                 staging_versions.append(full_version)\n",
    "        \n",
    "#         if not staging_versions:\n",
    "#             raise ValueError(f\"No model with alias '{alias}' found for {model_name}\")\n",
    "        \n",
    "#         # Get latest version from staging\n",
    "#         latest_staging = max(staging_versions, key=lambda x: int(x.version))\n",
    "#         version = latest_staging.version\n",
    "#         run_id = latest_staging.run_id\n",
    "        \n",
    "#         print(f\"   Found {len(staging_versions)} version(s) with @{alias} alias\")\n",
    "#         print(f\"   Loading version: v{version}\")\n",
    "        \n",
    "#         model_uri = f\"models:/{model_name}@{alias}\"\n",
    "#         model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "#         print(f\"\\n‚úÖ Model Loaded Successfully!\")\n",
    "#         print(f\"   Version: v{version}\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Status: {latest_staging.status}\")\n",
    "        \n",
    "#         # Get metric from tags if available\n",
    "#         metric_tag = latest_staging.tags.get(\"metric_rmse\", \"N/A\")\n",
    "#         print(f\"   Training RMSE: {metric_tag}\")\n",
    "        \n",
    "#         return model, version, run_id\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Failed to load model from {alias}: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise ValueError(f\"Model loading failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 2Ô∏è‚É£ Load Delta table for inference\n",
    "# # =============================================================\n",
    "# def load_data(spark):\n",
    "#     \"\"\"\n",
    "#     Load UAT data from Delta table with proper feature selection\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 2: Loading UAT Data\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"   Loading from: {DELTA_INPUT_TABLE}\")\n",
    "#         df_spark = spark.table(DELTA_INPUT_TABLE)\n",
    "#         df = df_spark.toPandas()\n",
    "\n",
    "#         print(f\"   Total rows loaded: {len(df)}\")\n",
    "#         print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "#         # Validate required columns exist\n",
    "#         missing_features = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "#         if missing_features:\n",
    "#             raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
    "\n",
    "#         if LABEL_COL not in df.columns:\n",
    "#             raise ValueError(f\"Missing label column: {LABEL_COL}\")\n",
    "\n",
    "#         # Select only required features and label\n",
    "#         X = df[FEATURE_COLS]\n",
    "#         y_true = df[LABEL_COL]\n",
    "\n",
    "#         print(f\"\\n‚úÖ Data Loaded Successfully!\")\n",
    "#         print(f\"   Features shape: {X.shape}\")\n",
    "#         print(f\"   Labels shape: {y_true.shape}\")\n",
    "        \n",
    "#         return df, X, y_true\n",
    "\n",
    "#     except Exception as e:\n",
    "#         error_msg = str(e)\n",
    "#         if \"TABLE_OR_VIEW_NOT_FOUND\" in error_msg or \"cannot be found\" in error_msg:\n",
    "#             print(f\"\\n‚ùå Delta table '{DELTA_INPUT_TABLE}' does not exist.\")\n",
    "#             print(f\"   Please create the table first or verify the table name.\")\n",
    "#             print(f\"   Expected format: catalog.schema.table_name\")\n",
    "#         else:\n",
    "#             print(f\"\\n‚ùå Failed to load input table: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise ValueError(f\"Data loading failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 3Ô∏è‚É£ Run inference\n",
    "# # =============================================================\n",
    "# def run_inference(model, X):\n",
    "#     \"\"\"\n",
    "#     Run model inference on UAT data\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 3: Running Inference\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"   Running predictions on {len(X)} samples...\")\n",
    "#         y_pred = model.predict(X)\n",
    "        \n",
    "#         print(f\"\\n‚úÖ Inference Complete!\")\n",
    "#         print(f\"   Predictions generated: {len(y_pred)}\")\n",
    "#         print(f\"   Sample predictions: {y_pred[:5]}\")\n",
    "        \n",
    "#         return y_pred\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Inference failed: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 4Ô∏è‚É£ Calculate metrics\n",
    "# # =============================================================\n",
    "# def evaluate(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Calculate evaluation metrics for UAT\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 4: Evaluating Model Performance\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         mae = mean_absolute_error(y_true, y_pred)\n",
    "#         rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "#         r2 = r2_score(y_true, y_pred)\n",
    "#         mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#         print(f\"\\nüìä Evaluation Metrics:\")\n",
    "#         print(f\"   MAE  : {mae:.3f}\")\n",
    "#         print(f\"   RMSE : {rmse:.3f}\")\n",
    "#         print(f\"   R¬≤   : {r2:.3f}\")\n",
    "#         print(f\"   MAPE : {mape:.2f}%\")\n",
    "        \n",
    "#         return mae, rmse, r2, mape\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Evaluation failed: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 5Ô∏è‚É£ Threshold validation (UAT pass/fail)\n",
    "# # =============================================================\n",
    "# def validate(mape, r2):\n",
    "#     \"\"\"\n",
    "#     Validate model performance against UAT thresholds\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 5: UAT Validation\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     print(f\"\\nüìè Validation Thresholds:\")\n",
    "#     print(f\"   MAPE threshold: ‚â§ {MAPE_THRESHOLD}%\")\n",
    "#     print(f\"   R¬≤ threshold:   ‚â• {R2_THRESHOLD}\")\n",
    "    \n",
    "#     print(f\"\\nüìä Actual Performance:\")\n",
    "#     print(f\"   MAPE: {mape:.2f}% {'‚úÖ' if mape <= MAPE_THRESHOLD else '‚ùå'}\")\n",
    "#     print(f\"   R¬≤:   {r2:.3f}  {'‚úÖ' if r2 >= R2_THRESHOLD else '‚ùå'}\")\n",
    "    \n",
    "#     if mape <= MAPE_THRESHOLD and r2 >= R2_THRESHOLD:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"‚úÖ‚úÖ UAT PASSED ‚úÖ‚úÖ\")\n",
    "#         print(f\"{'='*70}\")\n",
    "#         return \"PASSED\"\n",
    "#     else:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"‚ùå‚ùå UAT FAILED ‚ùå‚ùå\")\n",
    "#         print(f\"{'='*70}\")\n",
    "        \n",
    "#         # Show which criteria failed\n",
    "#         if mape > MAPE_THRESHOLD:\n",
    "#             print(f\"   ‚ö†Ô∏è MAPE too high: {mape:.2f}% > {MAPE_THRESHOLD}%\")\n",
    "#         if r2 < R2_THRESHOLD:\n",
    "#             print(f\"   ‚ö†Ô∏è R¬≤ too low: {r2:.3f} < {R2_THRESHOLD}\")\n",
    "        \n",
    "#         return \"FAILED\"\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 6Ô∏è‚É£ Log results to Delta table (with smart schema handling)\n",
    "# # =============================================================\n",
    "# def log_results(spark, model_name, model_version, run_id, mae, rmse, r2, mape, status):\n",
    "#     \"\"\"\n",
    "#     Log UAT results to Delta table with duplicate prevention and backward compatibility\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 6: Logging Results\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Check if table exists and its schema\n",
    "#         table_exists = False\n",
    "        \n",
    "#         try:\n",
    "#             existing = spark.table(OUTPUT_TABLE)\n",
    "#             table_exists = True\n",
    "#             existing_df = existing.toPandas()\n",
    "#             print(f\"   Table exists: Yes\")\n",
    "#             print(f\"   Existing rows: {len(existing_df)}\")\n",
    "            \n",
    "#             # Check for duplicates\n",
    "#             if not existing_df.empty:\n",
    "#                 last = existing_df.iloc[-1]\n",
    "                \n",
    "#                 # Check if metrics are identical to last run\n",
    "#                 is_duplicate = (\n",
    "#                     int(last.model_version) == int(model_version) and\n",
    "#                     math.isclose(float(last.mae), mae, rel_tol=1e-6) and\n",
    "#                     math.isclose(float(last.rmse), rmse, rel_tol=1e-6) and\n",
    "#                     math.isclose(float(last.r2), r2, rel_tol=1e-6) and\n",
    "#                     math.isclose(float(last.mape), mape, rel_tol=1e-6)\n",
    "#                 )\n",
    "                \n",
    "#                 if is_duplicate:\n",
    "#                     print(\"\\n‚ÑπÔ∏è Duplicate Entry Detected\")\n",
    "#                     print(\"   Metrics unchanged from last run ‚Üí Skipping log\")\n",
    "#                     return\n",
    "                    \n",
    "#         except Exception as e:\n",
    "#             print(f\"   Table exists: No (will be created)\")\n",
    "#             print(f\"   Note: {e}\")\n",
    "        \n",
    "#         # Prepare data for logging\n",
    "#         result_df = pd.DataFrame([{\n",
    "#             \"timestamp\": datetime.now(),\n",
    "#             \"model_name\": model_name,\n",
    "#             \"model_version\": int(model_version),\n",
    "#             \"run_id\": run_id,\n",
    "#             \"mae\": float(mae),\n",
    "#             \"rmse\": float(rmse),\n",
    "#             \"r2\": float(r2),\n",
    "#             \"mape\": float(mape),\n",
    "#             \"uat_status\": status\n",
    "#         }])\n",
    "\n",
    "#         # Write to Delta table\n",
    "#         spark_df = spark.createDataFrame(result_df)\n",
    "        \n",
    "#         if table_exists:\n",
    "#             # Table exists - append with schema evolution if needed\n",
    "#             spark_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "#         else:\n",
    "#             # New table - create it\n",
    "#             spark_df.write.mode(\"append\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "#         print(f\"\\n‚úÖ Results Logged Successfully!\")\n",
    "#         print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "#         print(f\"   Model Name: {model_name}\")\n",
    "#         print(f\"   Model Version: v{model_version}\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   UAT Status: {status}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Failed to log results: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ MAIN EXECUTION FLOW\n",
    "# # =============================================================\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Main execution flow for UAT inference\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"üé¨ STARTING UAT INFERENCE PIPELINE\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         # Step 1: Load model\n",
    "#         model, model_version, run_id = load_staging_model(client, MODEL_NAME, STAGING_ALIAS)\n",
    "        \n",
    "#         # Step 2: Load data\n",
    "#         df, X, y_true = load_data(spark)\n",
    "        \n",
    "#         # Step 3: Run inference\n",
    "#         y_pred = run_inference(model, X)\n",
    "        \n",
    "#         # Step 4: Evaluate\n",
    "#         mae, rmse, r2, mape = evaluate(y_true, y_pred)\n",
    "        \n",
    "#         # Step 5: Validate\n",
    "#         status = validate(mape, r2)\n",
    "        \n",
    "#         # Step 6: Log results\n",
    "#         log_results(spark, MODEL_NAME, model_version, run_id, mae, rmse, r2, mape, status)\n",
    "\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"‚ú® UAT INFERENCE COMPLETED SUCCESSFULLY ‚ú®\")\n",
    "#         print(\"=\"*80)\n",
    "#         print(f\"\\nüìä Summary:\")\n",
    "#         print(f\"   Model: {MODEL_NAME}\")\n",
    "#         print(f\"   Version: v{model_version}\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   UAT Status: {status}\")\n",
    "#         print(f\"   RMSE: {rmse:.3f}\")\n",
    "#         print(f\"   MAPE: {mape:.2f}%\")\n",
    "#         print(f\"   R¬≤: {r2:.3f}\")\n",
    "#         print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"‚ùå UAT INFERENCE FAILED\")\n",
    "#         print(\"=\"*80)\n",
    "#         print(f\"Error: {str(e)}\")\n",
    "#         print(\"=\"*80 + \"\\n\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ EXECUTE\n",
    "# # =============================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
