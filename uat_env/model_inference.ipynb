{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "3b5c92ea",
=======
   "id": "578d3230",
>>>>>>> a4f8c79 (fix the issue)
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================\n",
    "# ‚úÖ UAT MODEL INFERENCE SCRIPT (COMPLETE FIXED VERSION)\n",
    "# =============================================================\n",
    "# COMMAND ----------\n",
    "%pip install xgboost requests\n",
    "\n",
    "# COMMAND ----------\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ CONFIGURATION\n",
    "# =============================================================\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "STAGING_ALIAS = \"Staging\"\n",
    "\n",
    "DELTA_INPUT_TABLE = \"workspace.default.house_price_delta\"\n",
    "FEATURE_COLS = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "LABEL_COL = 'price'\n",
    "\n",
    "MAPE_THRESHOLD = 15.0\n",
    "R2_THRESHOLD   = 0.75\n",
    "\n",
    "OUTPUT_TABLE = \"workspace.default.uat_inference_house_price_xgboost\"\n",
    "\n",
    "# ‚úÖ Slack Webhook (store securely as Databricks secret)\n",
    "SLACK_WEBHOOK_URL = dbutils.secrets.get(\"dev-scope\", \"SLACK_WEBHOOK_URL\")\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ NOTIFICATION FUNCTION\n",
    "# =============================================================\n",
    "def send_slack_notification(message):\n",
    "    \"\"\"Send notification to Slack channel\"\"\"\n",
    "    if not SLACK_WEBHOOK_URL:\n",
    "        print(\"‚ö†Ô∏è Slack webhook URL not found. Skipping notification.\")\n",
    "        return\n",
    "    try:\n",
    "        response = requests.post(SLACK_WEBHOOK_URL, json={\"text\": message})\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Slack Notification Sent: {message}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Slack Notification Failed: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error sending notification: {e}\")\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ INITIALIZATION\n",
    "# =============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ UAT MODEL INFERENCE - COMPLETE VERSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UAT_Inference_Fixed\").getOrCreate()\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Alias: {STAGING_ALIAS}\")\n",
    "print(f\"   Input Table: {DELTA_INPUT_TABLE}\")\n",
    "print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "print(f\"   Feature Columns: {FEATURE_COLS}\")\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ 1Ô∏è‚É£ Load model from STAGING alias (COMPLETE IMPLEMENTATION)\n",
    "# =============================================================\n",
    "def load_staging_model(client, model_name, alias):\n",
    "    \"\"\"Load model from Unity Catalog using alias\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìã STEP 1: Loading Model from @{alias}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Search for model versions with staging alias\n",
    "        model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "        \n",
    "        if not model_versions:\n",
    "            raise ValueError(f\"No model versions found for {model_name}\")\n",
    "        \n",
    "        # Filter versions that have the staging alias\n",
    "        staging_versions = []\n",
    "        for v in model_versions:\n",
    "            full_version = client.get_model_version(model_name, v.version)\n",
    "            aliases = [a.lower() for a in full_version.aliases] if full_version.aliases else []\n",
    "            if alias.lower() in aliases:\n",
    "                staging_versions.append(full_version)\n",
    "        \n",
    "        if not staging_versions:\n",
    "            raise ValueError(f\"No model with alias '{alias}' found for {model_name}\")\n",
    "        \n",
    "        # Get latest version from staging\n",
    "        latest_staging = max(staging_versions, key=lambda x: int(x.version))\n",
    "        version = latest_staging.version\n",
    "        run_id = latest_staging.run_id\n",
    "        \n",
    "        print(f\"   Found {len(staging_versions)} version(s) with @{alias} alias\")\n",
    "        print(f\"   Loading version: v{version}\")\n",
    "        \n",
    "        model_uri = f\"models:/{model_name}@{alias}\"\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "        print(f\"\\n‚úÖ Model Loaded Successfully!\")\n",
    "        print(f\"   Version: v{version}\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Status: {latest_staging.status}\")\n",
    "        \n",
    "        # Get metric from tags if available\n",
    "        metric_tag = latest_staging.tags.get(\"metric_rmse\", \"N/A\")\n",
    "        print(f\"   Training RMSE: {metric_tag}\")\n",
    "        \n",
    "        return model, version, run_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Failed to load model from {alias}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ 2Ô∏è‚É£ Load Delta table for inference\n",
    "# =============================================================\n",
    "def load_data(spark):\n",
    "    \"\"\"Load UAT data from Delta table\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 2: Loading UAT Data\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"   Loading from: {DELTA_INPUT_TABLE}\")\n",
    "        df_spark = spark.table(DELTA_INPUT_TABLE)\n",
    "        df = df_spark.toPandas()\n",
    "\n",
    "        print(f\"   Total rows loaded: {len(df)}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "        # Validate required columns exist\n",
    "        missing_features = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
    "\n",
    "        if LABEL_COL not in df.columns:\n",
    "            raise ValueError(f\"Missing label column: {LABEL_COL}\")\n",
    "\n",
    "        # Select only required features and label\n",
    "        X = df[FEATURE_COLS]\n",
    "        y_true = df[LABEL_COL]\n",
    "\n",
    "        print(f\"\\n‚úÖ Data Loaded Successfully!\")\n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Labels shape: {y_true.shape}\")\n",
    "        \n",
    "        return df, X, y_true\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in error_msg or \"cannot be found\" in error_msg:\n",
    "            print(f\"\\n‚ùå Delta table '{DELTA_INPUT_TABLE}' does not exist.\")\n",
    "            print(f\"   Please create the table first or verify the table name.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Failed to load input table: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ 3Ô∏è‚É£ Run inference\n",
    "# =============================================================\n",
    "def run_inference(model, X):\n",
    "    \"\"\"Run model inference on UAT data\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 3: Running Inference\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"   Running predictions on {len(X)} samples...\")\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Inference Complete!\")\n",
    "        print(f\"   Predictions generated: {len(y_pred)}\")\n",
    "        print(f\"   Sample predictions: {y_pred[:5]}\")\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Inference failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ 4Ô∏è‚É£ Calculate metrics\n",
    "# =============================================================\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Calculate evaluation metrics for UAT\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 4: Evaluating Model Performance\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        print(f\"\\nüìä Evaluation Metrics:\")\n",
    "        print(f\"   MAE  : {mae:.3f}\")\n",
    "        print(f\"   RMSE : {rmse:.3f}\")\n",
    "        print(f\"   R¬≤   : {r2:.3f}\")\n",
    "        print(f\"   MAPE : {mape:.2f}%\")\n",
    "        \n",
    "        return mae, rmse, r2, mape\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Evaluation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ 5Ô∏è‚É£ Threshold validation (UAT pass/fail)\n",
    "# =============================================================\n",
    "def validate(mape, r2, model_name=None, model_version=None):\n",
    "    \"\"\"Validate model performance against UAT thresholds and send Slack notifications\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 5: UAT Validation\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\nüìè Validation Thresholds:\")\n",
    "    print(f\"   MAPE threshold: ‚â§ {MAPE_THRESHOLD}%\")\n",
    "    print(f\"   R¬≤ threshold:   ‚â• {R2_THRESHOLD}\")\n",
    "\n",
    "    print(f\"\\nüìä Actual Performance:\")\n",
    "    print(f\"   MAPE: {mape:.2f}% {'‚úÖ' if mape <= MAPE_THRESHOLD else '‚ùå'}\")\n",
    "    print(f\"   R¬≤:   {r2:.3f}  {'‚úÖ' if r2 >= R2_THRESHOLD else '‚ùå'}\")\n",
    "\n",
    "    if mape <= MAPE_THRESHOLD and r2 >= R2_THRESHOLD:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚úÖ‚úÖ UAT PASSED ‚úÖ‚úÖ\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # ‚úÖ Slack notification for success\n",
    "        send_slack_notification(\n",
    "            f\"‚úÖ Model `{model_name}` (v{model_version}) PASSED UAT.\\n\"\n",
    "            f\"MAPE={mape:.2f}%, R¬≤={r2:.3f}.\\nPromoting to PRODUCTION...\"\n",
    "        )\n",
    "        return \"PASSED\"\n",
    "    else:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚ùå‚ùå UAT FAILED ‚ùå‚ùå\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Identify failed metrics\n",
    "        fail_reasons = []\n",
    "        if mape > MAPE_THRESHOLD:\n",
    "            fail_reasons.append(f\"MAPE too high ({mape:.2f}% > {MAPE_THRESHOLD}%)\")\n",
    "        if r2 < R2_THRESHOLD:\n",
    "            fail_reasons.append(f\"R¬≤ too low ({r2:.3f} < {R2_THRESHOLD})\")\n",
    "\n",
    "        # ‚ùå Slack notification for failure\n",
    "        send_slack_notification(\n",
    "            f\"‚ùå Model `{model_name}` (v{model_version}) FAILED UAT.\\n\"\n",
    "            f\"Reasons: {', '.join(fail_reasons)}.\\nPromotion stopped.\"\n",
    "        )\n",
    "\n",
    "        return \"FAILED\"\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ 6Ô∏è‚É£ Log results to Delta table\n",
    "# =============================================================\n",
    "def log_results(spark, model_name, model_version, run_id, mae, rmse, r2, mape, status):\n",
    "    \"\"\"Log UAT results to Delta table with duplicate prevention\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 6: Logging Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists and its schema\n",
    "        table_exists = False\n",
    "        \n",
    "        try:\n",
    "            existing = spark.table(OUTPUT_TABLE)\n",
    "            table_exists = True\n",
    "            existing_df = existing.toPandas()\n",
    "            print(f\"   Table exists: Yes\")\n",
    "            print(f\"   Existing rows: {len(existing_df)}\")\n",
    "            \n",
    "            # Check for duplicates\n",
    "            if not existing_df.empty:\n",
    "                last = existing_df.iloc[-1]\n",
    "                \n",
    "                # Check if metrics are identical to last run\n",
    "                is_duplicate = (\n",
    "                    int(last.model_version) == int(model_version) and\n",
    "                    math.isclose(float(last.mae), mae, rel_tol=1e-6) and\n",
    "                    math.isclose(float(last.rmse), rmse, rel_tol=1e-6) and\n",
    "                    math.isclose(float(last.r2), r2, rel_tol=1e-6) and\n",
    "                    math.isclose(float(last.mape), mape, rel_tol=1e-6)\n",
    "                )\n",
    "                \n",
    "                if is_duplicate:\n",
    "                    print(\"\\n‚ÑπÔ∏è Duplicate Entry Detected\")\n",
    "                    print(\"   Metrics unchanged from last run ‚Üí Skipping log\")\n",
    "                    return\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   Table exists: No (will be created)\")\n",
    "        \n",
    "        # Prepare data for logging\n",
    "        result_df = pd.DataFrame([{\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": int(model_version),\n",
    "            \"run_id\": run_id,\n",
    "            \"mae\": float(mae),\n",
    "            \"rmse\": float(rmse),\n",
    "            \"r2\": float(r2),\n",
    "            \"mape\": float(mape),\n",
    "            \"uat_status\": status\n",
    "        }])\n",
    "\n",
    "        # Write to Delta table\n",
    "        spark_df = spark.createDataFrame(result_df)\n",
    "        \n",
    "        if table_exists:\n",
    "            spark_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "        else:\n",
    "            spark_df.write.mode(\"append\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "        print(f\"\\n‚úÖ Results Logged Successfully!\")\n",
    "        print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "        print(f\"   Model Version: v{model_version}\")\n",
    "        print(f\"   UAT Status: {status}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Failed to log results: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ MAIN EXECUTION FLOW\n",
    "# =============================================================\n",
    "def main():\n",
    "    \"\"\"Main execution flow for UAT inference\"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üé¨ STARTING UAT INFERENCE PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        model, model_version, run_id = load_staging_model(client, MODEL_NAME, STAGING_ALIAS)\n",
    "        df, X, y_true = load_data(spark)\n",
    "        y_pred = run_inference(model, X)\n",
    "        mae, rmse, r2, mape = evaluate(y_true, y_pred)\n",
    "\n",
    "        # ‚úÖ Pass model info to validation for notification\n",
    "        status = validate(mape, r2, MODEL_NAME, model_version)\n",
    "\n",
    "        log_results(spark, MODEL_NAME, model_version, run_id, mae, rmse, r2, mape, status)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ú® UAT INFERENCE COMPLETED SUCCESSFULLY ‚ú®\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"   Model: {MODEL_NAME}\")\n",
    "        print(f\"   Version: v{model_version}\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   UAT Status: {status}\")\n",
    "        print(f\"   RMSE: {rmse:.3f}\")\n",
    "        print(f\"   MAPE: {mape:.2f}%\")\n",
    "        print(f\"   R¬≤: {r2:.3f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ùå UAT INFERENCE FAILED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        send_slack_notification(f\"‚ùå UAT pipeline failed with error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# =============================================================\n",
    "# ‚úÖ EXECUTE\n",
    "# =============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================\n",
    "# # ‚úÖ UAT MODEL INFERENCE SCRIPT (FINAL ALIGNED VERSION)\n",
    "# # =============================================================\n",
    "# # COMMAND ----------\n",
    "# %pip install xgboost\n",
    "\n",
    "# # COMMAND ----------\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# from pyspark.sql import SparkSession\n",
    "# from datetime import datetime\n",
    "# import warnings\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ CONFIGURATION (ALIGNED WITH REGISTRATION & STAGING SCRIPTS)\n",
    "# # =============================================================\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "# STAGING_ALIAS = \"Staging\"\n",
    "\n",
    "# # Delta input table for UAT inference\n",
    "# DELTA_INPUT_TABLE = \"workspace.default.house_price_delta\"\n",
    "\n",
    "# # Feature columns (must match training script)\n",
    "# FEATURE_COLS = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "# LABEL_COL = 'price'\n",
    "\n",
    "# # Thresholds for validation\n",
    "# MAPE_THRESHOLD = 15.0   # target < 15%\n",
    "# R2_THRESHOLD   = 0.75   # target > 0.75\n",
    "\n",
    "# # Output table for UAT results\n",
    "# OUTPUT_TABLE = \"workspace.default.uat_inference_house_price_xgboost\"\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ INITIALIZATION\n",
    "# # =============================================================\n",
    "# print(\"=\"*80)\n",
    "# print(\"üöÄ UAT MODEL INFERENCE - ALIGNED VERSION\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"UAT_Inference_Aligned\").getOrCreate()\n",
    "# mlflow.set_registry_uri(\"databricks-uc\")\n",
    "# client = MlflowClient()\n",
    "\n",
    "# print(f\"\\nüìã Configuration:\")\n",
    "# print(f\"   Model: {MODEL_NAME}\")\n",
    "# print(f\"   Alias: {STAGING_ALIAS}\")\n",
    "# print(f\"   Input Table: {DELTA_INPUT_TABLE}\")\n",
    "# print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "# print(f\"   Feature Columns: {FEATURE_COLS}\")\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 1Ô∏è‚É£ Load model from STAGING alias\n",
    "# # =============================================================\n",
    "# def load_staging_model(client, model_name, alias):\n",
    "#     \"\"\"\n",
    "#     Load model from Unity Catalog using alias (aligned with staging script)\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"üìã STEP 1: Loading Model from @{alias}\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Search for model versions with staging alias\n",
    "#         model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "        \n",
    "#         # Filter versions that have the staging alias\n",
    "#         staging_versions = []\n",
    "#         for v in model_versions:\n",
    "#             full_version = client.get_model_version(model_name, v.version)\n",
    "#             aliases = [a.lower() for a in full_version.aliases] if full_version.aliases else []\n",
    "#             if alias.lower() in aliases:\n",
    "#                 staging_versions.append(full_version)\n",
    "        \n",
    "#         if not staging_versions:\n",
    "#             raise ValueError(f\"No model with alias '{alias}' found for {model_name}\")\n",
    "        \n",
    "#         # Get latest version from staging\n",
    "#         latest_staging = max(staging_versions, key=lambda x: int(x.version))\n",
    "#         version = latest_staging.version\n",
    "#         run_id = latest_staging.run_id\n",
    "        \n",
    "#         print(f\"   Found {len(staging_versions)} version(s) with @{alias} alias\")\n",
    "#         print(f\"   Loading version: v{version}\")\n",
    "        \n",
    "#         model_uri = f\"models:/{model_name}@{alias}\"\n",
    "#         model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "#         print(f\"\\n‚úÖ Model Loaded Successfully!\")\n",
    "#         print(f\"   Version: v{version}\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Status: {latest_staging.status}\")\n",
    "        \n",
    "#         # Get metric from tags if available\n",
    "#         metric_tag = latest_staging.tags.get(\"metric_rmse\", \"N/A\")\n",
    "#         print(f\"   Training RMSE: {metric_tag}\")\n",
    "        \n",
    "#         return model, version, run_id\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Failed to load model from {alias}: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise ValueError(f\"Model loading failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 2Ô∏è‚É£ Load Delta table for inference\n",
    "# # =============================================================\n",
    "# def load_data(spark):\n",
    "#     \"\"\"\n",
    "#     Load UAT data from Delta table with proper feature selection\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 2: Loading UAT Data\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"   Loading from: {DELTA_INPUT_TABLE}\")\n",
    "#         df_spark = spark.table(DELTA_INPUT_TABLE)\n",
    "#         df = df_spark.toPandas()\n",
    "\n",
    "#         print(f\"   Total rows loaded: {len(df)}\")\n",
    "#         print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "#         # Validate required columns exist\n",
    "#         missing_features = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "#         if missing_features:\n",
    "#             raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
    "\n",
    "#         if LABEL_COL not in df.columns:\n",
    "#             raise ValueError(f\"Missing label column: {LABEL_COL}\")\n",
    "\n",
    "#         # Select only required features and label\n",
    "#         X = df[FEATURE_COLS]\n",
    "#         y_true = df[LABEL_COL]\n",
    "\n",
    "#         print(f\"\\n‚úÖ Data Loaded Successfully!\")\n",
    "#         print(f\"   Features shape: {X.shape}\")\n",
    "#         print(f\"   Labels shape: {y_true.shape}\")\n",
    "        \n",
    "#         return df, X, y_true\n",
    "\n",
    "#     except Exception as e:\n",
    "#         error_msg = str(e)\n",
    "#         if \"TABLE_OR_VIEW_NOT_FOUND\" in error_msg or \"cannot be found\" in error_msg:\n",
    "#             print(f\"\\n‚ùå Delta table '{DELTA_INPUT_TABLE}' does not exist.\")\n",
    "#             print(f\"   Please create the table first or verify the table name.\")\n",
    "#             print(f\"   Expected format: catalog.schema.table_name\")\n",
    "#         else:\n",
    "#             print(f\"\\n‚ùå Failed to load input table: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise ValueError(f\"Data loading failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 3Ô∏è‚É£ Run inference\n",
    "# # =============================================================\n",
    "# def run_inference(model, X):\n",
    "#     \"\"\"\n",
    "#     Run model inference on UAT data\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 3: Running Inference\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"   Running predictions on {len(X)} samples...\")\n",
    "#         y_pred = model.predict(X)\n",
    "        \n",
    "#         print(f\"\\n‚úÖ Inference Complete!\")\n",
    "#         print(f\"   Predictions generated: {len(y_pred)}\")\n",
    "#         print(f\"   Sample predictions: {y_pred[:5]}\")\n",
    "        \n",
    "#         return y_pred\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Inference failed: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 4Ô∏è‚É£ Calculate metrics\n",
    "# # =============================================================\n",
    "# def evaluate(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Calculate evaluation metrics for UAT\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 4: Evaluating Model Performance\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         mae = mean_absolute_error(y_true, y_pred)\n",
    "#         rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "#         r2 = r2_score(y_true, y_pred)\n",
    "#         mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#         print(f\"\\nüìä Evaluation Metrics:\")\n",
    "#         print(f\"   MAE  : {mae:.3f}\")\n",
    "#         print(f\"   RMSE : {rmse:.3f}\")\n",
    "#         print(f\"   R¬≤   : {r2:.3f}\")\n",
    "#         print(f\"   MAPE : {mape:.2f}%\")\n",
    "        \n",
    "#         return mae, rmse, r2, mape\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Evaluation failed: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 5Ô∏è‚É£ Threshold validation (UAT pass/fail)\n",
    "# # =============================================================\n",
    "# def validate(mape, r2):\n",
    "#     \"\"\"\n",
    "#     Validate model performance against UAT thresholds\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 5: UAT Validation\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     print(f\"\\nüìè Validation Thresholds:\")\n",
    "#     print(f\"   MAPE threshold: ‚â§ {MAPE_THRESHOLD}%\")\n",
    "#     print(f\"   R¬≤ threshold:   ‚â• {R2_THRESHOLD}\")\n",
    "    \n",
    "#     print(f\"\\nüìä Actual Performance:\")\n",
    "#     print(f\"   MAPE: {mape:.2f}% {'‚úÖ' if mape <= MAPE_THRESHOLD else '‚ùå'}\")\n",
    "#     print(f\"   R¬≤:   {r2:.3f}  {'‚úÖ' if r2 >= R2_THRESHOLD else '‚ùå'}\")\n",
    "    \n",
    "#     if mape <= MAPE_THRESHOLD and r2 >= R2_THRESHOLD:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"‚úÖ‚úÖ UAT PASSED ‚úÖ‚úÖ\")\n",
    "#         print(f\"{'='*70}\")\n",
    "#         return \"PASSED\"\n",
    "#     else:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"‚ùå‚ùå UAT FAILED ‚ùå‚ùå\")\n",
    "#         print(f\"{'='*70}\")\n",
    "        \n",
    "#         # Show which criteria failed\n",
    "#         if mape > MAPE_THRESHOLD:\n",
    "#             print(f\"   ‚ö†Ô∏è MAPE too high: {mape:.2f}% > {MAPE_THRESHOLD}%\")\n",
    "#         if r2 < R2_THRESHOLD:\n",
    "#             print(f\"   ‚ö†Ô∏è R¬≤ too low: {r2:.3f} < {R2_THRESHOLD}\")\n",
    "        \n",
    "#         return \"FAILED\"\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ 6Ô∏è‚É£ Log results to Delta table (with smart schema handling)\n",
    "# # =============================================================\n",
    "# def log_results(spark, model_name, model_version, run_id, mae, rmse, r2, mape, status):\n",
    "#     \"\"\"\n",
    "#     Log UAT results to Delta table with duplicate prevention and backward compatibility\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 6: Logging Results\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Check if table exists and its schema\n",
    "#         table_exists = False\n",
    "        \n",
    "#         try:\n",
    "#             existing = spark.table(OUTPUT_TABLE)\n",
    "#             table_exists = True\n",
    "#             existing_df = existing.toPandas()\n",
    "#             print(f\"   Table exists: Yes\")\n",
    "#             print(f\"   Existing rows: {len(existing_df)}\")\n",
    "            \n",
    "#             # Check for duplicates\n",
    "#             if not existing_df.empty:\n",
    "#                 last = existing_df.iloc[-1]\n",
    "                \n",
    "#                 # Check if metrics are identical to last run\n",
    "#                 is_duplicate = (\n",
    "#                     int(last.model_version) == int(model_version) and\n",
    "#                     math.isclose(float(last.mae), mae, rel_tol=1e-6) and\n",
    "#                     math.isclose(float(last.rmse), rmse, rel_tol=1e-6) and\n",
    "#                     math.isclose(float(last.r2), r2, rel_tol=1e-6) and\n",
    "#                     math.isclose(float(last.mape), mape, rel_tol=1e-6)\n",
    "#                 )\n",
    "                \n",
    "#                 if is_duplicate:\n",
    "#                     print(\"\\n‚ÑπÔ∏è Duplicate Entry Detected\")\n",
    "#                     print(\"   Metrics unchanged from last run ‚Üí Skipping log\")\n",
    "#                     return\n",
    "                    \n",
    "#         except Exception as e:\n",
    "#             print(f\"   Table exists: No (will be created)\")\n",
    "#             print(f\"   Note: {e}\")\n",
    "        \n",
    "#         # Prepare data for logging\n",
    "#         result_df = pd.DataFrame([{\n",
    "#             \"timestamp\": datetime.now(),\n",
    "#             \"model_name\": model_name,\n",
    "#             \"model_version\": int(model_version),\n",
    "#             \"run_id\": run_id,\n",
    "#             \"mae\": float(mae),\n",
    "#             \"rmse\": float(rmse),\n",
    "#             \"r2\": float(r2),\n",
    "#             \"mape\": float(mape),\n",
    "#             \"uat_status\": status\n",
    "#         }])\n",
    "\n",
    "#         # Write to Delta table\n",
    "#         spark_df = spark.createDataFrame(result_df)\n",
    "        \n",
    "#         if table_exists:\n",
    "#             # Table exists - append with schema evolution if needed\n",
    "#             spark_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "#         else:\n",
    "#             # New table - create it\n",
    "#             spark_df.write.mode(\"append\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "#         print(f\"\\n‚úÖ Results Logged Successfully!\")\n",
    "#         print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "#         print(f\"   Model Name: {model_name}\")\n",
    "#         print(f\"   Model Version: v{model_version}\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   UAT Status: {status}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Failed to log results: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ MAIN EXECUTION FLOW\n",
    "# # =============================================================\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Main execution flow for UAT inference\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"üé¨ STARTING UAT INFERENCE PIPELINE\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         # Step 1: Load model\n",
    "#         model, model_version, run_id = load_staging_model(client, MODEL_NAME, STAGING_ALIAS)\n",
    "        \n",
    "#         # Step 2: Load data\n",
    "#         df, X, y_true = load_data(spark)\n",
    "        \n",
    "#         # Step 3: Run inference\n",
    "#         y_pred = run_inference(model, X)\n",
    "        \n",
    "#         # Step 4: Evaluate\n",
    "#         mae, rmse, r2, mape = evaluate(y_true, y_pred)\n",
    "        \n",
    "#         # Step 5: Validate\n",
    "#         status = validate(mape, r2)\n",
    "        \n",
    "#         # Step 6: Log results\n",
    "#         log_results(spark, MODEL_NAME, model_version, run_id, mae, rmse, r2, mape, status)\n",
    "\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"‚ú® UAT INFERENCE COMPLETED SUCCESSFULLY ‚ú®\")\n",
    "#         print(\"=\"*80)\n",
    "#         print(f\"\\nüìä Summary:\")\n",
    "#         print(f\"   Model: {MODEL_NAME}\")\n",
    "#         print(f\"   Version: v{model_version}\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   UAT Status: {status}\")\n",
    "#         print(f\"   RMSE: {rmse:.3f}\")\n",
    "#         print(f\"   MAPE: {mape:.2f}%\")\n",
    "#         print(f\"   R¬≤: {r2:.3f}\")\n",
    "#         print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"‚ùå UAT INFERENCE FAILED\")\n",
    "#         print(\"=\"*80)\n",
    "#         print(f\"Error: {str(e)}\")\n",
    "#         print(\"=\"*80 + \"\\n\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================\n",
    "# # ‚úÖ EXECUTE\n",
    "# # =============================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
