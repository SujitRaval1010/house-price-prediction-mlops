{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f235234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================================\n",
    "# üß™ UAT MODEL INFERENCE - CONFIG DRIVEN (COMPLETE FIXED VERSION)\n",
    "# =============================================================================\n",
    "# Purpose: Validate staging model performance on UAT data\n",
    "# Now reads from pipeline_config.yml - No hardcoding!\n",
    "# Prerequisites: Run 04_uat_staging.py first\n",
    "# =============================================================================\n",
    "\n",
    "# COMMAND ----------\n",
    "%pip install xgboost requests\n",
    "\n",
    "# COMMAND ----------\n",
    "# üîÑ Restart Python to use updated packages\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "# COMMAND ----------\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import traceback\n",
    "import yaml\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ UAT MODEL INFERENCE (CONFIG-DRIVEN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ LOAD PIPELINE CONFIGURATION (Dynamic Path)\n",
    "# =============================================================================\n",
    "print(\"\\nüìã Loading pipeline configuration from pipeline_config.yml...\")\n",
    "\n",
    "try:\n",
    "    # Dynamically detect project structure\n",
    "    try:\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        current_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "\n",
    "    # First try current folder (uat_env)\n",
    "    config_path = os.path.join(current_dir, \"pipeline_config.yml\")\n",
    "\n",
    "    # If not found, try dev_env folder\n",
    "    if not os.path.exists(config_path):\n",
    "        config_path = os.path.join(project_root, \"dev_env\", \"pipeline_config.yml\")\n",
    "\n",
    "    # Load the YAML config\n",
    "    with open(config_path, \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "\n",
    "    print(f\"‚úÖ Loaded pipeline_config.yml from: {config_path}\")\n",
    "\n",
    "    # Model Configuration\n",
    "    MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "    UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "    UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "    BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "    # Auto-generate model name (same as all other scripts)\n",
    "    MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}_uc2\"\n",
    "\n",
    "    STAGING_ALIAS = pipeline_cfg[\"aliases\"][\"staging\"]\n",
    "\n",
    "    # Data Configuration\n",
    "    DELTA_INPUT_TABLE = pipeline_cfg[\"data\"][\"input_table\"]\n",
    "    FEATURE_COLS = pipeline_cfg[\"data\"][\"features\"]\n",
    "    LABEL_COL = pipeline_cfg[\"data\"][\"label\"]\n",
    "\n",
    "    # UAT Thresholds\n",
    "    MAPE_THRESHOLD = pipeline_cfg[\"uat\"][\"mape_threshold\"]\n",
    "    R2_THRESHOLD = pipeline_cfg[\"uat\"][\"r2_threshold\"]\n",
    "\n",
    "    # Output Table\n",
    "    OUTPUT_TABLE = pipeline_cfg[\"tables\"][\"uat_results\"]\n",
    "\n",
    "    print(f\"‚úÖ Configuration loaded successfully!\")\n",
    "    print(f\"\\nüìä Configuration Details:\")\n",
    "    print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "    print(f\"   Model Name: {MODEL_NAME}\")\n",
    "    print(f\"   Staging Alias: @{STAGING_ALIAS}\")\n",
    "    print(f\"   Input Table: {DELTA_INPUT_TABLE}\")\n",
    "    print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "    print(f\"   Features: {FEATURE_COLS}\")\n",
    "    print(f\"   Label: {LABEL_COL}\")\n",
    "    print(f\"   MAPE Threshold: ‚â§ {MAPE_THRESHOLD}%\")\n",
    "    print(f\"   R¬≤ Threshold: ‚â• {R2_THRESHOLD}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: pipeline_config.yml not found!\")\n",
    "    print(\"üí° Please create pipeline_config.yml in the same directory or in dev_env/\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading configuration: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ SLACK NOTIFICATION SETUP\n",
    "# =============================================================================\n",
    "def get_slack_webhook():\n",
    "    \"\"\"Retrieve Slack webhook from secrets with fallback scopes\"\"\"\n",
    "    for scope in [\"shared-scope\", \"dev-scope\"]:\n",
    "        try:\n",
    "            webhook = dbutils.secrets.get(scope, \"SLACK_WEBHOOK_URL\")\n",
    "            if webhook and webhook.strip():\n",
    "                print(f\"‚úì Slack webhook configured from scope '{scope}'\")\n",
    "                return webhook\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Slack webhook not found in scope '{scope}': {e}\")\n",
    "    return None\n",
    "\n",
    "SLACK_WEBHOOK_URL = get_slack_webhook()\n",
    "\n",
    "def send_slack_notification(message, level=\"info\"):\n",
    "    \"\"\"Send notification to Slack channel\"\"\"\n",
    "    if not SLACK_WEBHOOK_URL:\n",
    "        print(f\"‚ö†Ô∏è Slack webhook not configured\")\n",
    "        print(f\"üì¢ Message: {message}\")\n",
    "        return\n",
    "    \n",
    "    emoji_map = {\n",
    "        \"info\": \"‚ÑπÔ∏è\",\n",
    "        \"success\": \"‚úÖ\",\n",
    "        \"warning\": \"‚ö†Ô∏è\",\n",
    "        \"error\": \"‚ùå\"\n",
    "    }\n",
    "    \n",
    "    formatted_message = f\"{emoji_map.get(level, '‚ÑπÔ∏è')} {message}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            SLACK_WEBHOOK_URL, \n",
    "            json={\"text\": formatted_message},\n",
    "            timeout=5\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Slack notification sent: {level}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Slack notification failed: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error sending Slack notification: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ INITIALIZATION\n",
    "# =============================================================================\n",
    "spark = SparkSession.builder.appName(\"UAT_Inference\").getOrCreate()\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"\\n‚úÖ MLflow and Spark initialized\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 1: LOAD MODEL FROM STAGING ALIAS\n",
    "# =============================================================================\n",
    "def load_staging_model():\n",
    "    \"\"\"Load model from Unity Catalog using Staging alias\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìã STEP 1: Loading Model from @{STAGING_ALIAS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Try direct alias lookup\n",
    "        print(f\"‚è≥ Attempting to load: models:/{MODEL_NAME}@{STAGING_ALIAS}\")\n",
    "        \n",
    "        try:\n",
    "            model_version = client.get_model_version_by_alias(MODEL_NAME, STAGING_ALIAS)\n",
    "            version = model_version.version\n",
    "            run_id = model_version.run_id\n",
    "            \n",
    "            print(f\"‚úÖ Found model with @{STAGING_ALIAS} alias\")\n",
    "            print(f\"   Version: v{version}\")\n",
    "            print(f\"   Run ID: {run_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Direct alias lookup failed: {e}\")\n",
    "            print(f\"   Trying alternative search method...\")\n",
    "            \n",
    "            # Method 2: Search through all versions\n",
    "            model_versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
    "            \n",
    "            if not model_versions:\n",
    "                raise ValueError(\n",
    "                    f\"‚ùå No model versions found for {MODEL_NAME}\\n\"\n",
    "                    f\"üí° Solution: Run Model_Registration script first\"\n",
    "                )\n",
    "            \n",
    "            # Filter versions with the staging alias\n",
    "            staging_versions = []\n",
    "            print(f\"\\nüîç Searching through {len(model_versions)} version(s)...\")\n",
    "            \n",
    "            for v in model_versions:\n",
    "                full_version = client.get_model_version(MODEL_NAME, v.version)\n",
    "                version_aliases = full_version.aliases if full_version.aliases else []\n",
    "                \n",
    "                # Case-insensitive comparison\n",
    "                if any(alias.lower() == STAGING_ALIAS.lower() for alias in version_aliases):\n",
    "                    staging_versions.append(full_version)\n",
    "                    print(f\"   ‚úì Version v{v.version} has @{STAGING_ALIAS} alias\")\n",
    "            \n",
    "            if not staging_versions:\n",
    "                # List available versions for debugging\n",
    "                print(f\"\\n‚ùå No model with alias '@{STAGING_ALIAS}' found!\")\n",
    "                print(f\"\\nüìã Available versions for {MODEL_NAME}:\")\n",
    "                for v in model_versions[:10]:\n",
    "                    full_v = client.get_model_version(MODEL_NAME, v.version)\n",
    "                    v_aliases = full_v.aliases if full_v.aliases else [\"No aliases\"]\n",
    "                    print(f\"   Version v{v.version}: Aliases = {v_aliases}\")\n",
    "                \n",
    "                raise ValueError(\n",
    "                    f\"\\n‚ùå No model with alias '@{STAGING_ALIAS}' found\\n\"\n",
    "                    f\"üí° Solution: Run 04_uat_staging.py first\"\n",
    "                )\n",
    "            \n",
    "            # Get latest version from staging\n",
    "            model_version = max(staging_versions, key=lambda x: int(x.version))\n",
    "            version = model_version.version\n",
    "            run_id = model_version.run_id\n",
    "            \n",
    "            print(f\"\\n‚úÖ Found {len(staging_versions)} version(s) with @{STAGING_ALIAS} alias\")\n",
    "            print(f\"   Loading latest: v{version}\")\n",
    "        \n",
    "        # Load the model\n",
    "        model_uri = f\"models:/{MODEL_NAME}@{STAGING_ALIAS}\"\n",
    "        print(f\"\\n‚è≥ Loading model...\")\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ MODEL LOADED SUCCESSFULLY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Model: {MODEL_NAME}\")\n",
    "        print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "        print(f\"   Version: v{version}\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Status: {model_version.status}\")\n",
    "        \n",
    "        # Get metric from tags if available\n",
    "        metric_tag = model_version.tags.get(\"metric_rmse\", \"N/A\")\n",
    "        print(f\"   Training RMSE: {metric_tag}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return model, version, run_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚ùå FAILED TO LOAD MODEL\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"\\nüí° Troubleshooting Steps:\")\n",
    "        print(f\"   1. Verify model exists: {MODEL_NAME}\")\n",
    "        print(f\"   2. Run 04_uat_staging.py to promote a model to @{STAGING_ALIAS}\")\n",
    "        print(f\"   3. Verify alias is exactly '{STAGING_ALIAS}' (case-sensitive)\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 2: LOAD UAT DATA\n",
    "# =============================================================================\n",
    "def load_uat_data():\n",
    "    \"\"\"Load UAT data from Delta table\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 2: Loading UAT Data\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"   Loading from: {DELTA_INPUT_TABLE}\")\n",
    "        df_spark = spark.table(DELTA_INPUT_TABLE)\n",
    "        df = df_spark.toPandas()\n",
    "\n",
    "        print(f\"   Total rows: {len(df)}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "        # Validate required columns\n",
    "        missing_features = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
    "\n",
    "        if LABEL_COL not in df.columns:\n",
    "            raise ValueError(f\"Missing label column: {LABEL_COL}\")\n",
    "\n",
    "        # Select features and labels\n",
    "        X = df[FEATURE_COLS]\n",
    "        y_true = df[LABEL_COL]\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ DATA LOADED SUCCESSFULLY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Labels shape: {y_true.shape}\")\n",
    "        print(f\"   Sample features:\\n{X.head(3)}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return df, X, y_true\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚ùå FAILED TO LOAD DATA\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in error_msg or \"cannot be found\" in error_msg:\n",
    "            print(f\"   Delta table '{DELTA_INPUT_TABLE}' does not exist\")\n",
    "            print(f\"\\nüí° Solution:\")\n",
    "            print(f\"   1. Create the table first\")\n",
    "            print(f\"   2. Verify the table name in pipeline_config.yml\")\n",
    "        else:\n",
    "            print(f\"   Error: {e}\")\n",
    "        \n",
    "        print(f\"{'='*80}\\n\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 3: RUN INFERENCE\n",
    "# =============================================================================\n",
    "def run_inference(model, X):\n",
    "    \"\"\"Run model inference on UAT data\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 3: Running Inference\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"   Generating predictions for {len(X)} samples...\")\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ INFERENCE COMPLETE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Predictions generated: {len(y_pred)}\")\n",
    "        print(f\"   Sample predictions: {y_pred[:5]}\")\n",
    "        print(f\"   Min: {y_pred.min():.2f}, Max: {y_pred.max():.2f}, Mean: {y_pred.mean():.2f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚ùå INFERENCE FAILED\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 4: CALCULATE METRICS\n",
    "# =============================================================================\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 4: Evaluating Model Performance\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        print(f\"\\nüìä Evaluation Metrics:\")\n",
    "        print(f\"   MAE  : {mae:>12,.2f}\")\n",
    "        print(f\"   RMSE : {rmse:>12,.2f}\")\n",
    "        print(f\"   R¬≤   : {r2:>12.4f}\")\n",
    "        print(f\"   MAPE : {mape:>12.2f}%\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return mae, rmse, r2, mape\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Evaluation failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 5: UAT VALIDATION\n",
    "# =============================================================================\n",
    "def validate_uat(mape, r2, model_version):\n",
    "    \"\"\"Validate model against UAT thresholds\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 5: UAT Validation\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\nüìè Validation Thresholds:\")\n",
    "    print(f\"   MAPE: ‚â§ {MAPE_THRESHOLD}%\")\n",
    "    print(f\"   R¬≤:   ‚â• {R2_THRESHOLD}\")\n",
    "\n",
    "    print(f\"\\nüìä Actual Performance:\")\n",
    "    mape_pass = mape <= MAPE_THRESHOLD\n",
    "    r2_pass = r2 >= R2_THRESHOLD\n",
    "    \n",
    "    print(f\"   MAPE: {mape:.2f}% {'‚úÖ PASS' if mape_pass else '‚ùå FAIL'}\")\n",
    "    print(f\"   R¬≤:   {r2:.4f}  {'‚úÖ PASS' if r2_pass else '‚ùå FAIL'}\")\n",
    "\n",
    "    if mape_pass and r2_pass:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ‚úÖ UAT PASSED ‚úÖ‚úÖ\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Model v{model_version} is ready for production!\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "        send_slack_notification(\n",
    "            f\"‚úÖ Model `{MODEL_NAME}` (Type: {MODEL_TYPE.upper()}) v{model_version} PASSED UAT\\n\"\n",
    "            f\"üìä MAPE: {mape:.2f}%, R¬≤: {r2:.4f}\\n\"\n",
    "            f\"üöÄ Ready for production promotion!\",\n",
    "            level=\"success\"\n",
    "        )\n",
    "        return \"PASSED\"\n",
    "    else:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚ùå‚ùå UAT FAILED ‚ùå‚ùå\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        fail_reasons = []\n",
    "        if not mape_pass:\n",
    "            fail_reasons.append(f\"MAPE too high ({mape:.2f}% > {MAPE_THRESHOLD}%)\")\n",
    "        if not r2_pass:\n",
    "            fail_reasons.append(f\"R¬≤ too low ({r2:.4f} < {R2_THRESHOLD})\")\n",
    "\n",
    "        print(f\"   Failure reasons:\")\n",
    "        for reason in fail_reasons:\n",
    "            print(f\"   ‚Ä¢ {reason}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "        send_slack_notification(\n",
    "            f\"‚ùå Model `{MODEL_NAME}` (Type: {MODEL_TYPE.upper()}) v{model_version} FAILED UAT\\n\"\n",
    "            f\"üìä MAPE: {mape:.2f}%, R¬≤: {r2:.4f}\\n\"\n",
    "            f\"üö´ Reasons: {', '.join(fail_reasons)}\",\n",
    "            level=\"error\"\n",
    "        )\n",
    "\n",
    "        return \"FAILED\"\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ STEP 6: LOG RESULTS  (FIXED VERSION)\n",
    "# =============================================================================\n",
    "def log_results(model_version, run_id, mae, rmse, r2, mape, status):\n",
    "    \"\"\"Log UAT results to Delta table (type-safe, merge-safe)\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã STEP 6: Logging Results\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists\n",
    "        table_exists = False\n",
    "        existing_df = None\n",
    "        \n",
    "        try:\n",
    "            existing_df = spark.table(OUTPUT_TABLE).toPandas()\n",
    "            table_exists = True\n",
    "            print(f\"   Table exists: Yes  |  Rows: {len(existing_df)}\")\n",
    "        except Exception:\n",
    "            print(f\"   Table exists: No (will be created)\")\n",
    "\n",
    "        # Prepare result data\n",
    "        result_df = pd.DataFrame([{\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"model_type\": MODEL_TYPE,\n",
    "            \"model_version\": str(model_version),   # üîß Cast to string to match schema\n",
    "            \"run_id\": run_id,\n",
    "            \"mae\": float(mae),\n",
    "            \"rmse\": float(rmse),\n",
    "            \"r2\": float(r2),\n",
    "            \"mape\": float(mape),\n",
    "            \"uat_status\": status,\n",
    "            \"mape_threshold\": float(MAPE_THRESHOLD),\n",
    "            \"r2_threshold\": float(R2_THRESHOLD)\n",
    "        }])\n",
    "\n",
    "        # Convert to Spark DF\n",
    "        spark_df = spark.createDataFrame(result_df)\n",
    "\n",
    "        # üîß Force schema alignment with existing table if present\n",
    "        if table_exists:\n",
    "            target_schema = spark.table(OUTPUT_TABLE).schema\n",
    "            # Convert model_version to string if target expects string\n",
    "            target_field = next((f for f in target_schema if f.name == \"model_version\"), None)\n",
    "            if target_field and str(target_field.dataType).lower().startswith(\"string\"):\n",
    "                spark_df = spark_df.withColumn(\"model_version\", spark_df[\"model_version\"].cast(\"string\"))\n",
    "            else:\n",
    "                spark_df = spark_df.withColumn(\"model_version\", spark_df[\"model_version\"].cast(\"int\"))\n",
    "\n",
    "            # Append safely\n",
    "            spark_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "        else:\n",
    "            spark_df.write.mode(\"append\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ RESULTS LOGGED SUCCESSFULLY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Output Table: {OUTPUT_TABLE}\")\n",
    "        print(f\"   Model: {MODEL_NAME}\")\n",
    "        print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "        print(f\"   Version: v{model_version}\")\n",
    "        print(f\"   UAT Status: {status}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Failed to log results: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ MAIN EXECUTION\n",
    "# =============================================================================\n",
    "def main():\n",
    "    \"\"\"Main UAT inference pipeline\"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üé¨ STARTING UAT INFERENCE PIPELINE\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        # Execute pipeline steps\n",
    "        model, model_version, run_id = load_staging_model()\n",
    "        df, X, y_true = load_uat_data()\n",
    "        y_pred = run_inference(model, X)\n",
    "        mae, rmse, r2, mape = evaluate_model(y_true, y_pred)\n",
    "        status = validate_uat(mape, r2, model_version)\n",
    "        log_results(model_version, run_id, mae, rmse, r2, mape, status)\n",
    "\n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ú® UAT INFERENCE COMPLETED SUCCESSFULLY ‚ú®\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nüìä Final Summary:\")\n",
    "        print(f\"   Model: {MODEL_NAME}\")\n",
    "        print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "        print(f\"   Version: v{model_version}\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   UAT Status: {status}\")\n",
    "        print(f\"   Metrics:\")\n",
    "        print(f\"     ‚Ä¢ RMSE: {rmse:,.2f}\")\n",
    "        print(f\"     ‚Ä¢ MAPE: {mape:.2f}%\")\n",
    "        print(f\"     ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "        print(f\"     ‚Ä¢ MAE:  {mae:,.2f}\")\n",
    "        \n",
    "        if status == \"PASSED\":\n",
    "            print(f\"\\nüìå Next Step:\")\n",
    "            print(f\"   Run 06_production_promotion.py to promote to production\")\n",
    "        \n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        # Save for workflow\n",
    "        try:\n",
    "            dbutils.jobs.taskValues.set(key=\"uat_status\", value=status)\n",
    "            dbutils.jobs.taskValues.set(key=\"uat_mape\", value=mape)\n",
    "            dbutils.jobs.taskValues.set(key=\"uat_r2\", value=r2)\n",
    "            print(\"‚úÖ Task values saved for workflow\")\n",
    "        except:\n",
    "            print(\"‚ÑπÔ∏è Not running in workflow - skipping task values\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ùå UAT INFERENCE FAILED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        send_slack_notification(\n",
    "            f\"‚ùå UAT pipeline failed for `{MODEL_NAME}` (Type: {MODEL_TYPE.upper()})\\n\"\n",
    "            f\"Error: {str(e)}\",\n",
    "            level=\"error\"\n",
    "        )\n",
    "        \n",
    "        sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ EXECUTE\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
