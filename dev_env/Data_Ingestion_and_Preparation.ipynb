{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34641b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# data_ingest_and_preparation.py\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DELTA_TABLE_NAME = \"house_price_delta\"\n",
    "OUTPUT_DELTA_TABLE = \"house_price_scaled_delta\"\n",
    "\n",
    "# ==================== FUNCTIONS ====================\n",
    "\n",
    "def ingest_data(table_name):\n",
    "    \"\"\"Delta table ‡§∏‡•á ‡§°‡•á‡§ü‡§æ ingest ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\"\"\"\n",
    "    print(f\"üì• Loading data from Delta table '{table_name}' ...\")\n",
    "    try:\n",
    "        # spark variable Databricks ‡§Æ‡•á‡§Ç globally available ‡§π‡•à\n",
    "        df = spark.read.format(\"delta\").table(table_name)\n",
    "        feature_cols = [\"sq_feet\", \"num_bedrooms\", \"num_bathrooms\", \"year_built\", \"location_score\"]\n",
    "        label_col = \"price\"\n",
    "\n",
    "        # Ensure all columns are double for scaler\n",
    "        for c in feature_cols + [label_col]:\n",
    "            df = df.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "        df = df.select(*feature_cols, col(label_col).alias(\"label\"))\n",
    "\n",
    "        print(f\"‚úÖ Data successfully ingested. Total rows: {df.count()}\")\n",
    "        df.printSchema()\n",
    "        return df, feature_cols\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data ingestion failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_data(df, feature_cols):\n",
    "    \"\"\"Feature scaling (StandardScaler) ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ scaled data ‡§ï‡•ã Delta ‡§Æ‡•á‡§Ç save ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\"\"\"\n",
    "    print(\"\\n‚öôÔ∏è Feature scaling started...\")\n",
    "\n",
    "    # Step 0: Handle missing / NaN values\n",
    "    print(\"üßπ Checking and filling missing values...\")\n",
    "    df = df.na.fill(0)\n",
    "\n",
    "    # Step 1: Assemble features\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vector\")\n",
    "\n",
    "    # Step 2: Apply StandardScaler\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_vector\", \n",
    "        outputCol=\"scaled_features\", \n",
    "        withMean=True, \n",
    "        withStd=True\n",
    "    )\n",
    "\n",
    "    # Step 3: Build pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    try:\n",
    "        print(\"üîÑ Fitting scaling pipeline...\")\n",
    "        model = pipeline.fit(df)\n",
    "        scaled_data = model.transform(df)\n",
    "        print(\"‚úÖ Pipeline fitted and transformed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Scaling pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "    # Step 4: Extract individual scaled columns\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    for i, c in enumerate(feature_cols):\n",
    "        scaled_data = scaled_data.withColumn(f\"{c}_scaled\", col(\"scaled_array\")[i])\n",
    "\n",
    "    # Step 5: Select final columns\n",
    "    final_cols = [f\"{c}_scaled\" for c in feature_cols] + [\"label\"]\n",
    "    scaled_final_df = scaled_data.select(*final_cols)\n",
    "\n",
    "    print(\"‚úÖ Feature scaling completed successfully.\")\n",
    "    scaled_final_df.show(5)\n",
    "\n",
    "    # Step 6: Save to Delta table (overwrite mode)\n",
    "    try:\n",
    "        print(f\"üíæ Saving scaled data to Delta table '{OUTPUT_DELTA_TABLE}'...\")\n",
    "        scaled_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_DELTA_TABLE)\n",
    "        print(f\"‚úÖ Scaled data successfully saved to Delta table '{OUTPUT_DELTA_TABLE}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save scaled data to Delta: {e}\")\n",
    "        raise\n",
    "\n",
    "    return scaled_final_df\n",
    "\n",
    "# ==================== EXECUTION ====================\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ DATA INGESTION AND PREPARATION PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Verify spark session is available\n",
    "        print(f\"‚úÖ Using Databricks SparkSession: {spark.version}\")\n",
    "    except NameError:\n",
    "        print(\"‚ùå Error: 'spark' variable not found!\")\n",
    "        print(\"This script must be run in a Databricks notebook\")\n",
    "        raise Exception(\"SparkSession not available\")\n",
    "\n",
    "    # Step 1: Data ingestion\n",
    "    ingested_df, feature_cols = ingest_data(DELTA_TABLE_NAME)\n",
    "\n",
    "    # Step 2: Feature scaling and Delta save\n",
    "    scaled_df = prepare_data(ingested_df, feature_cols)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ DATA PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úÖ Scaled data available in: {OUTPUT_DELTA_TABLE}\")\n",
    "    print(f\"‚úÖ Total records processed: {scaled_df.count()}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "try:\n",
    "    main()\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ùå PIPELINE FAILED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Fail the Databricks job\n",
    "    try:\n",
    "        dbutils.notebook.exit(\"FAILED\")\n",
    "    except:\n",
    "        raise  # Re-raise for non-notebook environments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
