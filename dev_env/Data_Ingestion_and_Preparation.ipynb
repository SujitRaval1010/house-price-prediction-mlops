{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34641b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ingest.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DELTA_TABLE_NAME = \"house_price_delta\"\n",
    "\n",
    "# Global variables to hold data/features for the next script (in a Databricks Notebook environment)\n",
    "# In a true MLOps pipeline, you would save this to another Delta table or a file (e.g., /tmp/data.parquet)\n",
    "global TRAINING_DATA_DF\n",
    "global FEATURE_COLUMNS\n",
    "TRAINING_DATA_DF = None\n",
    "FEATURE_COLUMNS = None \n",
    "\n",
    "# ==================== FUNCTIONS ====================\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"SparkSession को इनिशियलाइज़ करता है (यदि पहले से नहीं है)\"\"\"\n",
    "    # Databricks में, SparkSession पहले से ही 'spark' variable में उपलब्ध होती है।\n",
    "    if 'spark' in globals() and isinstance(globals()['spark'], SparkSession):\n",
    "        return globals()['spark']\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"DataIngest\").getOrCreate()\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"SparkSession initialization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_data(spark: SparkSession, table_name: str):\n",
    "    \"\"\"\n",
    "    डेल्टा टेबल से हाउस प्राइस डेटा को इंगेस्ट करता है और फीचर्स/लेबल को अलग करता है।\n",
    "    \"\"\"\n",
    "    if spark is None:\n",
    "        print(\"❌ SparkSession उपलब्ध नहीं है। डेटा इंगेस्ट नहीं किया जा सकता।\")\n",
    "        return None, None\n",
    "        \n",
    "    print(f\"डेल्टा टेबल '{table_name}' से डेटा लोड हो रहा है...\")\n",
    "\n",
    "    try:\n",
    "        # डेल्टा टेबल से डेटा पढ़ें\n",
    "        house_df = spark.read.format(\"delta\").table(table_name)\n",
    "        \n",
    "        feature_cols = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "        label_col = 'price'\n",
    "        \n",
    "        # फ़ीचर्स (Features) और लेबल (Label) को अलग करें (price को label के रूप में)\n",
    "        ingested_df = house_df.select(*feature_cols, col(label_col).alias(\"label\"))\n",
    "        \n",
    "        print(f\"डेटा सफलतापूर्वक इंगेस्ट हुआ। कुल पंक्तियाँ: {ingested_df.count()}\")\n",
    "        ingested_df.printSchema()\n",
    "        \n",
    "        return ingested_df, feature_cols\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"डेटा इंगेस्ट में त्रुटि: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==================== EXECUTION ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = initialize_spark()\n",
    "    ingested_data_df, features = ingest_data(spark_session, DELTA_TABLE_NAME)\n",
    "\n",
    "    if ingested_data_df is not None:\n",
    "        # ग्लोबल वेरिएबल में सेव करें ताकि ट्रेनिंग स्क्रिप्ट इसका उपयोग कर सके\n",
    "        # (ध्यान दें: Databricks Notebooks में, आप MAGIC COMMAND्स जैसे %run या job parameters का उपयोग करेंगे)\n",
    "        globals()['TRAINING_DATA_DF'] = ingested_data_df\n",
    "        globals()['FEATURE_COLUMNS'] = features\n",
    "        print(\"✅ डेटा को ग्लोबल वेरिएबल्स में सेव किया गया।\")\n",
    "    else:\n",
    "        print(\"❌ डेटा इंगेस्ट विफल रहा।\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
