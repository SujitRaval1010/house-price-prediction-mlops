{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34641b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ingest_and_preparation.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DELTA_TABLE_NAME = \"house_price_delta\"\n",
    "OUTPUT_DELTA_TABLE = \"house_price_scaled_delta\"\n",
    "\n",
    "# ==================== FUNCTIONS ====================\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"SparkSession ‡§ï‡•ã ‡§á‡§®‡§ø‡§∂‡§ø‡§Ø‡§≤‡§æ‡§á‡§ú‡§º ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (‡§Ø‡§¶‡§ø ‡§™‡§π‡§≤‡•á ‡§∏‡•á ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à)\"\"\"\n",
    "    # Databricks ‡§Æ‡•á‡§Ç, SparkSession ‡§™‡§π‡§≤‡•á ‡§∏‡•á ‡§π‡•Ä 'spark' variable ‡§Æ‡•á‡§Ç ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§\n",
    "    if 'spark' in globals() and isinstance(globals()['spark'], SparkSession):\n",
    "        print(\"‚úÖ Using existing Databricks SparkSession\")\n",
    "        return globals()['spark']\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"DataIngestAndPreparation\").getOrCreate()\n",
    "        print(\"‚úÖ SparkSession initialized successfully\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SparkSession initialization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_data(spark, table_name):\n",
    "    \"\"\"Delta table ‡§∏‡•á ‡§°‡•á‡§ü‡§æ ingest ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\"\"\"\n",
    "    if spark is None:\n",
    "        print(\"‚ùå SparkSession ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§ ‡§°‡•á‡§ü‡§æ ‡§á‡§Ç‡§ó‡•á‡§∏‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ‡•§\")\n",
    "        return None, None\n",
    "        \n",
    "    print(f\"üì• Loading data from Delta table '{table_name}' ...\")\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").table(table_name)\n",
    "        feature_cols = [\"sq_feet\", \"num_bedrooms\", \"num_bathrooms\", \"year_built\", \"location_score\"]\n",
    "        label_col = \"price\"\n",
    "\n",
    "        # Ensure all columns are double for scaler\n",
    "        for c in feature_cols + [label_col]:\n",
    "            df = df.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "        df = df.select(*feature_cols, col(label_col).alias(\"label\"))\n",
    "\n",
    "        print(f\"‚úÖ Data successfully ingested. Total rows: {df.count()}\")\n",
    "        df.printSchema()\n",
    "        return df, feature_cols\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data ingestion failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def prepare_data(spark, df, feature_cols):\n",
    "    \"\"\"Feature scaling (StandardScaler) ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ scaled data ‡§ï‡•ã Delta ‡§Æ‡•á‡§Ç save ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\"\"\"\n",
    "    if spark is None or df is None:\n",
    "        print(\"‚ùå Invalid SparkSession or DataFrame. Cannot proceed with scaling.\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\n‚öôÔ∏è Feature scaling started...\")\n",
    "\n",
    "    # Step 0: Handle missing / NaN values\n",
    "    print(\"üßπ Checking and filling missing values...\")\n",
    "    df = df.na.fill(0)\n",
    "\n",
    "    # Step 1: Assemble features\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vector\")\n",
    "\n",
    "    # Step 2: Apply StandardScaler\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_vector\", \n",
    "        outputCol=\"scaled_features\", \n",
    "        withMean=True, \n",
    "        withStd=True\n",
    "    )\n",
    "\n",
    "    # Step 3: Build pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    try:\n",
    "        print(\"üîÑ Fitting scaling pipeline...\")\n",
    "        model = pipeline.fit(df)\n",
    "        scaled_data = model.transform(df)\n",
    "        print(\"‚úÖ Pipeline fitted and transformed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Scaling pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    # Step 4: Extract individual scaled columns\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    for i, c in enumerate(feature_cols):\n",
    "        scaled_data = scaled_data.withColumn(f\"{c}_scaled\", col(\"scaled_array\")[i])\n",
    "\n",
    "    # Step 5: Select final columns\n",
    "    final_cols = [f\"{c}_scaled\" for c in feature_cols] + [\"label\"]\n",
    "    scaled_final_df = scaled_data.select(*final_cols)\n",
    "\n",
    "    print(\"‚úÖ Feature scaling completed successfully.\")\n",
    "    scaled_final_df.show(5)\n",
    "\n",
    "    # Step 6: Save to Delta table (overwrite mode)\n",
    "    try:\n",
    "        print(f\"üíæ Saving scaled data to Delta table '{OUTPUT_DELTA_TABLE}'...\")\n",
    "        scaled_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_DELTA_TABLE)\n",
    "        print(f\"‚úÖ Scaled data successfully saved to Delta table '{OUTPUT_DELTA_TABLE}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save scaled data to Delta: {e}\")\n",
    "        return None\n",
    "\n",
    "    return scaled_final_df\n",
    "\n",
    "# ==================== EXECUTION ====================\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ DATA INGESTION AND PREPARATION PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize SparkSession (uses Databricks global 'spark' if available)\n",
    "    spark_session = initialize_spark()\n",
    "    \n",
    "    if spark_session is None:\n",
    "        print(\"‚ùå Cannot proceed without SparkSession\")\n",
    "        raise Exception(\"SparkSession initialization failed\")\n",
    "\n",
    "    # Step 1: Data ingestion\n",
    "    ingested_df, feature_cols = ingest_data(spark_session, DELTA_TABLE_NAME)\n",
    "    \n",
    "    if ingested_df is None or feature_cols is None:\n",
    "        print(\"‚ùå Data ingestion failed. Pipeline cannot continue.\")\n",
    "        raise Exception(\"Data ingestion failed\")\n",
    "\n",
    "    # Step 2: Feature scaling and Delta save\n",
    "    scaled_df = prepare_data(spark_session, ingested_df, feature_cols)\n",
    "    \n",
    "    if scaled_df is None:\n",
    "        print(\"‚ùå Data preparation failed. Pipeline cannot continue.\")\n",
    "        raise Exception(\"Data preparation failed\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ DATA PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úÖ Scaled data available in: {OUTPUT_DELTA_TABLE}\")\n",
    "    print(f\"‚úÖ Total records processed: {scaled_df.count()}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"‚ùå PIPELINE FAILED\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise  # Re-raise to fail the Databricks job"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
