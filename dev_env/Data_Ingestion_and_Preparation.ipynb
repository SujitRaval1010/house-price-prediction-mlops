{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34641b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ingest_and_preparation.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DELTA_TABLE_NAME = \"house_price_delta\"\n",
    "OUTPUT_DELTA_TABLE = \"house_price_scaled_delta\"\n",
    "\n",
    "# Global variables\n",
    "global TRAINING_DATA_DF\n",
    "global FEATURE_COLUMNS\n",
    "TRAINING_DATA_DF = None\n",
    "FEATURE_COLUMNS = None\n",
    "\n",
    "# ==================== FUNCTIONS ====================\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"SparkSession को इनिशियलाइज़ करता है\"\"\"\n",
    "    if 'spark' in globals() and isinstance(globals()['spark'], SparkSession):\n",
    "        return globals()['spark']\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"DataIngestAndPreparation\").getOrCreate()\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"SparkSession initialization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_data(spark: SparkSession, table_name: str):\n",
    "    \"\"\"डेल्टा टेबल से हाउस प्राइस डेटा को इंगेस्ट करता है\"\"\"\n",
    "    if spark is None:\n",
    "        print(\"❌ SparkSession उपलब्ध नहीं है। डेटा इंगेस्ट नहीं किया जा सकता।\")\n",
    "        return None, None\n",
    "        \n",
    "    print(f\"डेल्टा टेबल '{table_name}' से डेटा लोड हो रहा है...\")\n",
    "\n",
    "    try:\n",
    "        house_df = spark.read.format(\"delta\").table(table_name)\n",
    "        \n",
    "        feature_cols = ['sq_feet', 'num_bedrooms', 'num_bathrooms', 'year_built', 'location_score']\n",
    "        label_col = 'price'\n",
    "        \n",
    "        ingested_df = house_df.select(*feature_cols, col(label_col).alias(\"label\"))\n",
    "        \n",
    "        print(f\"✅ डेटा सफलतापूर्वक इंगेस्ट हुआ। कुल पंक्तियाँ: {ingested_df.count()}\")\n",
    "        ingested_df.printSchema()\n",
    "        \n",
    "        return ingested_df, feature_cols\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"डेटा इंगेस्ट में त्रुटि: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def prepare_data(spark, df, feature_cols):\n",
    "    \"\"\"फीचर स्केलिंग (StandardScaler) लागू करता है\"\"\"\n",
    "    print(\"\\n⚙️ फीचर स्केलिंग शुरू हो रही है...\")\n",
    "\n",
    "    # Step 1: Assemble features into a single vector\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vector\")\n",
    "\n",
    "    # Step 2: Apply StandardScaler\n",
    "    scaler = StandardScaler(inputCol=\"features_vector\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "\n",
    "    # Step 3: Pipeline for transformation\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    model = pipeline.fit(df)\n",
    "    scaled_data = model.transform(df)\n",
    "\n",
    "    # Step 4: Scaled features को अलग-अलग कॉलम में बदलना\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    for i, c in enumerate(feature_cols):\n",
    "        scaled_data = scaled_data.withColumn(f\"{c}_scaled\", col(\"scaled_array\")[i])\n",
    "\n",
    "    # Step 5: Final scaled DataFrame बनाना (scaled features + label)\n",
    "    final_cols = [f\"{c}_scaled\" for c in feature_cols] + [\"label\"]\n",
    "    scaled_final_df = scaled_data.select(*final_cols)\n",
    "\n",
    "    print(\"✅ फीचर स्केलिंग पूरी हुई।\")\n",
    "    scaled_final_df.show(5)\n",
    "\n",
    "    # Step 6: Scaled data को Delta table में सेव करना\n",
    "    scaled_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_DELTA_TABLE)\n",
    "    print(f\"✅ स्केल्ड डेटा Delta table '{OUTPUT_DELTA_TABLE}' में सेव हो गया।\")\n",
    "\n",
    "    return scaled_final_df\n",
    "\n",
    "\n",
    "# ==================== EXECUTION ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = initialize_spark()\n",
    "    ingested_data_df, features = ingest_data(spark_session, DELTA_TABLE_NAME)\n",
    "\n",
    "    if ingested_data_df is not None:\n",
    "        globals()['TRAINING_DATA_DF'] = ingested_data_df\n",
    "        globals()['FEATURE_COLUMNS'] = features\n",
    "        print(\"✅ डेटा को ग्लोबल वेरिएबल्स में सेव किया गया।\")\n",
    "\n",
    "        # Feature scaling step\n",
    "        scaled_df = prepare_data(spark_session, ingested_data_df, features)\n",
    "\n",
    "    else:\n",
    "        print(\"❌ डेटा इंगेस्ट विफल रहा।\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
