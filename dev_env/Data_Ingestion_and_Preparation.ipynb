{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34641b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ingest_and_preparation.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DELTA_TABLE_NAME = \"house_price_delta\"\n",
    "OUTPUT_DELTA_TABLE = \"house_price_scaled_delta\"\n",
    "\n",
    "# ==================== FUNCTIONS ====================\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"SparkSession initialize ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (Databricks job compatible)\"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"DataIngestAndPreparation\") \\\n",
    "            .getOrCreate()\n",
    "        print(\"‚úÖ SparkSession initialized successfully\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SparkSession initialization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def ingest_data(spark, table_name):\n",
    "    \"\"\"Delta table ‡§∏‡•á ‡§°‡•á‡§ü‡§æ ingest ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\"\"\"\n",
    "    print(f\"üì• Loading data from Delta table '{table_name}' ...\")\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").table(table_name)\n",
    "        feature_cols = [\"sq_feet\", \"num_bedrooms\", \"num_bathrooms\", \"year_built\", \"location_score\"]\n",
    "        label_col = \"price\"\n",
    "\n",
    "        # Ensure all columns are double for scaler\n",
    "        for c in feature_cols + [label_col]:\n",
    "            df = df.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "        df = df.select(*feature_cols, col(label_col).alias(\"label\"))\n",
    "\n",
    "        print(f\"‚úÖ Data successfully ingested. Total rows: {df.count()}\")\n",
    "        df.printSchema()\n",
    "        return df, feature_cols\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data ingestion failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_data(spark, df, feature_cols):\n",
    "    \"\"\"Feature scaling (StandardScaler) ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ scaled data ‡§ï‡•ã Delta ‡§Æ‡•á‡§Ç save ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\"\"\"\n",
    "    print(\"\\n‚öôÔ∏è Feature scaling started...\")\n",
    "\n",
    "    # Step 0: Handle missing / NaN values\n",
    "    print(\"üßπ Checking and filling missing values...\")\n",
    "    df = df.na.fill(0)\n",
    "\n",
    "    # Step 1: Assemble features\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vector\")\n",
    "\n",
    "    # Step 2: Apply StandardScaler\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_vector\", outputCol=\"scaled_features\", withMean=True, withStd=True\n",
    "    )\n",
    "\n",
    "    # Step 3: Build pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    try:\n",
    "        model = pipeline.fit(df)\n",
    "        scaled_data = model.transform(df)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Scaling pipeline failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Step 4: Extract individual scaled columns\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    for i, c in enumerate(feature_cols):\n",
    "        scaled_data = scaled_data.withColumn(f\"{c}_scaled\", col(\"scaled_array\")[i])\n",
    "\n",
    "    # Step 5: Select final columns\n",
    "    final_cols = [f\"{c}_scaled\" for c in feature_cols] + [\"label\"]\n",
    "    scaled_final_df = scaled_data.select(*final_cols)\n",
    "\n",
    "    print(\"‚úÖ Feature scaling completed successfully.\")\n",
    "    scaled_final_df.show(5)\n",
    "\n",
    "    # Step 6: Save to Delta table (overwrite mode)\n",
    "    try:\n",
    "        scaled_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_DELTA_TABLE)\n",
    "        print(f\"‚úÖ Scaled data successfully saved to Delta table '{OUTPUT_DELTA_TABLE}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save scaled data to Delta: {e}\")\n",
    "        raise\n",
    "\n",
    "    return scaled_final_df\n",
    "\n",
    "# ==================== EXECUTION ====================\n",
    "def main():\n",
    "    print(\"üöÄ Starting data ingestion and preparation job...\")\n",
    "    \n",
    "    # Always initialize SparkSession at the start\n",
    "    spark_session = initialize_spark()\n",
    "\n",
    "    # Step 1: Data ingestion\n",
    "    ingested_df, feature_cols = ingest_data(spark_session, DELTA_TABLE_NAME)\n",
    "\n",
    "    # Step 2: Feature scaling and Delta save\n",
    "    scaled_df = prepare_data(spark_session, ingested_df, feature_cols)\n",
    "\n",
    "    print(\"üéØ Data ingestion and preparation pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
