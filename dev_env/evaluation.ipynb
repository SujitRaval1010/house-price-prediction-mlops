{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================================\n",
    "# üéØ MODEL EVALUATION SCRIPT - CONFIG DRIVEN (FIXED)\n",
    "# =============================================================================\n",
    "# Purpose: Find best model from experiment and prepare for registration\n",
    "# Now reads from pipeline_config.yml - No hardcoding!\n",
    "# =============================================================================\n",
    "\n",
    "%pip install xgboost requests\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MODEL EVALUATION SYSTEM (CONFIG-DRIVEN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ LOAD PIPELINE CONFIGURATION (NEW - REPLACES ALL HARDCODING!)\n",
    "# =============================================================================\n",
    "print(\"\\nüìã Loading pipeline configuration from pipeline_config.yml...\")\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    \n",
    "    # Extract configuration values\n",
    "    MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "    UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "    UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "    BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "    \n",
    "    # Auto-generate model name\n",
    "    MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}_uc2\"\n",
    "    \n",
    "    EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "    MODEL_ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "    \n",
    "    METRIC_KEY = pipeline_cfg[\"metrics\"][\"primary_metric\"]\n",
    "    IMPROVEMENT_THRESHOLD = pipeline_cfg[\"metrics\"][\"improvement_threshold\"]\n",
    "    \n",
    "    # Delta Tables\n",
    "    EVALUATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"evaluation_log\"]\n",
    "    BEST_MODEL_METADATA_TABLE = pipeline_cfg[\"tables\"][\"best_model_metadata\"]\n",
    "    \n",
    "    print(f\"‚úÖ Pipeline configuration loaded successfully!\")\n",
    "    print(f\"\\nüìä Configuration Details:\")\n",
    "    print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "    print(f\"   Model Name: {MODEL_NAME}\")\n",
    "    print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "    print(f\"   Metric: {METRIC_KEY} (lower is better)\")\n",
    "    print(f\"   Improvement Threshold: {IMPROVEMENT_THRESHOLD * 100}%\")\n",
    "    print(f\"   Metadata Table: {BEST_MODEL_METADATA_TABLE}\")\n",
    "    print(f\"   Log Table: {EVALUATION_LOG_TABLE}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: pipeline_config.yml not found!\")\n",
    "    print(\"üí° Please create pipeline_config.yml in the same directory\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading configuration: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ INITIALIZATION\n",
    "# =============================================================================\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    print(\"\\n‚úÖ MLflow and Spark initialized\")\n",
    "\n",
    "    # Verify experiment exists\n",
    "    exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if exp is None:\n",
    "        print(f\"‚ùå ERROR: Experiment '{EXPERIMENT_NAME}' not found!\")\n",
    "        print(\"\\nüí° Available experiments:\")\n",
    "        all_exps = client.search_experiments(max_results=20)\n",
    "        for e in all_exps:\n",
    "            print(f\"   - {e.name}\")\n",
    "        print(f\"\\nüí° Please run training script first to create the experiment\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"‚úÖ Experiment found: {EXPERIMENT_NAME}\")\n",
    "    print(f\"   Experiment ID: {exp.experiment_id}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Initialization failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# üìä STEP 1: GET BEST MODEL FROM EXPERIMENT\n",
    "# =============================================================================\n",
    "def get_best_model_from_experiment():\n",
    "    \"\"\"Find the best performing model from all experiment runs\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 1: Finding BEST Model From Experiment\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "        \n",
    "        # Get all runs sorted by metric (ascending = best first for RMSE)\n",
    "        all_runs = client.search_runs(\n",
    "            [exp.experiment_id],\n",
    "            filter_string=f\"metrics.{METRIC_KEY} > 0\",\n",
    "            order_by=[f\"metrics.{METRIC_KEY} ASC\"],\n",
    "            max_results=1000\n",
    "        )\n",
    "\n",
    "        if not all_runs:\n",
    "            print(f\"\\n‚ùå ERROR: No runs found with valid '{METRIC_KEY}' metric!\")\n",
    "            print(f\"\\nüí° Please run training script first\")\n",
    "            print(f\"   Expected experiment: {EXPERIMENT_NAME}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"‚úÖ Total runs in experiment: {len(all_runs)}\")\n",
    "\n",
    "        # Show top 10 models\n",
    "        print(f\"\\nüìä Top 10 Models (by {METRIC_KEY}):\")\n",
    "        print(f\"{'Rank':<6} {'Run Name':<40} {METRIC_KEY.upper():<15} {'Timestamp':<20}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        for i, run in enumerate(all_runs[:10], 1):\n",
    "            run_name = run.info.run_name or \"Unnamed\"\n",
    "            metric_val = run.data.metrics.get(METRIC_KEY, float('inf'))\n",
    "            timestamp = datetime.fromtimestamp(run.info.start_time/1000).strftime('%Y-%m-%d %H:%M')\n",
    "            marker = \"üëë BEST\" if i == 1 else f\"{i}.\"\n",
    "            print(f\"{marker:<6} {run_name:<40} {metric_val:<15.6f} {timestamp}\")\n",
    "\n",
    "        # Select best model\n",
    "        best_run = all_runs[0]\n",
    "        run_id = best_run.info.run_id\n",
    "        run_name = best_run.info.run_name or \"Unnamed\"\n",
    "        metrics = best_run.data.metrics\n",
    "        params = best_run.data.params\n",
    "        metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "        print(f\"\\n‚úÖ BEST Model Selected:\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Run Name: {run_name}\")\n",
    "        print(f\"   {METRIC_KEY}: {metric_value:.6f}\")\n",
    "        print(f\"   Rank: #1 out of {len(all_runs)} runs\")\n",
    "        print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "\n",
    "        return {\n",
    "            'run_id': run_id,\n",
    "            'run_name': run_name,\n",
    "            'metric_key': METRIC_KEY,\n",
    "            'metric_value': metric_value,\n",
    "            'params': params,\n",
    "            'all_metrics': metrics,\n",
    "            'timestamp': best_run.info.start_time,\n",
    "            'total_runs': len(all_runs),\n",
    "            'model_uri': f\"runs:/{run_id}/{MODEL_ARTIFACT_PATH}\",\n",
    "            'artifact_path': MODEL_ARTIFACT_PATH\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting best model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# üîß HELPER: GET MODEL ALIASES SAFELY\n",
    "# =============================================================================\n",
    "def get_model_aliases_safe(model_name, version):\n",
    "    \"\"\"Safely get aliases for a model version\"\"\"\n",
    "    try:\n",
    "        common_aliases = ['production', 'Staging', 'champion', 'baseline']\n",
    "        found_aliases = []\n",
    "        \n",
    "        for alias in common_aliases:\n",
    "            try:\n",
    "                alias_version = client.get_model_version_by_alias(model_name, alias)\n",
    "                if alias_version and str(alias_version.version) == str(version):\n",
    "                    found_aliases.append(alias)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return found_aliases\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# üèÜ STEP 2: GET CURRENT REGISTERED MODEL\n",
    "# =============================================================================\n",
    "def get_current_registered_model():\n",
    "    \"\"\"Get current registered model from registry\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 2: Checking Current Registered Model\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   Looking for: {MODEL_NAME}\")\n",
    "\n",
    "    try:\n",
    "        # Search for model versions\n",
    "        versions = client.search_model_versions(f\"name = '{MODEL_NAME}'\")\n",
    "        \n",
    "        if not versions:\n",
    "            print(\"‚ÑπÔ∏è No models in registry (first model registration)\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to list safely\n",
    "        versions_list = list(versions)\n",
    "        \n",
    "        if not versions_list:\n",
    "            print(\"‚ÑπÔ∏è No models in registry (first model registration)\")\n",
    "            return None\n",
    "\n",
    "        print(f\"‚úÖ Found {len(versions_list)} existing version(s)\")\n",
    "\n",
    "        # Find best priority version (Production > Staging > Latest)\n",
    "        best_version = None\n",
    "        best_priority = 999\n",
    "        \n",
    "        for v in versions_list:\n",
    "            try:\n",
    "                version_aliases = get_model_aliases_safe(MODEL_NAME, v.version)\n",
    "                \n",
    "                priority = 999\n",
    "                if 'production' in version_aliases:\n",
    "                    priority = 1\n",
    "                elif 'Staging' in version_aliases:\n",
    "                    priority = 2\n",
    "                elif 'champion' in version_aliases:\n",
    "                    priority = 3\n",
    "                else:\n",
    "                    priority = 10\n",
    "                \n",
    "                if priority < best_priority:\n",
    "                    best_priority = priority\n",
    "                    best_version = v\n",
    "                    \n",
    "                    if priority == 1:\n",
    "                        print(f\"‚úÖ Found Production model: Version {v.version}\")\n",
    "                        break\n",
    "                    elif priority == 2:\n",
    "                        print(f\"‚úÖ Found Staging model: Version {v.version}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing version {v.version}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # If no aliased version, use latest\n",
    "        if best_priority == 999 and versions_list:\n",
    "            best_version = versions_list[0]\n",
    "            print(f\"‚úÖ Using latest model: Version {best_version.version}\")\n",
    "\n",
    "        if best_version:\n",
    "            try:\n",
    "                run = client.get_run(best_version.run_id)\n",
    "                metric = run.data.metrics.get(METRIC_KEY)\n",
    "                final_aliases = get_model_aliases_safe(MODEL_NAME, best_version.version)\n",
    "                \n",
    "                print(f\"   Version: {best_version.version}\")\n",
    "                print(f\"   Run ID: {best_version.run_id}\")\n",
    "                print(f\"   {METRIC_KEY}: {metric:.6f}\" if metric else \"   Metric: N/A\")\n",
    "                print(f\"   Aliases: {', '.join(final_aliases) if final_aliases else 'None'}\")\n",
    "                \n",
    "                return {\n",
    "                    'version': best_version.version,\n",
    "                    'run_id': best_version.run_id,\n",
    "                    'metric_value': metric if metric else 0.0,\n",
    "                    'aliases': final_aliases\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error fetching run details: {e}\")\n",
    "                return None\n",
    "        \n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è No registered model found: {e}\")\n",
    "        print(\"   (This is expected for first-time registration)\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# üîç STEP 3: EVALUATE MODEL QUALITY\n",
    "# =============================================================================\n",
    "def evaluate_model(new_model, current_model):\n",
    "    \"\"\"Evaluate if new model should be registered\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 3: Model Evaluation\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # First model - automatic approval\n",
    "    if not current_model:\n",
    "        print(\"‚úÖ APPROVED: First model (no baseline to compare)\")\n",
    "        return {\n",
    "            'should_register': True,\n",
    "            'reason': 'First model registration',\n",
    "            'improvement_pct': 0.0,\n",
    "            'decision': 'APPROVE'\n",
    "        }\n",
    "\n",
    "    # Compare with existing model\n",
    "    new_metric = new_model['metric_value']\n",
    "    current_metric = current_model['metric_value']\n",
    "    \n",
    "    if current_metric is None or current_metric == 0:\n",
    "        print(\"‚ö†Ô∏è Current model has no valid metric, approving new model\")\n",
    "        return {\n",
    "            'should_register': True,\n",
    "            'reason': 'Current model has invalid metric',\n",
    "            'improvement_pct': 0.0,\n",
    "            'decision': 'APPROVE'\n",
    "        }\n",
    "    \n",
    "    improvement = (current_metric - new_metric) / current_metric\n",
    "    improvement_pct = improvement * 100\n",
    "\n",
    "    print(f\"\\nüìä Comparison:\")\n",
    "    print(f\"   New Model {METRIC_KEY.upper()}: {new_metric:.6f}\")\n",
    "    print(f\"   Current Model {METRIC_KEY.upper()}: {current_metric:.6f}\")\n",
    "    print(f\"   Improvement: {improvement_pct:.2f}%\")\n",
    "    print(f\"   Threshold: {IMPROVEMENT_THRESHOLD * 100}%\")\n",
    "\n",
    "    if improvement >= IMPROVEMENT_THRESHOLD:\n",
    "        print(f\"\\n‚úÖ APPROVED: Model improved by {improvement_pct:.2f}%\")\n",
    "        return {\n",
    "            'should_register': True,\n",
    "            'reason': f'Improvement: {improvement_pct:.2f}%',\n",
    "            'improvement_pct': improvement_pct,\n",
    "            'decision': 'APPROVE'\n",
    "        }\n",
    "    else:\n",
    "        print(f\"\\n‚ùå REJECTED: Insufficient improvement ({improvement_pct:.2f}%)\")\n",
    "        return {\n",
    "            'should_register': False,\n",
    "            'reason': f'Insufficient improvement: {improvement_pct:.2f}%',\n",
    "            'improvement_pct': improvement_pct,\n",
    "            'decision': 'REJECT'\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# üíæ STEP 4: SAVE BEST MODEL METADATA\n",
    "# =============================================================================\n",
    "def save_best_model_metadata(model_info, evaluation_result):\n",
    "    \"\"\"Save best model metadata to Delta table for registration script\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 4: Saving Best Model Metadata\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        metadata = {\n",
    "            \"evaluation_timestamp\": [datetime.now()],\n",
    "            \"run_id\": [model_info['run_id']],\n",
    "            \"run_name\": [model_info['run_name']],\n",
    "            \"model_uri\": [model_info['model_uri']],\n",
    "            \"artifact_path\": [model_info['artifact_path']],\n",
    "            \"metric_key\": [model_info['metric_key']],\n",
    "            \"metric_value\": [float(model_info['metric_value'])],\n",
    "            \"should_register\": [bool(evaluation_result['should_register'])],\n",
    "            \"evaluation_reason\": [str(evaluation_result['reason'])],\n",
    "            \"improvement_pct\": [float(evaluation_result['improvement_pct'])],\n",
    "            \"model_name\": [MODEL_NAME],\n",
    "            \"total_runs_evaluated\": [int(model_info['total_runs'])],\n",
    "            \"params_json\": [json.dumps(dict(model_info['params']))]\n",
    "        }\n",
    "        \n",
    "        df = spark.createDataFrame(pd.DataFrame(metadata))\n",
    "        \n",
    "        # Overwrite latest evaluation result\n",
    "        df.write.format(\"delta\")\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .option(\"overwriteSchema\", \"true\")\\\n",
    "            .saveAsTable(BEST_MODEL_METADATA_TABLE)\n",
    "        \n",
    "        print(f\"‚úÖ Metadata saved to: {BEST_MODEL_METADATA_TABLE}\")\n",
    "        print(f\"   Run ID: {model_info['run_id']}\")\n",
    "        print(f\"   Decision: {evaluation_result['decision']}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save metadata: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# üìù STEP 5: LOG EVALUATION HISTORY\n",
    "# =============================================================================\n",
    "def log_evaluation_history(model_info, current_model, evaluation_result):\n",
    "    \"\"\"Log evaluation to history table\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 5: Logging Evaluation History\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        from pyspark.sql.types import (StructType, StructField, StringType, \n",
    "                                       DoubleType, BooleanType, LongType, TimestampType)\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"new_run_id\", StringType(), True),\n",
    "            StructField(\"new_run_name\", StringType(), True),\n",
    "            StructField(\"new_metric\", DoubleType(), True),\n",
    "            StructField(\"current_version\", LongType(), True),\n",
    "            StructField(\"current_metric\", DoubleType(), True),\n",
    "            StructField(\"current_alias\", StringType(), True),\n",
    "            StructField(\"should_promote\", BooleanType(), True),\n",
    "            StructField(\"promotion_reason\", StringType(), True),\n",
    "            StructField(\"improvement_pct\", DoubleType(), True),\n",
    "            StructField(\"promoted_to_staging\", BooleanType(), True),\n",
    "            StructField(\"promoted_version\", LongType(), True),\n",
    "            StructField(\"threshold_used\", DoubleType(), True),\n",
    "            StructField(\"total_runs_evaluated\", LongType(), True),\n",
    "            StructField(\"selection_method\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        log_data = {\n",
    "            \"timestamp\": [datetime.now()],\n",
    "            \"new_run_id\": [model_info['run_id']],\n",
    "            \"new_run_name\": [model_info['run_name']],\n",
    "            \"new_metric\": [float(model_info['metric_value'])],\n",
    "            \"current_version\": [int(current_model['version']) if current_model else None],\n",
    "            \"current_metric\": [float(current_model['metric_value']) if current_model and current_model['metric_value'] else None],\n",
    "            \"current_alias\": ['Staging' if current_model else None],\n",
    "            \"should_promote\": [bool(evaluation_result['should_register'])],\n",
    "            \"promotion_reason\": [str(evaluation_result['reason'])],\n",
    "            \"improvement_pct\": [float(evaluation_result['improvement_pct'])],\n",
    "            \"promoted_to_staging\": [False],\n",
    "            \"promoted_version\": [None],\n",
    "            \"threshold_used\": [float(IMPROVEMENT_THRESHOLD * 100)],\n",
    "            \"total_runs_evaluated\": [int(model_info['total_runs'])],\n",
    "            \"selection_method\": [\"ALL-TIME BEST\"]\n",
    "        }\n",
    "        \n",
    "        df = spark.createDataFrame(pd.DataFrame(log_data), schema=schema)\n",
    "        \n",
    "        df.write.format(\"delta\")\\\n",
    "            .mode(\"append\")\\\n",
    "            .option(\"mergeSchema\", \"true\")\\\n",
    "            .saveAsTable(EVALUATION_LOG_TABLE)\n",
    "        \n",
    "        print(f\"‚úÖ History logged to: {EVALUATION_LOG_TABLE}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to log history: {e}\")\n",
    "        print(\"   (Non-critical error - continuing)\")\n",
    "\n",
    "# =============================================================================\n",
    "# üé¨ MAIN EXECUTION\n",
    "# =============================================================================\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üöÄ STARTING MODEL EVALUATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Step 1: Find best model from experiment\n",
    "    best_model = get_best_model_from_experiment()\n",
    "    if not best_model:\n",
    "        print(\"\\n‚ùå EVALUATION FAILED - No valid models found\")\n",
    "        return False\n",
    "\n",
    "    # Step 2: Get current registered model\n",
    "    current_model = get_current_registered_model()\n",
    "    \n",
    "    # Step 3: Evaluate model\n",
    "    evaluation_result = evaluate_model(best_model, current_model)\n",
    "\n",
    "    # Step 4: Save metadata for registration script\n",
    "    metadata_saved = save_best_model_metadata(best_model, evaluation_result)\n",
    "    \n",
    "    # Step 5: Log to history\n",
    "    log_evaluation_history(best_model, current_model, evaluation_result)\n",
    "\n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ MODEL EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Selected Model:\")\n",
    "    print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "    print(f\"   Target Registry: {MODEL_NAME}\")\n",
    "    print(f\"   Run ID: {best_model['run_id']}\")\n",
    "    print(f\"   Run Name: {best_model['run_name']}\")\n",
    "    print(f\"   {METRIC_KEY.upper()}: {best_model['metric_value']:.6f}\")\n",
    "    print(f\"   Rank: #1 from {best_model['total_runs']} runs\")\n",
    "    print(f\"\\nüéØ Evaluation Decision: {evaluation_result['decision']}\")\n",
    "    print(f\"   Reason: {evaluation_result['reason']}\")\n",
    "    print(f\"   Should Register: {'YES ‚úÖ' if evaluation_result['should_register'] else 'NO ‚ùå'}\")\n",
    "    \n",
    "    if metadata_saved:\n",
    "        print(f\"\\nüì¶ Next Step:\")\n",
    "        print(f\"   Run Model_Registration script to register approved model\")\n",
    "        print(f\"   Metadata saved in: {BEST_MODEL_METADATA_TABLE}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Save for workflow\n",
    "    try:\n",
    "        dbutils.jobs.taskValues.set(key=\"model_type\", value=MODEL_TYPE)\n",
    "        dbutils.jobs.taskValues.set(key=\"model_name\", value=MODEL_NAME)\n",
    "        dbutils.jobs.taskValues.set(key=\"should_register\", value=evaluation_result['should_register'])\n",
    "        print(\"‚úÖ Task values saved for workflow\")\n",
    "    except:\n",
    "        print(\"‚ÑπÔ∏è Not running in workflow - skipping task values\")\n",
    "    \n",
    "    return evaluation_result['should_register']\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    approved = main()\n",
    "    print(f\"\\nüéØ MODEL APPROVAL STATUS: {'APPROVED ‚úÖ' if approved else 'REJECTED ‚ùå'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
