{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_train.py\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from mlflow.models.signature import infer_signature\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# ‡§ö‡•á‡§§‡§æ‡§µ‡§®‡•Ä (Warnings) ‡§ï‡•ã ‡§Ö‡§®‡§¶‡•á‡§ñ‡§æ ‡§ï‡§∞‡•á‡§Ç\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Delta_RF\"\n",
    "MODEL_ARTIFACT_PATH = \"sklearn_rf_model\"\n",
    "DELTA_TABLE_NAME = \"house_price_scaled_delta\"   # ‚úÖ scaled table use karni hai\n",
    "\n",
    "# ‡§Æ‡•â‡§°‡§≤ ‡§î‡§∞ ‡§°‡•á‡§ü‡§æ ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§®\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# ‚úÖ Updated feature columns (scaled ones)\n",
    "FEATURE_COLS = [\n",
    "    'sq_feet_scaled',\n",
    "    'num_bedrooms_scaled',\n",
    "    'num_bathrooms_scaled',\n",
    "    'year_built_scaled',\n",
    "    'location_score_scaled'\n",
    "]\n",
    "\n",
    "LABEL_COL = 'label'   # ‚úÖ scaling script ke baad label ka naam yeh hi hai\n",
    "\n",
    "# Cross-validation ‡§ï‡•á ‡§≤‡§ø‡§è parameter grid\n",
    "PARAM_GRID = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [10, 12, 15],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# ==================== FUNCTIONS ====================\n",
    "\n",
    "def setup_mlflow_experiment():\n",
    "    \"\"\"MLflow ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§î‡§∞ ‡§∞‡§ú‡§ø‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä ‡§ï‡•ã Databricks UC ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§\"\"\"\n",
    "    if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(\"databricks\")\n",
    "            mlflow.set_registry_uri(\"databricks-uc\")\n",
    "            print(\"‚úì MLflow configured for Databricks UC (Tracking & Registry).\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Warning: MLflow Registry setup failed with: {e}\")\n",
    "            \n",
    "    try:\n",
    "        mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"‚úì MLflow Experiment set to: {EXPERIMENT_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical: MLflow Experiment setup failed! Error: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_data_for_training(spark: SparkSession, table_name: str):\n",
    "    \"\"\"‡§°‡•á‡§≤‡•ç‡§ü‡§æ ‡§ü‡•á‡§¨‡§≤ ‡§∏‡•á ‡§∏‡•ç‡§ï‡•á‡§≤‡•ç‡§° ‡§°‡•á‡§ü‡§æ ‡§≤‡•ã‡§° ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ Pandas DataFrame ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§§‡§æ ‡§π‡•à‡•§\"\"\"\n",
    "    print(f\"üíæ Loading data from Delta Table: {table_name}\")\n",
    "    try:\n",
    "        df_spark = spark.read.format(\"delta\").table(table_name)\n",
    "        \n",
    "        # ‚úÖ Scaled feature columns and label select karna\n",
    "        df_pd = df_spark.select(*FEATURE_COLS, col(LABEL_COL)).toPandas()\n",
    "        \n",
    "        print(f\"‚úì Data loaded from scaled Delta table. Total rows: {len(df_pd)}\")\n",
    "        \n",
    "        X = df_pd[FEATURE_COLS]\n",
    "        y = df_pd[LABEL_COL]\n",
    "        \n",
    "        return X, y\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data from Delta: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_and_log_model(X, y):\n",
    "    \"\"\"Cross-validation ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ MLflow ‡§Æ‡•á‡§Ç ‡§≤‡•â‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"RandomForest_House_Price_Model_CV\") as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f\"üöÄ MLflow Run Started with ID: {run_id}\")\n",
    "        \n",
    "        base_model = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=PARAM_GRID,\n",
    "            cv=CV_FOLDS,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(\"‚úì GridSearchCV completed successfully.\")\n",
    "        \n",
    "        best_params = grid_search.best_params_\n",
    "        for param_name, param_value in best_params.items():\n",
    "            mlflow.log_param(f\"best_{param_name}\", param_value)\n",
    "        \n",
    "        best_cv_score = -grid_search.best_score_\n",
    "        mlflow.log_metric(\"best_cv_rmse\", np.sqrt(best_cv_score))\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        predictions = best_model.predict(X_test)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "\n",
    "        mlflow.log_metric(\"test_rmse\", rmse)\n",
    "        mlflow.log_metric(\"test_r2_score\", r2)\n",
    "        print(f\"‚úì Test Metrics Logged: RMSE={rmse:.2f}, R2={r2:.4f}\")\n",
    "        \n",
    "        model_signature = infer_signature(X_train, best_model.predict(X_train))\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model, \n",
    "            artifact_path=MODEL_ARTIFACT_PATH,\n",
    "            signature=model_signature,\n",
    "            registered_model_name=None \n",
    "        )\n",
    "        print(f\"‚úì Best model logged with signature to artifact path: {MODEL_ARTIFACT_PATH}\")\n",
    "        \n",
    "        return run_id\n",
    "\n",
    "\n",
    "# ==================== EXECUTION ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"ModelTrain\").getOrCreate()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SparkSession creation failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    setup_mlflow_experiment()\n",
    "    \n",
    "    X, y = get_data_for_training(spark, DELTA_TABLE_NAME)\n",
    "\n",
    "    if X is not None:\n",
    "        training_run_id = train_and_log_model(X, y)\n",
    "        \n",
    "        if training_run_id:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(f\"‚úÖ TRAINING & LOGGING COMPLETE! New Run ID: {training_run_id}\")\n",
    "            print(f\"‡§Ö‡§ó‡§≤‡§æ ‡§ï‡§¶‡§Æ: 'model_register.py' ‡§ï‡•ã ‡§á‡§∏ Run ID ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ö‡§≤‡§æ‡§è‡§Å‡•§\")\n",
    "            print(\"=\" * 60)\n",
    "        else:\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
