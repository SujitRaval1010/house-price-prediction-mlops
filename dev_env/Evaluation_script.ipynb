{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================================\n",
    "# ğŸ¯ MODEL EVALUATION SCRIPT (EVALUATION ONLY - NO REGISTRATION)\n",
    "# =============================================================================\n",
    "# Purpose: Find best model from experiment and prepare for registration\n",
    "# Fixed: Completely rewritten to avoid aliases iteration issues\n",
    "# =============================================================================\n",
    "\n",
    "%pip install xgboost requests\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ MODEL EVALUATION SYSTEM (EVALUATION ONLY)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… CONFIGURATION (MUST MATCH TRAINING SCRIPT)\n",
    "# =============================================================================\n",
    "EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "MODEL_ARTIFACT_PATH = \"xgboost_model\"\n",
    "METRIC_KEY = \"test_rmse\"\n",
    "IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement required\n",
    "\n",
    "# Delta Tables\n",
    "EVALUATION_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "BEST_MODEL_METADATA_TABLE = \"workspace.default.best_model_metadata\"\n",
    "\n",
    "print(\"\\nğŸ“‹ CONFIGURATION:\")\n",
    "print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"   Model Name: {MODEL_NAME}\")\n",
    "print(f\"   Metric: {METRIC_KEY} (lower is better)\")\n",
    "print(f\"   Threshold: {IMPROVEMENT_THRESHOLD * 100}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… INITIALIZATION\n",
    "# =============================================================================\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "\n",
    "    # Verify experiment exists\n",
    "    exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if exp is None:\n",
    "        print(f\"âŒ ERROR: Experiment '{EXPERIMENT_NAME}' not found!\")\n",
    "        print(\"\\nğŸ’¡ Available experiments:\")\n",
    "        all_exps = client.search_experiments(max_results=20)\n",
    "        for e in all_exps:\n",
    "            print(f\"   - {e.name}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"âœ… Experiment found: {EXPERIMENT_NAME}\")\n",
    "    print(f\"   Experiment ID: {exp.experiment_id}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Initialization failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“Š STEP 1: GET BEST MODEL FROM EXPERIMENT\n",
    "# =============================================================================\n",
    "def get_best_model_from_experiment():\n",
    "    \"\"\"Find the best performing model from all experiment runs\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 1: Finding BEST Model From Experiment\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "        \n",
    "        # Get all runs sorted by RMSE (ascending = best first)\n",
    "        all_runs = client.search_runs(\n",
    "            [exp.experiment_id],\n",
    "            filter_string=f\"metrics.{METRIC_KEY} > 0\",\n",
    "            order_by=[f\"metrics.{METRIC_KEY} ASC\"],\n",
    "            max_results=1000\n",
    "        )\n",
    "\n",
    "        if not all_runs:\n",
    "            print(f\"\\nâŒ ERROR: No runs found with valid '{METRIC_KEY}' metric!\")\n",
    "            print(\"\\nğŸ’¡ Please run training_script.py first\")\n",
    "            return None\n",
    "\n",
    "        print(f\"âœ… Total runs in experiment: {len(all_runs)}\")\n",
    "\n",
    "        # Show top 10 models\n",
    "        print(f\"\\nğŸ“Š Top 10 Models (by {METRIC_KEY}):\")\n",
    "        print(f\"{'Rank':<6} {'Run Name':<40} {'RMSE':<15} {'Timestamp':<20}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        for i, run in enumerate(all_runs[:10], 1):\n",
    "            run_name = run.info.run_name or \"Unnamed\"\n",
    "            metric_val = run.data.metrics.get(METRIC_KEY, float('inf'))\n",
    "            timestamp = datetime.fromtimestamp(run.info.start_time/1000).strftime('%Y-%m-%d %H:%M')\n",
    "            marker = \"ğŸ‘‘ BEST\" if i == 1 else f\"{i}.\"\n",
    "            print(f\"{marker:<6} {run_name:<40} {metric_val:<15.6f} {timestamp}\")\n",
    "\n",
    "        # Select best model\n",
    "        best_run = all_runs[0]\n",
    "        run_id = best_run.info.run_id\n",
    "        run_name = best_run.info.run_name or \"Unnamed\"\n",
    "        metrics = best_run.data.metrics\n",
    "        params = best_run.data.params\n",
    "        metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "        print(f\"\\nâœ… BEST Model Selected:\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Run Name: {run_name}\")\n",
    "        print(f\"   {METRIC_KEY}: {metric_value:.6f}\")\n",
    "        print(f\"   Rank: #1 out of {len(all_runs)} runs\")\n",
    "        print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "\n",
    "        return {\n",
    "            'run_id': run_id,\n",
    "            'run_name': run_name,\n",
    "            'metric_key': METRIC_KEY,\n",
    "            'metric_value': metric_value,\n",
    "            'params': params,\n",
    "            'all_metrics': metrics,\n",
    "            'timestamp': best_run.info.start_time,\n",
    "            'total_runs': len(all_runs),\n",
    "            'model_uri': f\"runs:/{run_id}/{MODEL_ARTIFACT_PATH}\",\n",
    "            'artifact_path': MODEL_ARTIFACT_PATH\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting best model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ”§ HELPER: GET MODEL ALIASES SAFELY\n",
    "# =============================================================================\n",
    "def get_model_aliases_safe(model_name, version):\n",
    "    \"\"\"\n",
    "    Safely get aliases for a model version using direct API call\n",
    "    Returns list of alias strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use get_model_version_by_alias to check each common alias\n",
    "        common_aliases = ['production', 'Staging', 'champion', 'baseline']\n",
    "        found_aliases = []\n",
    "        \n",
    "        for alias in common_aliases:\n",
    "            try:\n",
    "                # Try to get version by this alias\n",
    "                alias_version = client.get_model_version_by_alias(model_name, alias)\n",
    "                if alias_version and str(alias_version.version) == str(version):\n",
    "                    found_aliases.append(alias)\n",
    "            except:\n",
    "                # Alias doesn't exist or doesn't point to this version\n",
    "                continue\n",
    "        \n",
    "        return found_aliases\n",
    "    except Exception as e:\n",
    "        # If all fails, return empty list\n",
    "        return []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ† STEP 2: GET CURRENT REGISTERED MODEL (COMPLETELY REWRITTEN)\n",
    "# =============================================================================\n",
    "def get_current_registered_model():\n",
    "    \"\"\"Get current registered model from registry - completely rewritten to avoid aliases issues\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 2: Checking Current Registered Model\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        # Search for model versions\n",
    "        versions = client.search_model_versions(f\"name = '{MODEL_NAME}'\")\n",
    "        \n",
    "        if not versions:\n",
    "            print(\"â„¹ï¸ No models in registry (first model registration)\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to list safely\n",
    "        versions_list = []\n",
    "        try:\n",
    "            for v in versions:\n",
    "                versions_list.append(v)\n",
    "        except Exception as e:\n",
    "            print(f\"â„¹ï¸ Unable to iterate versions: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if not versions_list:\n",
    "            print(\"â„¹ï¸ No models in registry (first model registration)\")\n",
    "            return None\n",
    "\n",
    "        print(f\"âœ… Found {len(versions_list)} existing version(s)\")\n",
    "\n",
    "        # Strategy: Check aliases using direct API calls\n",
    "        best_version = None\n",
    "        best_priority = 999  # Lower is better\n",
    "        \n",
    "        for v in versions_list:\n",
    "            try:\n",
    "                # Get aliases safely\n",
    "                version_aliases = get_model_aliases_safe(MODEL_NAME, v.version)\n",
    "                \n",
    "                # Assign priority\n",
    "                priority = 999\n",
    "                if 'production' in version_aliases:\n",
    "                    priority = 1\n",
    "                elif 'Staging' in version_aliases:\n",
    "                    priority = 2\n",
    "                elif 'champion' in version_aliases:\n",
    "                    priority = 3\n",
    "                else:\n",
    "                    priority = 10  # No alias\n",
    "                \n",
    "                # Keep best priority version\n",
    "                if priority < best_priority:\n",
    "                    best_priority = priority\n",
    "                    best_version = v\n",
    "                    \n",
    "                    if priority == 1:\n",
    "                        print(f\"âœ… Found Production model: Version {v.version}\")\n",
    "                        break  # Production is best, stop searching\n",
    "                    elif priority == 2:\n",
    "                        print(f\"âœ… Found Staging model: Version {v.version}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error processing version {v.version}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # If no aliased version, use latest\n",
    "        if best_priority == 999 and versions_list:\n",
    "            best_version = versions_list[0]\n",
    "            print(f\"âœ… Using latest model: Version {best_version.version}\")\n",
    "\n",
    "        if best_version:\n",
    "            try:\n",
    "                # Get run details\n",
    "                run = client.get_run(best_version.run_id)\n",
    "                metric = run.data.metrics.get(METRIC_KEY)\n",
    "                \n",
    "                # Get aliases one more time for display\n",
    "                final_aliases = get_model_aliases_safe(MODEL_NAME, best_version.version)\n",
    "                \n",
    "                print(f\"   Version: {best_version.version}\")\n",
    "                print(f\"   Run ID: {best_version.run_id}\")\n",
    "                print(f\"   {METRIC_KEY}: {metric:.6f}\" if metric else \"   Metric: N/A\")\n",
    "                print(f\"   Aliases: {', '.join(final_aliases) if final_aliases else 'None'}\")\n",
    "                \n",
    "                return {\n",
    "                    'version': best_version.version,\n",
    "                    'run_id': best_version.run_id,\n",
    "                    'metric_value': metric if metric else 0.0,\n",
    "                    'aliases': final_aliases\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error fetching run details: {e}\")\n",
    "                return None\n",
    "        \n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"â„¹ï¸ No registered model found: {e}\")\n",
    "        print(\"   (This is expected for first-time registration)\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ” STEP 3: EVALUATE MODEL QUALITY\n",
    "# =============================================================================\n",
    "def evaluate_model(new_model, current_model):\n",
    "    \"\"\"Evaluate if new model should be registered\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 3: Model Evaluation\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # First model - automatic approval\n",
    "    if not current_model:\n",
    "        print(\"âœ… APPROVED: First model (no baseline to compare)\")\n",
    "        return {\n",
    "            'should_register': True,\n",
    "            'reason': 'First model registration',\n",
    "            'improvement_pct': 0.0,\n",
    "            'decision': 'APPROVE'\n",
    "        }\n",
    "\n",
    "    # Compare with existing model\n",
    "    new_metric = new_model['metric_value']\n",
    "    current_metric = current_model['metric_value']\n",
    "    \n",
    "    # Handle case where current_metric might be 0 or None\n",
    "    if current_metric is None or current_metric == 0:\n",
    "        print(\"âš ï¸ Current model has no valid metric, approving new model\")\n",
    "        return {\n",
    "            'should_register': True,\n",
    "            'reason': 'Current model has invalid metric',\n",
    "            'improvement_pct': 0.0,\n",
    "            'decision': 'APPROVE'\n",
    "        }\n",
    "    \n",
    "    improvement = (current_metric - new_metric) / current_metric\n",
    "    improvement_pct = improvement * 100\n",
    "\n",
    "    print(f\"\\nğŸ“Š Comparison:\")\n",
    "    print(f\"   New Model RMSE: {new_metric:.6f}\")\n",
    "    print(f\"   Current Model RMSE: {current_metric:.6f}\")\n",
    "    print(f\"   Improvement: {improvement_pct:.2f}%\")\n",
    "    print(f\"   Threshold: {IMPROVEMENT_THRESHOLD * 100}%\")\n",
    "\n",
    "    if improvement >= IMPROVEMENT_THRESHOLD:\n",
    "        print(f\"\\nâœ… APPROVED: Model improved by {improvement_pct:.2f}%\")\n",
    "        return {\n",
    "            'should_register': True,\n",
    "            'reason': f'Improvement: {improvement_pct:.2f}%',\n",
    "            'improvement_pct': improvement_pct,\n",
    "            'decision': 'APPROVE'\n",
    "        }\n",
    "    else:\n",
    "        print(f\"\\nâŒ REJECTED: Insufficient improvement ({improvement_pct:.2f}%)\")\n",
    "        return {\n",
    "            'should_register': False,\n",
    "            'reason': f'Insufficient improvement: {improvement_pct:.2f}%',\n",
    "            'improvement_pct': improvement_pct,\n",
    "            'decision': 'REJECT'\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ’¾ STEP 4: SAVE BEST MODEL METADATA\n",
    "# =============================================================================\n",
    "def save_best_model_metadata(model_info, evaluation_result):\n",
    "    \"\"\"Save best model metadata to Delta table for registration script\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 4: Saving Best Model Metadata\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        metadata = {\n",
    "            \"evaluation_timestamp\": [datetime.now()],\n",
    "            \"run_id\": [model_info['run_id']],\n",
    "            \"run_name\": [model_info['run_name']],\n",
    "            \"model_uri\": [model_info['model_uri']],\n",
    "            \"artifact_path\": [model_info['artifact_path']],\n",
    "            \"metric_key\": [model_info['metric_key']],\n",
    "            \"metric_value\": [float(model_info['metric_value'])],\n",
    "            \"should_register\": [bool(evaluation_result['should_register'])],\n",
    "            \"evaluation_reason\": [str(evaluation_result['reason'])],\n",
    "            \"improvement_pct\": [float(evaluation_result['improvement_pct'])],\n",
    "            \"model_name\": [MODEL_NAME],\n",
    "            \"total_runs_evaluated\": [int(model_info['total_runs'])],\n",
    "            \"params_json\": [json.dumps(dict(model_info['params']))]\n",
    "        }\n",
    "        \n",
    "        df = spark.createDataFrame(pd.DataFrame(metadata))\n",
    "        \n",
    "        # Overwrite latest evaluation result\n",
    "        df.write.format(\"delta\")\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .option(\"overwriteSchema\", \"true\")\\\n",
    "            .saveAsTable(BEST_MODEL_METADATA_TABLE)\n",
    "        \n",
    "        print(f\"âœ… Metadata saved to: {BEST_MODEL_METADATA_TABLE}\")\n",
    "        print(f\"   Run ID: {model_info['run_id']}\")\n",
    "        print(f\"   Decision: {evaluation_result['decision']}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to save metadata: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“ STEP 5: LOG EVALUATION HISTORY\n",
    "# =============================================================================\n",
    "def log_evaluation_history(model_info, current_model, evaluation_result):\n",
    "    \"\"\"Log evaluation to history table\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 5: Logging Evaluation History\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        log_data = {\n",
    "            \"timestamp\": [datetime.now()],\n",
    "            \"new_run_id\": [model_info['run_id']],\n",
    "            \"new_run_name\": [model_info['run_name']],\n",
    "            \"new_metric\": [float(model_info['metric_value'])],\n",
    "            \"current_version\": [int(current_model['version']) if current_model else None],\n",
    "            \"current_metric\": [float(current_model['metric_value']) if current_model and current_model['metric_value'] else None],\n",
    "            \"current_alias\": ['Staging' if current_model else None],\n",
    "            \"should_promote\": [bool(evaluation_result['should_register'])],\n",
    "            \"promotion_reason\": [str(evaluation_result['reason'])],\n",
    "            \"improvement_pct\": [float(evaluation_result['improvement_pct'])],\n",
    "            \"promoted_to_staging\": [False],  # Will be updated by registration script\n",
    "            \"promoted_version\": [None],  # Will be updated by registration script\n",
    "            \"threshold_used\": [float(IMPROVEMENT_THRESHOLD * 100)],\n",
    "            \"total_runs_evaluated\": [int(model_info['total_runs'])],\n",
    "            \"selection_method\": [\"ALL-TIME BEST\"]\n",
    "        }\n",
    "        \n",
    "        df = spark.createDataFrame(pd.DataFrame(log_data))\n",
    "        \n",
    "        df.write.format(\"delta\")\\\n",
    "            .mode(\"append\")\\\n",
    "            .option(\"mergeSchema\", \"true\")\\\n",
    "            .saveAsTable(EVALUATION_LOG_TABLE)\n",
    "        \n",
    "        print(f\"âœ… History logged to: {EVALUATION_LOG_TABLE}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to log history: {e}\")\n",
    "        print(\"   (Non-critical error - continuing)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ¬ MAIN EXECUTION\n",
    "# =============================================================================\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸš€ STARTING MODEL EVALUATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Step 1: Find best model from experiment\n",
    "    best_model = get_best_model_from_experiment()\n",
    "    if not best_model:\n",
    "        print(\"\\nâŒ EVALUATION FAILED - No valid models found\")\n",
    "        return False  # Instead of sys.exit(1)\n",
    "\n",
    "    # Step 2: Get current registered model\n",
    "    current_model = get_current_registered_model()\n",
    "    \n",
    "    # Step 3: Evaluate model\n",
    "    evaluation_result = evaluate_model(best_model, current_model)\n",
    "\n",
    "    # Step 4: Save metadata for registration script\n",
    "    metadata_saved = save_best_model_metadata(best_model, evaluation_result)\n",
    "    \n",
    "    # Step 5: Log to history\n",
    "    log_evaluation_history(best_model, current_model, evaluation_result)\n",
    "\n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸ“Š Selected Model:\")\n",
    "    print(f\"   Run ID: {best_model['run_id']}\")\n",
    "    print(f\"   Run Name: {best_model['run_name']}\")\n",
    "    print(f\"   RMSE: {best_model['metric_value']:.6f}\")\n",
    "    print(f\"   Rank: #1 from {best_model['total_runs']} runs\")\n",
    "    print(f\"\\nğŸ¯ Evaluation Decision: {evaluation_result['decision']}\")\n",
    "    print(f\"   Reason: {evaluation_result['reason']}\")\n",
    "    print(f\"   Should Register: {'YES âœ…' if evaluation_result['should_register'] else 'NO âŒ'}\")\n",
    "    \n",
    "    if metadata_saved:\n",
    "        print(f\"\\nğŸ“¦ Next Step:\")\n",
    "        print(f\"   Run registration script to register approved model\")\n",
    "        print(f\"   Metadata available in: {BEST_MODEL_METADATA_TABLE}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Return True if approved, False if rejected\n",
    "    return evaluation_result['should_register']\n",
    "\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    approved = main()\n",
    "    print(f\"\\nğŸ¯ MODEL APPROVAL STATUS: {'APPROVED âœ…' if approved else 'REJECTED âŒ'}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # ğŸ”„ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # âœ… exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Logging Config\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ“Š STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 1: Finding Latest Trained Model (Metric-driven)\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"metrics.\" + METRIC_KEY + \" DESC\"],  # Fetch best metric, not latest timestamp\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         best_run = runs[0]\n",
    "#         run_id = best_run.info.run_id\n",
    "#         run_name = best_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = best_run.data.metrics\n",
    "#         params = best_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\nâœ… Best Training Run Found (by {METRIC_KEY}):\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': best_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error getting best model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ† STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\nâœ… Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\nâ„¹ï¸ No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âš–ï¸ STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nğŸŸ¢ DECISION: PROMOTE â€” First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nğŸ”´ DECISION: DO NOT PROMOTE â€” Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nğŸŸ¢ DECISION: PROMOTE â€” Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nğŸ“Š Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nğŸŸ¢ PROMOTE â€” New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nğŸŸ¡ NO PROMOTION â€” Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nğŸ”´ DO NOT PROMOTE â€” Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI â†’ {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\nâœ… Model Registered & Promoted â†’ @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ“ STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"âœ… Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ¬ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"âŒ No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # ğŸ”„ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # âœ… exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Notification & Logging Config\n",
    "# ENABLE_SLACK = False\n",
    "# SLACK_WEBHOOK_URL = \"\"\n",
    "# ENABLE_EMAIL = False\n",
    "# EMAIL_RECIPIENT = \"\"\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ“Š STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 1: Finding Latest Trained Model\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"start_time DESC\"],\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         latest_run = runs[0]\n",
    "#         run_id = latest_run.info.run_id\n",
    "#         run_name = latest_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = latest_run.data.metrics\n",
    "#         params = latest_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\nâœ… Latest Training Run Found:\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(latest_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': latest_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error getting latest model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ† STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\nâœ… Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\nâ„¹ï¸ No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âš–ï¸ STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nğŸŸ¢ DECISION: PROMOTE â€” First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nğŸ”´ DECISION: DO NOT PROMOTE â€” Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nğŸŸ¢ DECISION: PROMOTE â€” Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nğŸ“Š Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nğŸŸ¢ PROMOTE â€” New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nğŸŸ¡ NO PROMOTION â€” Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nğŸ”´ DO NOT PROMOTE â€” Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI â†’ {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\nâœ… Model Registered & Promoted â†’ @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ“ STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"âœ… Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ¬ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"âŒ No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
