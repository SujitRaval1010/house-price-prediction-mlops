{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================================\n",
    "# ğŸ¯ FIXED MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# =============================================================================\n",
    "# Hard-coded configuration to match training_script.py and Model_Registration.ipynb\n",
    "# =============================================================================\n",
    "\n",
    "%pip install xgboost requests\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import traceback\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… HARD-CODED CONFIGURATION (MUST MATCH OTHER SCRIPTS!)\n",
    "# =============================================================================\n",
    "# These values are DIRECTLY from training_script.py and Model_Registration.ipynb\n",
    "\n",
    "EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"  # âœ… From training script\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"  # âœ… From registration script\n",
    "\n",
    "STAGING_ALIAS = \"Staging\"\n",
    "PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "MODEL_ARTIFACT_PATH = \"xgboost_model\"  # âœ… From training script\n",
    "METRIC_KEY = \"test_rmse\"              # âœ… From training script\n",
    "IMPROVEMENT_THRESHOLD = 0.02          # 2% improvement needed for promotion\n",
    "\n",
    "# Logging Config\n",
    "COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "print(\"\\nğŸ“‹ CONFIGURATION:\")\n",
    "print(f\"   Experiment Name: {EXPERIMENT_NAME}\")\n",
    "print(f\"   Model Name: {MODEL_NAME}\")\n",
    "print(f\"   Artifact Path: {MODEL_ARTIFACT_PATH}\")\n",
    "print(f\"   Metric Key: {METRIC_KEY}\")\n",
    "print(f\"   Improvement Threshold: {IMPROVEMENT_THRESHOLD * 100}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… INITIALIZATION\n",
    "# =============================================================================\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "\n",
    "    # Verify experiment exists\n",
    "    exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if exp is None:\n",
    "        print(f\"âŒ ERROR: Experiment '{EXPERIMENT_NAME}' not found!\")\n",
    "        print(\"\\nğŸ’¡ Available experiments:\")\n",
    "        all_exps = client.search_experiments(max_results=20)\n",
    "        for e in all_exps:\n",
    "            print(f\"   - {e.name}\")\n",
    "        print(f\"\\nâš ï¸ Please ensure the experiment name matches your training script!\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"âœ… Experiment found: {EXPERIMENT_NAME}\")\n",
    "    print(f\"   Experiment ID: {exp.experiment_id}\\n\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Initialization failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“Š STEP 1: GET BEST MODEL FROM ALL EXPERIMENT RUNS\n",
    "# =============================================================================\n",
    "def get_best_model_from_experiment():\n",
    "    \"\"\"Find the best performing model from all experiment runs\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 1: Finding BEST Model Across ALL Experiment Runs\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "        print(f\"âœ… Experiment: {EXPERIMENT_NAME}\")\n",
    "        print(f\"   Experiment ID: {exp.experiment_id}\")\n",
    "\n",
    "        # Get ALL runs with valid metrics, sorted by RMSE (best first)\n",
    "        all_runs = client.search_runs(\n",
    "            [exp.experiment_id],\n",
    "            filter_string=f\"metrics.{METRIC_KEY} > 0\",\n",
    "            order_by=[f\"metrics.{METRIC_KEY} ASC\"],\n",
    "            max_results=1000\n",
    "        )\n",
    "\n",
    "        if not all_runs:\n",
    "            print(f\"\\nâŒ ERROR: No runs found with valid '{METRIC_KEY}' metric!\")\n",
    "            print(\"\\nğŸ’¡ Troubleshooting:\")\n",
    "            print(\"   1. Verify training script completed successfully\")\n",
    "            print(\"   2. Check that models were logged with test_rmse metric\")\n",
    "            print(\"   3. Run training_script.py first to generate runs\")\n",
    "            return None\n",
    "\n",
    "        print(f\"âœ… Total runs in experiment: {len(all_runs)}\")\n",
    "\n",
    "        best_run = all_runs[0]\n",
    "\n",
    "        # Show top 10 models\n",
    "        print(f\"\\nğŸ“Š Top 10 Models in Experiment (by {METRIC_KEY}):\")\n",
    "        print(f\"{'Rank':<6} {'Run Name':<40} {'RMSE':<15} {'Timestamp':<20}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        for i, run in enumerate(all_runs[:10], 1):\n",
    "            run_name = run.info.run_name or \"Unnamed\"\n",
    "            metric_val = run.data.metrics.get(METRIC_KEY, float('inf'))\n",
    "            timestamp = datetime.fromtimestamp(run.info.start_time/1000).strftime('%Y-%m-%d %H:%M')\n",
    "            marker = \"ğŸ‘‘ BEST\" if i == 1 else f\"{i}.\"\n",
    "            print(f\"{marker:<6} {run_name:<40} {metric_val:<15.6f} {timestamp}\")\n",
    "\n",
    "        run_id = best_run.info.run_id\n",
    "        run_name = best_run.info.run_name or \"Unnamed\"\n",
    "        metrics = best_run.data.metrics\n",
    "        params = best_run.data.params\n",
    "        metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "        print(f\"\\nâœ… BEST Model Selected:\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Run Name: {run_name}\")\n",
    "        print(f\"   {METRIC_KEY}: {metric_value:.6f}\")\n",
    "        print(f\"   Rank: #1 out of {len(all_runs)} total runs\")\n",
    "        print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "        print(f\"   Parameters: {dict(list(params.items())[:5])}...\")\n",
    "\n",
    "        return {\n",
    "            'run_id': run_id,\n",
    "            'run_name': run_name,\n",
    "            'metric': metric_value,\n",
    "            'params': params,\n",
    "            'metrics_all': metrics,\n",
    "            'timestamp': best_run.info.start_time,\n",
    "            'total_runs': len(all_runs)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting best model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ† STEP 2: GET CURRENT BEST MODEL FROM REGISTRY\n",
    "# =============================================================================\n",
    "def get_current_best_model():\n",
    "    \"\"\"Get the current best model from model registry\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 2: Checking Current Best Model in Registry\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        # Search for models with Staging or Production alias\n",
    "        versions = client.search_model_versions(f\"name = '{MODEL_NAME}'\")\n",
    "        \n",
    "        if not versions:\n",
    "            print(\"â„¹ï¸ No models found in registry (this is the first model)\")\n",
    "            return None\n",
    "\n",
    "        # Find best version (prefer Production, then Staging)\n",
    "        best_version = None\n",
    "        for v in versions:\n",
    "            if v.aliases and PRODUCTION_ALIAS in v.aliases:\n",
    "                best_version = v\n",
    "                print(f\"âœ… Found Production model: Version {v.version}\")\n",
    "                break\n",
    "        \n",
    "        if not best_version:\n",
    "            for v in versions:\n",
    "                if v.aliases and STAGING_ALIAS in v.aliases:\n",
    "                    best_version = v\n",
    "                    print(f\"âœ… Found Staging model: Version {v.version}\")\n",
    "                    break\n",
    "        \n",
    "        if not best_version and versions:\n",
    "            best_version = versions[0]\n",
    "            print(f\"âœ… Found latest model: Version {best_version.version}\")\n",
    "\n",
    "        if best_version:\n",
    "            run = client.get_run(best_version.run_id)\n",
    "            metric = run.data.metrics.get(METRIC_KEY)\n",
    "            \n",
    "            print(f\"   Version: {best_version.version}\")\n",
    "            print(f\"   Run ID: {best_version.run_id}\")\n",
    "            print(f\"   {METRIC_KEY}: {metric:.6f}\")\n",
    "            print(f\"   Aliases: {', '.join(best_version.aliases) if best_version.aliases else 'None'}\")\n",
    "            \n",
    "            return {\n",
    "                'version': best_version.version,\n",
    "                'run_id': best_version.run_id,\n",
    "                'metric': metric,\n",
    "                'aliases': best_version.aliases or []\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"â„¹ï¸ No current model in registry: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ” STEP 3: COMPARE MODELS\n",
    "# =============================================================================\n",
    "def compare_models(new_model, current_model):\n",
    "    \"\"\"Compare new model with current best model\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 3: Model Comparison\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if not current_model:\n",
    "        print(\"âœ… No existing model - promoting first model to Staging\")\n",
    "        return True, \"First model registration\", 0.0\n",
    "\n",
    "    new_metric = new_model['metric']\n",
    "    current_metric = current_model['metric']\n",
    "    \n",
    "    improvement = (current_metric - new_metric) / current_metric\n",
    "    improvement_pct = improvement * 100\n",
    "\n",
    "    print(f\"\\nğŸ“Š Comparison Results:\")\n",
    "    print(f\"   New Model RMSE: {new_metric:.6f}\")\n",
    "    print(f\"   Current Model RMSE: {current_metric:.6f}\")\n",
    "    print(f\"   Improvement: {improvement_pct:.2f}%\")\n",
    "    print(f\"   Required Threshold: {IMPROVEMENT_THRESHOLD * 100}%\")\n",
    "\n",
    "    if improvement >= IMPROVEMENT_THRESHOLD:\n",
    "        print(f\"\\nâœ… PROMOTE: New model is {improvement_pct:.2f}% better\")\n",
    "        return True, f\"Improved by {improvement_pct:.2f}%\", improvement\n",
    "    else:\n",
    "        print(f\"\\nâŒ DO NOT PROMOTE: Improvement ({improvement_pct:.2f}%) below threshold\")\n",
    "        return False, f\"Insufficient improvement ({improvement_pct:.2f}%)\", improvement\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# =============================================================================\n",
    "def promote_to_staging(best_model, comparison_result):\n",
    "    \"\"\"Promote the best model to Staging\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 4: Promoting Model to Staging\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        model_uri = f\"runs:/{best_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "        \n",
    "        # Register the model\n",
    "        print(f\"â³ Registering model from run: {best_model['run_id']}\")\n",
    "        new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "        \n",
    "        print(f\"âœ… Model registered as Version {new_version.version}\")\n",
    "        \n",
    "        # Set Staging alias\n",
    "        print(f\"â³ Setting '{STAGING_ALIAS}' alias...\")\n",
    "        client.set_registered_model_alias(\n",
    "            MODEL_NAME, \n",
    "            STAGING_ALIAS, \n",
    "            new_version.version\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Version {new_version.version} promoted to @{STAGING_ALIAS}\")\n",
    "        \n",
    "        # Add tags\n",
    "        tags = {\n",
    "            \"promoted_from\": \"evaluation_pipeline\",\n",
    "            \"improvement_pct\": f\"{comparison_result['improvement'] * 100:.2f}\",\n",
    "            \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "            \"metric_rmse\": str(best_model['metric'])\n",
    "        }\n",
    "        \n",
    "        for key, value in tags.items():\n",
    "            client.set_model_version_tag(MODEL_NAME, new_version.version, key, value)\n",
    "        \n",
    "        print(f\"âœ… Tags added to Version {new_version.version}\")\n",
    "        \n",
    "        return new_version.version\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Promotion failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“ STEP 5: LOG COMPARISON TO DELTA TABLE\n",
    "# =============================================================================\n",
    "def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version):\n",
    "    \"\"\"Log evaluation results to Delta table\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ“‹ STEP 5: Logging Evaluation Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        log_data = {\n",
    "            \"evaluation_timestamp\": [datetime.now()],\n",
    "            \"new_run_id\": [new_model['run_id']],\n",
    "            \"new_run_name\": [new_model['run_name']],\n",
    "            \"new_metric\": [new_model['metric']],\n",
    "            \"current_version\": [current_model['version'] if current_model else None],\n",
    "            \"current_metric\": [current_model['metric'] if current_model else None],\n",
    "            \"improvement_pct\": [comparison_result['improvement'] * 100],\n",
    "            \"promoted\": [comparison_result['should_promote']],\n",
    "            \"promoted_version\": [promoted_version],\n",
    "            \"reason\": [comparison_result['reason']]\n",
    "        }\n",
    "        \n",
    "        df = spark.createDataFrame(pd.DataFrame(log_data))\n",
    "        \n",
    "        # Append to Delta table\n",
    "        df.write.format(\"delta\").mode(\"append\").saveAsTable(COMPARISON_LOG_TABLE)\n",
    "        \n",
    "        print(f\"âœ… Evaluation logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to log to Delta: {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ¬ MAIN EXECUTION\n",
    "# =============================================================================\n",
    "def main():\n",
    "    print(f\"\\nğŸ¯ Evaluation Strategy: ALL-TIME BEST\")\n",
    "    print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "    print(f\"   Model Registry: {MODEL_NAME}\")\n",
    "    print(f\"   Metric: {METRIC_KEY} (lower is better)\")\n",
    "    \n",
    "    # Step 1: Find best model from experiment\n",
    "    best_model = get_best_model_from_experiment()\n",
    "    if not best_model:\n",
    "        print(\"\\nâŒ EVALUATION FAILED - No valid runs found\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Step 2: Get current best model\n",
    "    current_model = get_current_best_model()\n",
    "    \n",
    "    # Step 3: Compare models\n",
    "    should_promote, reason, improvement = compare_models(best_model, current_model)\n",
    "    comparison_result = {\n",
    "        'should_promote': should_promote,\n",
    "        'reason': reason,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "    # Step 4: Promote if needed\n",
    "    promoted_version = None\n",
    "    if should_promote:\n",
    "        promoted_version = promote_to_staging(best_model, comparison_result)\n",
    "    \n",
    "    # Step 5: Log results\n",
    "    log_comparison_to_delta(best_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Decision: {'PROMOTED âœ…' if should_promote else 'NOT PROMOTED âŒ'}\")\n",
    "    print(f\"Reason: {reason}\")\n",
    "    print(f\"Selected: {best_model['run_name']} (Rank #1 from {best_model['total_runs']} runs)\")\n",
    "    print(f\"RMSE: {best_model['metric']:.6f}\")\n",
    "    if promoted_version:\n",
    "        print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sys.exit(0 if should_promote else 1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # ğŸ”„ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # âœ… exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Logging Config\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ“Š STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 1: Finding Latest Trained Model (Metric-driven)\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"metrics.\" + METRIC_KEY + \" DESC\"],  # Fetch best metric, not latest timestamp\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         best_run = runs[0]\n",
    "#         run_id = best_run.info.run_id\n",
    "#         run_name = best_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = best_run.data.metrics\n",
    "#         params = best_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\nâœ… Best Training Run Found (by {METRIC_KEY}):\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': best_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error getting best model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ† STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\nâœ… Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\nâ„¹ï¸ No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âš–ï¸ STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nğŸŸ¢ DECISION: PROMOTE â€” First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nğŸ”´ DECISION: DO NOT PROMOTE â€” Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nğŸŸ¢ DECISION: PROMOTE â€” Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nğŸ“Š Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nğŸŸ¢ PROMOTE â€” New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nğŸŸ¡ NO PROMOTION â€” Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nğŸ”´ DO NOT PROMOTE â€” Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI â†’ {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\nâœ… Model Registered & Promoted â†’ @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ“ STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"âœ… Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ¬ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"âŒ No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ğŸ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # ğŸ”„ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # âœ… exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Notification & Logging Config\n",
    "# ENABLE_SLACK = False\n",
    "# SLACK_WEBHOOK_URL = \"\"\n",
    "# ENABLE_EMAIL = False\n",
    "# EMAIL_RECIPIENT = \"\"\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ“Š STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 1: Finding Latest Trained Model\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"start_time DESC\"],\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         latest_run = runs[0]\n",
    "#         run_id = latest_run.info.run_id\n",
    "#         run_name = latest_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = latest_run.data.metrics\n",
    "#         params = latest_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\nâœ… Latest Training Run Found:\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(latest_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': latest_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error getting latest model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ† STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\nâœ… Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\nâ„¹ï¸ No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âš–ï¸ STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nğŸŸ¢ DECISION: PROMOTE â€” First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nğŸ”´ DECISION: DO NOT PROMOTE â€” Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nğŸŸ¢ DECISION: PROMOTE â€” Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nğŸ“Š Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nğŸŸ¢ PROMOTE â€” New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nğŸŸ¡ NO PROMOTION â€” Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nğŸ”´ DO NOT PROMOTE â€” Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ğŸ“‹ STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI â†’ {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\nâœ… Model Registered & Promoted â†’ @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ“ STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"âœ… Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ¬ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"âŒ No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
