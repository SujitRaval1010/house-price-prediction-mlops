{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================================\n",
    "# ðŸŽ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# =============================================================================\n",
    "# This script compares newly trained model with current best model\n",
    "# Auto-promotes if better, sends notifications, logs everything\n",
    "# =============================================================================\n",
    "\n",
    "%pip install xgboost requests\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# =============================================================================\n",
    "EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "STAGING_ALIAS = \"staging\"   # ðŸ”„ aligned lowercase alias for consistency\n",
    "PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "MODEL_ARTIFACT_PATH = \"xgboost_model\"   # âœ… exactly same as training script\n",
    "\n",
    "METRIC_KEY = \"test_rmse\"\n",
    "IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# Logging Config\n",
    "COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… INITIALIZATION\n",
    "# =============================================================================\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Initialization failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸ“Š STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# =============================================================================\n",
    "def get_latest_trained_model():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ“‹ STEP 1: Finding Latest Trained Model (Metric-driven)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "        if not exp:\n",
    "            raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "        runs = client.search_runs(\n",
    "            [exp.experiment_id],\n",
    "            order_by=[\"metrics.\" + METRIC_KEY + \" DESC\"],  # Fetch best metric, not latest timestamp\n",
    "            max_results=1\n",
    "        )\n",
    "\n",
    "        if not runs:\n",
    "            raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "        best_run = runs[0]\n",
    "        run_id = best_run.info.run_id\n",
    "        run_name = best_run.info.run_name or \"Unnamed\"\n",
    "        metrics = best_run.data.metrics\n",
    "        params = best_run.data.params\n",
    "        metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "        print(f\"\\nâœ… Best Training Run Found (by {METRIC_KEY}):\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Run Name: {run_name}\")\n",
    "        print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "        print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "        print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "\n",
    "        return {\n",
    "            'run_id': run_id,\n",
    "            'run_name': run_name,\n",
    "            'metric': metric_value,\n",
    "            'params': params,\n",
    "            'metrics_all': metrics,\n",
    "            'timestamp': best_run.info.start_time\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting best model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸ† STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# =============================================================================\n",
    "def get_current_best_model():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ“‹ STEP 2: Finding Current Best Model in Registry\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    best_model = None\n",
    "    for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "        try:\n",
    "            mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "            run = client.get_run(mv.run_id)\n",
    "            metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "            if metric_value is None:\n",
    "                metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "                metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "            best_model = {\n",
    "                'version': mv.version,\n",
    "                'run_id': mv.run_id,\n",
    "                'alias': alias_name,\n",
    "                'metric': metric_value,\n",
    "                'params': run.data.params,\n",
    "                'metrics_all': run.data.metrics\n",
    "            }\n",
    "\n",
    "            print(f\"\\nâœ… Found Model with @{alias_name} Alias:\")\n",
    "            print(f\"   Version: v{mv.version}\")\n",
    "            print(f\"   Run ID: {mv.run_id}\")\n",
    "            print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "            break\n",
    "        except Exception:\n",
    "            print(f\"   No model found with @{alias_name} alias\")\n",
    "            continue\n",
    "\n",
    "    if not best_model:\n",
    "        print(\"\\nâ„¹ï¸ No existing model in registry. This will be the first model.\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# âš–ï¸ STEP 3: COMPARE MODELS\n",
    "# =============================================================================\n",
    "def compare_models(new_model, current_model):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ“‹ STEP 3: Model Comparison Analysis\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if current_model is None:\n",
    "        print(\"\\nðŸŸ¢ DECISION: PROMOTE â€” First model, no existing baseline.\")\n",
    "        return True, \"First model - no comparison needed\", None\n",
    "\n",
    "    if new_model['metric'] is None:\n",
    "        print(\"\\nðŸ”´ DECISION: DO NOT PROMOTE â€” Missing new model metric.\")\n",
    "        return False, \"New model missing metric\", None\n",
    "\n",
    "    if current_model['metric'] is None:\n",
    "        print(\"\\nðŸŸ¢ DECISION: PROMOTE â€” Current model lacks metric.\")\n",
    "        return True, \"Current model lacks metric\", None\n",
    "\n",
    "    new_metric = new_model['metric']\n",
    "    current_metric = current_model['metric']\n",
    "\n",
    "    improvement = current_metric - new_metric\n",
    "    improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "    print(f\"\\nðŸ“Š Comparison Summary:\")\n",
    "    print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "    print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "    print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "    threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "    if improvement > threshold_value:\n",
    "        print(f\"\\nðŸŸ¢ PROMOTE â€” New model {improvement_pct:.2f}% better.\")\n",
    "        return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "    elif abs(improvement) <= threshold_value:\n",
    "        print(f\"\\nðŸŸ¡ NO PROMOTION â€” Similar performance.\")\n",
    "        return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "    else:\n",
    "        print(f\"\\nðŸ”´ DO NOT PROMOTE â€” Worse performance.\")\n",
    "        return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# =============================================================================\n",
    "def promote_to_staging(new_model, comparison_result):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ“‹ STEP 4: Register & Promote to Staging\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "        print(f\"Registering model from URI â†’ {model_uri}\")\n",
    "\n",
    "        new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "        client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "        client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "        client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "        client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "        print(f\"\\nâœ… Model Registered & Promoted â†’ @{STAGING_ALIAS}\")\n",
    "        print(f\"   Version: v{new_version.version}\")\n",
    "        print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "        print(f\"   Reason: {comparison_result['reason']}\")\n",
    "        return new_version.version\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Promotion failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸ“ STEP 5: LOG RESULTS\n",
    "# =============================================================================\n",
    "def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "    try:\n",
    "        log_data = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'new_run_id': new_model['run_id'],\n",
    "            'new_run_name': new_model['run_name'],\n",
    "            'new_metric': new_model['metric'],\n",
    "            'current_version': int(current_model['version']) if current_model else None,\n",
    "            'current_metric': current_model['metric'] if current_model else None,\n",
    "            'current_alias': current_model['alias'] if current_model else None,\n",
    "            'should_promote': comparison_result['should_promote'],\n",
    "            'promotion_reason': comparison_result['reason'],\n",
    "            'improvement_pct': comparison_result['improvement'],\n",
    "            'promoted_to_staging': promoted_version is not None,\n",
    "            'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "            'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "        }\n",
    "\n",
    "        spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "            .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "        print(f\"âœ… Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸŽ¬ MAIN EXECUTION\n",
    "# =============================================================================\n",
    "def main():\n",
    "    new_model = get_latest_trained_model()\n",
    "    if not new_model:\n",
    "        print(\"âŒ No new model found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    current_model = get_current_best_model()\n",
    "    should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "    comparison_result = {\n",
    "        'should_promote': should_promote,\n",
    "        'reason': reason,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "    promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "    log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "    print(f\"Reason: {reason}\")\n",
    "    if promoted_version:\n",
    "        print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… EXECUTE\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # ðŸŽ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ðŸŽ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # ðŸ”„ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # âœ… exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Notification & Logging Config\n",
    "# ENABLE_SLACK = False\n",
    "# SLACK_WEBHOOK_URL = \"\"\n",
    "# ENABLE_EMAIL = False\n",
    "# EMAIL_RECIPIENT = \"\"\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ“Š STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 1: Finding Latest Trained Model\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"start_time DESC\"],\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         latest_run = runs[0]\n",
    "#         run_id = latest_run.info.run_id\n",
    "#         run_name = latest_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = latest_run.data.metrics\n",
    "#         params = latest_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\nâœ… Latest Training Run Found:\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(latest_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': latest_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error getting latest model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ† STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\nâœ… Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\nâ„¹ï¸ No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âš–ï¸ STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nðŸŸ¢ DECISION: PROMOTE â€” First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nðŸ”´ DECISION: DO NOT PROMOTE â€” Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nðŸŸ¢ DECISION: PROMOTE â€” Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nðŸ“Š Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nðŸŸ¢ PROMOTE â€” New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nðŸŸ¡ NO PROMOTION â€” Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nðŸ”´ DO NOT PROMOTE â€” Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI â†’ {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\nâœ… Model Registered & Promoted â†’ @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ“ STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"âœ… Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸŽ¬ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"âŒ No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
