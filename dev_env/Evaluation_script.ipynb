{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =============================================================================\n",
    "# üéØ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "%pip install xgboost requests\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import traceback\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ CONFIGURATION - MUST MATCH training_script.py EXACTLY!\n",
    "# =============================================================================\n",
    "EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"  # ‚úÖ Must match training script!\n",
    "UC_CATALOG = \"workspace\"\n",
    "UC_SCHEMA = \"ml\"\n",
    "MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "STAGING_ALIAS = \"staging\"\n",
    "PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "MODEL_ARTIFACT_PATH = \"xgboost_model\"  # ‚úÖ Must match training script!\n",
    "METRIC_KEY = \"test_rmse\"\n",
    "IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# Logging Config\n",
    "COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ INITIALIZATION\n",
    "# =============================================================================\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    print(\"‚úÖ MLflow and Spark initialized\\n\")\n",
    "\n",
    "    # --- FIX: Ensure experiment exists ---\n",
    "    exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if exp is None:\n",
    "        print(f\"‚ö†Ô∏è Experiment '{EXPERIMENT_NAME}' not found. Creating it now...\")\n",
    "        mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"‚úÖ Experiment '{EXPERIMENT_NAME}' created\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Initialization failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# üìä STEP 1: GET BEST MODEL FROM ALL EXPERIMENT RUNS\n",
    "# =============================================================================\n",
    "def get_best_model_from_experiment():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 1: Finding BEST Model Across ALL Experiment Runs\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        # Experiment already ensured during initialization\n",
    "        exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "        print(f\"‚úÖ Experiment found: {EXPERIMENT_NAME}\")\n",
    "        print(f\"   Experiment ID: {exp.experiment_id}\")\n",
    "\n",
    "        # Get ALL runs with valid metrics, sorted by RMSE (best first)\n",
    "        all_runs = client.search_runs(\n",
    "            [exp.experiment_id],\n",
    "            filter_string=f\"metrics.{METRIC_KEY} > 0\",\n",
    "            order_by=[f\"metrics.{METRIC_KEY} ASC\"],\n",
    "            max_results=1000\n",
    "        )\n",
    "\n",
    "        if not all_runs:\n",
    "            print(f\"\\n‚ùå ERROR: No runs found with valid '{METRIC_KEY}' metric!\")\n",
    "            return None\n",
    "\n",
    "        print(f\"‚úÖ Total runs in experiment: {len(all_runs)}\")\n",
    "\n",
    "        best_run = all_runs[0]\n",
    "\n",
    "        # Show top 10 models\n",
    "        print(f\"\\nüìä Top 10 Models in Experiment (by {METRIC_KEY}):\")\n",
    "        print(f\"{'Rank':<6} {'Run Name':<40} {'RMSE':<15} {'Timestamp':<20}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        for i, run in enumerate(all_runs[:10], 1):\n",
    "            run_name = run.info.run_name or \"Unnamed\"\n",
    "            metric_val = run.data.metrics.get(METRIC_KEY, float('inf'))\n",
    "            timestamp = datetime.fromtimestamp(run.info.start_time/1000).strftime('%Y-%m-%d %H:%M')\n",
    "            marker = \"üëë BEST\" if i == 1 else f\"{i}.\"\n",
    "            print(f\"{marker:<6} {run_name:<40} {metric_val:<15.6f} {timestamp}\")\n",
    "\n",
    "        run_id = best_run.info.run_id\n",
    "        run_name = best_run.info.run_name or \"Unnamed\"\n",
    "        metrics = best_run.data.metrics\n",
    "        params = best_run.data.params\n",
    "        metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "        print(f\"\\n‚úÖ BEST Model Selected:\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Run Name: {run_name}\")\n",
    "        print(f\"   {METRIC_KEY}: {metric_value:.6f}\")\n",
    "        print(f\"   Rank: #1 out of {len(all_runs)} total runs\")\n",
    "        print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "        print(f\"   Parameters: {dict(list(params.items())[:5])}...\")\n",
    "\n",
    "        return {\n",
    "            'run_id': run_id,\n",
    "            'run_name': run_name,\n",
    "            'metric': metric_value,\n",
    "            'params': params,\n",
    "            'metrics_all': metrics,\n",
    "            'timestamp': best_run.info.start_time,\n",
    "            'total_runs': len(all_runs)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting best model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# The rest of your original code (get_current_best_model, compare_models,\n",
    "# promote_to_staging, log_comparison_to_delta, main) stays 100% unchanged\n",
    "# =============================================================================\n",
    "\n",
    "# Your original functions go here...\n",
    "# get_current_best_model()\n",
    "# compare_models()\n",
    "# promote_to_staging()\n",
    "# log_comparison_to_delta()\n",
    "\n",
    "# =============================================================================\n",
    "# üé¨ MAIN EXECUTION\n",
    "# =============================================================================\n",
    "def main():\n",
    "    print(f\"\\nüéØ Selection Strategy: ALL-TIME BEST\")\n",
    "    print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "    print(f\"   Model Registry: {MODEL_NAME}\")\n",
    "    print(f\"   Metric: {METRIC_KEY} (lower is better)\")\n",
    "    \n",
    "    best_model = get_best_model_from_experiment()\n",
    "    if not best_model:\n",
    "        print(\"\\n‚ùå EVALUATION FAILED - Cannot proceed\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    current_model = get_current_best_model()\n",
    "    should_promote, reason, improvement = compare_models(best_model, current_model)\n",
    "    comparison_result = {\n",
    "        'should_promote': should_promote,\n",
    "        'reason': reason,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "    promoted_version = None\n",
    "    if should_promote:\n",
    "        promoted_version = promote_to_staging(best_model, comparison_result)\n",
    "    \n",
    "    log_comparison_to_delta(best_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ MODEL EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Decision: {'PROMOTED ‚úÖ' if should_promote else 'NOT PROMOTED ‚ùå'}\")\n",
    "    print(f\"Reason: {reason}\")\n",
    "    print(f\"Selected: {best_model['run_name']} (Rank #1 from {best_model['total_runs']} runs)\")\n",
    "    print(f\"RMSE: {best_model['metric']:.6f}\")\n",
    "    if promoted_version:\n",
    "        print(f\"Promoted Version: v{promoted_version} ‚Üí @{STAGING_ALIAS}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sys.exit(0 if should_promote else 1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # üéØ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"üéØ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # ‚úÖ CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # üîÑ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # ‚úÖ exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Logging Config\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # ‚úÖ INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"‚úÖ MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üìä STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 1: Finding Latest Trained Model (Metric-driven)\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"metrics.\" + METRIC_KEY + \" DESC\"],  # Fetch best metric, not latest timestamp\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         best_run = runs[0]\n",
    "#         run_id = best_run.info.run_id\n",
    "#         run_name = best_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = best_run.data.metrics\n",
    "#         params = best_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\n‚úÖ Best Training Run Found (by {METRIC_KEY}):\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': best_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error getting best model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üèÜ STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\n‚úÖ Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\n‚ÑπÔ∏è No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ‚öñÔ∏è STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nüü¢ DECISION: PROMOTE ‚Äî First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nüî¥ DECISION: DO NOT PROMOTE ‚Äî Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nüü¢ DECISION: PROMOTE ‚Äî Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nüìä Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nüü¢ PROMOTE ‚Äî New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nüü° NO PROMOTION ‚Äî Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nüî¥ DO NOT PROMOTE ‚Äî Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üöÄ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI ‚Üí {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\n‚úÖ Model Registered & Promoted ‚Üí @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üìù STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"‚úÖ Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üé¨ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"‚ùå No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"‚úÖ MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} ‚Üí @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ‚úÖ EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # üéØ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"üéØ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # ‚úÖ CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # üîÑ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # ‚úÖ exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Notification & Logging Config\n",
    "# ENABLE_SLACK = False\n",
    "# SLACK_WEBHOOK_URL = \"\"\n",
    "# ENABLE_EMAIL = False\n",
    "# EMAIL_RECIPIENT = \"\"\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # ‚úÖ INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"‚úÖ MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üìä STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 1: Finding Latest Trained Model\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"start_time DESC\"],\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         latest_run = runs[0]\n",
    "#         run_id = latest_run.info.run_id\n",
    "#         run_name = latest_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = latest_run.data.metrics\n",
    "#         params = latest_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\n‚úÖ Latest Training Run Found:\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(latest_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': latest_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error getting latest model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üèÜ STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\n‚úÖ Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\n‚ÑπÔ∏è No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ‚öñÔ∏è STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nüü¢ DECISION: PROMOTE ‚Äî First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nüî¥ DECISION: DO NOT PROMOTE ‚Äî Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nüü¢ DECISION: PROMOTE ‚Äî Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nüìä Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nüü¢ PROMOTE ‚Äî New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nüü° NO PROMOTION ‚Äî Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nüî¥ DO NOT PROMOTE ‚Äî Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üöÄ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"üìã STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI ‚Üí {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\n‚úÖ Model Registered & Promoted ‚Üí @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üìù STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"‚úÖ Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # üé¨ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"‚ùå No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"‚úÖ MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} ‚Üí @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ‚úÖ EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
