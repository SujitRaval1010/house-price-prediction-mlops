{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ðŸ“ STEP 5: LOG EVALUATION HISTORY (FIXED)\n",
    "# =============================================================================\n",
    "def log_evaluation_history(model_info, current_model, evaluation_result):\n",
    "    \"\"\"Log evaluation to history table\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ“‹ STEP 5: Logging Evaluation History\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, LongType, TimestampType\n",
    "        \n",
    "        # Define explicit schema with promoted_version as LongType\n",
    "        schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"new_run_id\", StringType(), True),\n",
    "            StructField(\"new_run_name\", StringType(), True),\n",
    "            StructField(\"new_metric\", DoubleType(), True),\n",
    "            StructField(\"current_version\", LongType(), True),\n",
    "            StructField(\"current_metric\", DoubleType(), True),\n",
    "            StructField(\"current_alias\", StringType(), True),\n",
    "            StructField(\"should_promote\", BooleanType(), True),\n",
    "            StructField(\"promotion_reason\", StringType(), True),\n",
    "            StructField(\"improvement_pct\", DoubleType(), True),\n",
    "            StructField(\"promoted_to_staging\", BooleanType(), True),\n",
    "            StructField(\"promoted_version\", LongType(), True),  # âœ… Explicit LongType\n",
    "            StructField(\"threshold_used\", DoubleType(), True),\n",
    "            StructField(\"total_runs_evaluated\", LongType(), True),\n",
    "            StructField(\"selection_method\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        log_data = {\n",
    "            \"timestamp\": [datetime.now()],\n",
    "            \"new_run_id\": [model_info['run_id']],\n",
    "            \"new_run_name\": [model_info['run_name']],\n",
    "            \"new_metric\": [float(model_info['metric_value'])],\n",
    "            \"current_version\": [int(current_model['version']) if current_model else None],\n",
    "            \"current_metric\": [float(current_model['metric_value']) if current_model and current_model['metric_value'] else None],\n",
    "            \"current_alias\": ['Staging' if current_model else None],\n",
    "            \"should_promote\": [bool(evaluation_result['should_register'])],\n",
    "            \"promotion_reason\": [str(evaluation_result['reason'])],\n",
    "            \"improvement_pct\": [float(evaluation_result['improvement_pct'])],\n",
    "            \"promoted_to_staging\": [False],\n",
    "            \"promoted_version\": [None],  # âœ… Will be LongType due to schema\n",
    "            \"threshold_used\": [float(IMPROVEMENT_THRESHOLD * 100)],\n",
    "            \"total_runs_evaluated\": [int(model_info['total_runs'])],\n",
    "            \"selection_method\": [\"ALL-TIME BEST\"]\n",
    "        }\n",
    "        \n",
    "        df = spark.createDataFrame(pd.DataFrame(log_data), schema=schema)  # âœ… Use explicit schema\n",
    "        \n",
    "        df.write.format(\"delta\")\\\n",
    "            .mode(\"append\")\\\n",
    "            .option(\"mergeSchema\", \"true\")\\\n",
    "            .saveAsTable(EVALUATION_LOG_TABLE)\n",
    "        \n",
    "        print(f\"âœ… History logged to: {EVALUATION_LOG_TABLE}\")\n",
    "        print(f\"   Schema validated with promoted_version as LongType\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to log history: {e}\")\n",
    "        print(\"   (Non-critical error - continuing)\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # ðŸŽ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ðŸŽ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # ðŸ”„ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # âœ… exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Logging Config\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ“Š STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 1: Finding Latest Trained Model (Metric-driven)\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"metrics.\" + METRIC_KEY + \" DESC\"],  # Fetch best metric, not latest timestamp\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         best_run = runs[0]\n",
    "#         run_id = best_run.info.run_id\n",
    "#         run_name = best_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = best_run.data.metrics\n",
    "#         params = best_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\nâœ… Best Training Run Found (by {METRIC_KEY}):\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(best_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': best_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error getting best model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ† STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\nâœ… Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\nâ„¹ï¸ No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âš–ï¸ STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nðŸŸ¢ DECISION: PROMOTE â€” First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nðŸ”´ DECISION: DO NOT PROMOTE â€” Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nðŸŸ¢ DECISION: PROMOTE â€” Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nðŸ“Š Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nðŸŸ¢ PROMOTE â€” New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nðŸŸ¡ NO PROMOTION â€” Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nðŸ”´ DO NOT PROMOTE â€” Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI â†’ {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\nâœ… Model Registered & Promoted â†’ @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ“ STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"âœ… Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸŽ¬ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"âŒ No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Databricks notebook source\n",
    "# # =============================================================================\n",
    "# # ðŸŽ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\n",
    "# # =============================================================================\n",
    "# # This script compares newly trained model with current best model\n",
    "# # Auto-promotes if better, sends notifications, logs everything\n",
    "# # =============================================================================\n",
    "\n",
    "# %pip install xgboost requests\n",
    "# import mlflow\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql import SparkSession\n",
    "# import requests\n",
    "# import traceback\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ðŸŽ¯ INTELLIGENT MODEL EVALUATION & AUTO-PROMOTION SYSTEM\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… CONFIGURATION (ALIGNED WITH TRAINING SCRIPT)\n",
    "# # =============================================================================\n",
    "# EXPERIMENT_NAME = \"/Shared/House_Price_Prediction_Config_Runs\"\n",
    "# UC_CATALOG = \"workspace\"\n",
    "# UC_SCHEMA = \"ml\"\n",
    "# MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.house_price_xgboost_uc2\"\n",
    "\n",
    "# STAGING_ALIAS = \"staging\"   # ðŸ”„ aligned lowercase alias for consistency\n",
    "# PRODUCTION_ALIAS = \"production\"\n",
    "\n",
    "# MODEL_ARTIFACT_PATH = \"xgboost_model\"   # âœ… exactly same as training script\n",
    "\n",
    "# METRIC_KEY = \"test_rmse\"\n",
    "# IMPROVEMENT_THRESHOLD = 0.02  # 2% improvement needed for promotion\n",
    "\n",
    "# # Notification & Logging Config\n",
    "# ENABLE_SLACK = False\n",
    "# SLACK_WEBHOOK_URL = \"\"\n",
    "# ENABLE_EMAIL = False\n",
    "# EMAIL_RECIPIENT = \"\"\n",
    "# COMPARISON_LOG_TABLE = \"workspace.default.model_evaluation_log\"\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… INITIALIZATION\n",
    "# # =============================================================================\n",
    "# try:\n",
    "#     spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
    "#     mlflow.set_tracking_uri(\"databricks\")\n",
    "#     mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#     client = MlflowClient()\n",
    "#     print(\"âœ… MLflow and Spark initialized\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Initialization failed: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ“Š STEP 1: GET LATEST TRAINED MODEL FROM EXPERIMENT\n",
    "# # =============================================================================\n",
    "# def get_latest_trained_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 1: Finding Latest Trained Model\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "#         if not exp:\n",
    "#             raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found\")\n",
    "\n",
    "#         runs = client.search_runs(\n",
    "#             [exp.experiment_id],\n",
    "#             order_by=[\"start_time DESC\"],\n",
    "#             max_results=1\n",
    "#         )\n",
    "\n",
    "#         if not runs:\n",
    "#             raise ValueError(\"No runs found in experiment\")\n",
    "\n",
    "#         latest_run = runs[0]\n",
    "#         run_id = latest_run.info.run_id\n",
    "#         run_name = latest_run.info.run_name or \"Unnamed\"\n",
    "#         metrics = latest_run.data.metrics\n",
    "#         params = latest_run.data.params\n",
    "#         metric_value = metrics.get(METRIC_KEY)\n",
    "\n",
    "#         print(f\"\\nâœ… Latest Training Run Found:\")\n",
    "#         print(f\"   Run ID: {run_id}\")\n",
    "#         print(f\"   Run Name: {run_name}\")\n",
    "#         print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#         print(f\"   Parameters: {dict(list(params.items())[:3])}...\")\n",
    "#         print(f\"   Timestamp: {datetime.fromtimestamp(latest_run.info.start_time/1000)}\")\n",
    "\n",
    "#         return {\n",
    "#             'run_id': run_id,\n",
    "#             'run_name': run_name,\n",
    "#             'metric': metric_value,\n",
    "#             'params': params,\n",
    "#             'metrics_all': metrics,\n",
    "#             'timestamp': latest_run.info.start_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error getting latest model: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ† STEP 2: GET CURRENT BEST MODEL (STAGING/PRODUCTION)\n",
    "# # =============================================================================\n",
    "# def get_current_best_model():\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 2: Finding Current Best Model in Registry\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     best_model = None\n",
    "#     for alias_name in [PRODUCTION_ALIAS, STAGING_ALIAS]:\n",
    "#         try:\n",
    "#             mv = client.get_model_version_by_alias(MODEL_NAME, alias_name)\n",
    "#             run = client.get_run(mv.run_id)\n",
    "#             metric_value = run.data.metrics.get(METRIC_KEY)\n",
    "#             if metric_value is None:\n",
    "#                 metric_tag = mv.tags.get(\"metric_rmse\")\n",
    "#                 metric_value = float(metric_tag) if metric_tag else None\n",
    "\n",
    "#             best_model = {\n",
    "#                 'version': mv.version,\n",
    "#                 'run_id': mv.run_id,\n",
    "#                 'alias': alias_name,\n",
    "#                 'metric': metric_value,\n",
    "#                 'params': run.data.params,\n",
    "#                 'metrics_all': run.data.metrics\n",
    "#             }\n",
    "\n",
    "#             print(f\"\\nâœ… Found Model with @{alias_name} Alias:\")\n",
    "#             print(f\"   Version: v{mv.version}\")\n",
    "#             print(f\"   Run ID: {mv.run_id}\")\n",
    "#             print(f\"   {METRIC_KEY}: {metric_value:.6f}\" if metric_value else f\"   {METRIC_KEY}: N/A\")\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             print(f\"   No model found with @{alias_name} alias\")\n",
    "#             continue\n",
    "\n",
    "#     if not best_model:\n",
    "#         print(\"\\nâ„¹ï¸ No existing model in registry. This will be the first model.\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âš–ï¸ STEP 3: COMPARE MODELS\n",
    "# # =============================================================================\n",
    "# def compare_models(new_model, current_model):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 3: Model Comparison Analysis\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     if current_model is None:\n",
    "#         print(\"\\nðŸŸ¢ DECISION: PROMOTE â€” First model, no existing baseline.\")\n",
    "#         return True, \"First model - no comparison needed\", None\n",
    "\n",
    "#     if new_model['metric'] is None:\n",
    "#         print(\"\\nðŸ”´ DECISION: DO NOT PROMOTE â€” Missing new model metric.\")\n",
    "#         return False, \"New model missing metric\", None\n",
    "\n",
    "#     if current_model['metric'] is None:\n",
    "#         print(\"\\nðŸŸ¢ DECISION: PROMOTE â€” Current model lacks metric.\")\n",
    "#         return True, \"Current model lacks metric\", None\n",
    "\n",
    "#     new_metric = new_model['metric']\n",
    "#     current_metric = current_model['metric']\n",
    "\n",
    "#     improvement = current_metric - new_metric\n",
    "#     improvement_pct = (improvement / current_metric) * 100\n",
    "\n",
    "#     print(f\"\\nðŸ“Š Comparison Summary:\")\n",
    "#     print(f\"   New RMSE: {new_metric:.6f}\")\n",
    "#     print(f\"   Old RMSE: {current_metric:.6f}\")\n",
    "#     print(f\"   Improvement: {improvement:.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "#     threshold_value = current_metric * IMPROVEMENT_THRESHOLD\n",
    "\n",
    "#     if improvement > threshold_value:\n",
    "#         print(f\"\\nðŸŸ¢ PROMOTE â€” New model {improvement_pct:.2f}% better.\")\n",
    "#         return True, f\"Improved by {improvement_pct:.2f}%\", improvement_pct\n",
    "#     elif abs(improvement) <= threshold_value:\n",
    "#         print(f\"\\nðŸŸ¡ NO PROMOTION â€” Similar performance.\")\n",
    "#         return False, f\"Similar performance ({improvement_pct:+.2f}%)\", improvement_pct\n",
    "#     else:\n",
    "#         print(f\"\\nðŸ”´ DO NOT PROMOTE â€” Worse performance.\")\n",
    "#         return False, f\"Worse by {abs(improvement_pct):.2f}%\", improvement_pct\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸš€ STEP 4: PROMOTE TO STAGING\n",
    "# # =============================================================================\n",
    "# def promote_to_staging(new_model, comparison_result):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"ðŸ“‹ STEP 4: Register & Promote to Staging\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     try:\n",
    "#         model_uri = f\"runs:/{new_model['run_id']}/{MODEL_ARTIFACT_PATH}\"\n",
    "#         print(f\"Registering model from URI â†’ {model_uri}\")\n",
    "\n",
    "#         new_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"source_run_id\", new_model['run_id'])\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"metric_rmse\", str(new_model['metric']))\n",
    "#         client.set_model_version_tag(MODEL_NAME, new_version.version, \"promotion_reason\", comparison_result['reason'])\n",
    "\n",
    "#         client.set_registered_model_alias(MODEL_NAME, STAGING_ALIAS, new_version.version)\n",
    "\n",
    "#         print(f\"\\nâœ… Model Registered & Promoted â†’ @{STAGING_ALIAS}\")\n",
    "#         print(f\"   Version: v{new_version.version}\")\n",
    "#         print(f\"   RMSE: {new_model['metric']:.6f}\")\n",
    "#         print(f\"   Reason: {comparison_result['reason']}\")\n",
    "#         return new_version.version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Promotion failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸ“ STEP 5: LOG RESULTS\n",
    "# # =============================================================================\n",
    "# def log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version=None):\n",
    "#     try:\n",
    "#         log_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'new_run_id': new_model['run_id'],\n",
    "#             'new_run_name': new_model['run_name'],\n",
    "#             'new_metric': new_model['metric'],\n",
    "#             'current_version': int(current_model['version']) if current_model else None,\n",
    "#             'current_metric': current_model['metric'] if current_model else None,\n",
    "#             'current_alias': current_model['alias'] if current_model else None,\n",
    "#             'should_promote': comparison_result['should_promote'],\n",
    "#             'promotion_reason': comparison_result['reason'],\n",
    "#             'improvement_pct': comparison_result['improvement'],\n",
    "#             'promoted_to_staging': promoted_version is not None,\n",
    "#             'promoted_version': int(promoted_version) if promoted_version else None,\n",
    "#             'threshold_used': IMPROVEMENT_THRESHOLD * 100\n",
    "#         }\n",
    "\n",
    "#         spark.createDataFrame(pd.DataFrame([log_data])) \\\n",
    "#             .write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "#             .saveAsTable(COMPARISON_LOG_TABLE)\n",
    "\n",
    "#         print(f\"âœ… Logged to {COMPARISON_LOG_TABLE}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ Logging failed: {e}\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ðŸŽ¬ MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     new_model = get_latest_trained_model()\n",
    "#     if not new_model:\n",
    "#         print(\"âŒ No new model found.\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     current_model = get_current_best_model()\n",
    "#     should_promote, reason, improvement = compare_models(new_model, current_model)\n",
    "#     comparison_result = {\n",
    "#         'should_promote': should_promote,\n",
    "#         'reason': reason,\n",
    "#         'improvement': improvement\n",
    "#     }\n",
    "\n",
    "#     promoted_version = promote_to_staging(new_model, comparison_result) if should_promote else None\n",
    "#     log_comparison_to_delta(new_model, current_model, comparison_result, promoted_version)\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"âœ… MODEL EVALUATION COMPLETE\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Decision: {'PROMOTED' if should_promote else 'NOT PROMOTED'}\")\n",
    "#     print(f\"Reason: {reason}\")\n",
    "#     if promoted_version:\n",
    "#         print(f\"Promoted Version: v{promoted_version} â†’ @{STAGING_ALIAS}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # âœ… EXECUTE\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
