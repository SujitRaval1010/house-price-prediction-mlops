{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9d7c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# डमी डेटा बनाना\n",
    "np.random.seed(42)\n",
    "num_samples = 2000\n",
    "\n",
    "data = {\n",
    "    'sq_feet': np.random.randint(500, 1000, num_samples),\n",
    "    'num_bedrooms': np.random.randint(1, 4, num_samples),\n",
    "    'num_bathrooms': np.random.randint(1, 3, num_samples),\n",
    "    'year_built': np.random.randint(2000, 2023, num_samples),\n",
    "    'location_score': np.random.rand(num_samples) * 10\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# हाउस प्राइस का अनुमान लगाना (डमी)\n",
    "df['price'] = (df['sq_feet'] * 150 +\n",
    "               df['num_bedrooms'] * 20000 +\n",
    "               df['num_bathrooms'] * 15000 +\n",
    "               (df['year_built'] - 1990) * 500 +\n",
    "               df['location_score'] * 10000 +\n",
    "               np.random.normal(0, 10000, num_samples))\n",
    "\n",
    "# Pandas DataFrame को PySpark DataFrame में बदलना\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# PySpark DataFrame का स्कीमा देखें\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# डेटा को एक अस्थायी (temporary) टेबल के रूप में रजिस्टर करें\n",
    "spark_df.createOrReplaceTempView(\"temp_house_data\")\n",
    "\n",
    "# SQL कमांड का उपयोग करके इस अस्थायी टेबल से एक डेल्टा टेबल बनाएं\n",
    "# CREATE OR REPLACE TABLE कमांड यह सुनिश्चित करता है कि अगर टेबल पहले से मौजूद है, तो वह अपडेट हो जाए\n",
    "spark.sql(\"CREATE OR REPLACE TABLE house_price_delta USING DELTA AS SELECT * FROM temp_house_data\")\n",
    "\n",
    "print(\"डमी डेटा सफलतापूर्वक डेल्टा टेबल 'house_price_delta' के रूप में सेव किया गया है।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# डेल्टा टेबल से डेटा पढ़ें\n",
    "read_df = spark.read.format(\"delta\").table(\"house_price_delta\")\n",
    "\n",
    "# डेटा का कुछ हिस्सा दिखाएं\n",
    "read_df.show(5)\n",
    "\n",
    "# टेबल में कुल पंक्तियों (rows) की संख्या देखें\n",
    "print(f\"कुल पंक्तियाँ: {read_df.count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
